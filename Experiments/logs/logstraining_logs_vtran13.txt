2025-02-27 17:49:30 - INFO - Logging started
2025-02-27 17:50:36 - INFO - Full sequence shape : (212, 80)
2025-02-27 17:50:36 - INFO - Future price shape : (200, 1)
2025-02-27 17:50:36 - INFO - Future Prices : [[ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 3]
 [ 3]
 [ 4]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-3]
 [-3]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 4]
 [ 4]
 [ 4]
 [ 4]
 [ 4]
 [ 4]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-2]
 [-2]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [-2]
 [-3]
 [-5]
 [-5]
 [-4]
 [-4]
 [-3]
 [-1]
 [-1]
 [ 0]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 1]]
2025-02-27 17:50:36 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 17:50:36 - INFO - Previous Slope shape : (188,)
2025-02-27 17:50:36 - INFO - prev slope : 1088    1
1089    1
1090    0
1091    0
1092    0
1093    0
1094    0
1095    0
1096    1
1097    3
1098    3
1099    4
1100    3
1101    3
1102    3
1103    2
1104    2
1105    2
1106    1
1107    0
1108    0
1109   -1
1110   -1
1111   -1
1112    0
1113    0
1114    0
1115    0
1116    1
1117    0
1118    0
1119    0
1120    0
1121    0
1122    0
1123   -1
1124   -1
1125   -1
1126    0
1127    0
1128    1
1129    1
1130    1
1131    2
1132    2
1133    2
1134    2
1135    1
1136    0
1137    0
1138   -1
1139   -1
1140   -1
1141   -1
1142   -1
1143    0
1144    0
1145    1
1146    1
1147    1
1148    1
1149    1
1150    0
1151    0
1152    0
1153   -1
1154   -1
1155   -2
1156   -2
1157   -2
1158   -2
1159   -1
1160   -1
1161    0
1162    0
1163    0
1164   -1
1165   -1
1166   -1
1167   -2
1168   -2
1169   -2
1170   -2
1171   -3
1172   -3
1173   -2
1174   -1
1175   -1
1176   -1
1177   -1
1178    0
1179    1
1180    2
1181    3
1182    4
1183    5
1184    4
1185    4
1186    4
1187    4
1188    4
1189    4
1190    3
1191    3
1192    2
1193    2
1194    1
1195    1
1196    0
1197    0
1198    0
1199    0
1200    0
1201    1
1202    1
1203    2
1204    3
1205    3
1206    3
1207    3
1208    3
1209    3
1210    3
1211    3
1212    3
1213    2
1214    1
1215    2
1216    2
1217    2
1218    2
1219    1
1220    1
1221    1
1222    1
1223    1
1224    1
1225    1
1226    1
1227    2
1228    2
1229    2
1230    2
1231    1
1232    1
1233    0
1234    0
1235   -1
1236   -1
1237    0
1238    1
1239    1
1240    0
1241   -1
1242   -2
1243   -2
1244   -3
1245   -3
1246   -2
1247   -2
1248   -1
1249    1
1250    1
1251    1
1252    1
1253    0
1254   -2
1255   -3
1256   -5
1257   -5
1258   -4
1259   -4
1260   -3
1261   -1
1262   -1
1263    0
1264    2
1265    2
1266    2
1267    2
1268    3
1269    3
1270    3
1271    3
1272    2
1273    2
1274    2
1275    2
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 17:50:40 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 17:50:40 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 17:50:40 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 17:50:40 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 17:50:41 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:01:10 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:01:10 - INFO - Future price shape : (200, 1)
2025-02-27 18:01:10 - INFO - Future Prices : [[ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 2]
 [ 3]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 4]
 [ 3]
 [ 1]
 [-1]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 5]
 [ 4]
 [ 4]
 [ 4]
 [ 3]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 3]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 2]
 [ 3]
 [ 3]
 [ 4]
 [ 4]
 [ 4]
 [ 5]
 [ 5]
 [ 4]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [-1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 1]
 [ 0]
 [ 1]
 [ 2]
 [ 3]
 [ 3]
 [ 4]
 [ 4]
 [ 3]
 [ 2]
 [ 0]
 [-1]
 [-2]
 [-3]
 [-4]
 [-4]
 [-4]
 [-4]
 [-4]
 [-3]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]]
2025-02-27 18:01:10 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:01:10 - INFO - Previous Slope shape : (188,)
2025-02-27 18:01:10 - INFO - prev slope : 534    1
535    2
536    2
537    1
538    0
539   -1
540   -1
541   -1
542   -1
543    0
544    2
545    3
546    5
547    5
548    5
549    5
550    4
551    3
552    1
553   -1
554   -2
555   -2
556   -1
557   -1
558   -1
559   -1
560    0
561    1
562    0
563    1
564    1
565    1
566    1
567    2
568    3
569    4
570    5
571    5
572    4
573    4
574    4
575    3
576    2
577    2
578    1
579    1
580    1
581    1
582   -1
583   -2
584   -2
585   -2
586   -2
587   -1
588    0
589    0
590    1
591    1
592    2
593    3
594    1
595    0
596    0
597   -1
598   -1
599   -1
600   -1
601   -1
602   -2
603   -2
604   -2
605   -2
606   -2
607   -2
608   -2
609   -2
610   -1
611   -1
612    0
613    0
614    0
615    0
616    0
617    1
618    2
619    3
620    3
621    4
622    4
623    4
624    5
625    5
626    4
627    3
628    3
629    3
630    2
631    2
632    1
633    0
634   -1
635    0
636    0
637   -1
638   -1
639   -2
640   -1
641   -1
642   -1
643    0
644    0
645    0
646    1
647    1
648    2
649    2
650    1
651    1
652    1
653    1
654    1
655    1
656    1
657    1
658    0
659    0
660    0
661    0
662    0
663    1
664    1
665    2
666    2
667    2
668    2
669    2
670    2
671    1
672    1
673    1
674    0
675    0
676    1
677    0
678    1
679    1
680    1
681    1
682    1
683    1
684    1
685    1
686    2
687    2
688    2
689    2
690    2
691    1
692    1
693    1
694    0
695    1
696    0
697    1
698    2
699    3
700    3
701    4
702    4
703    3
704    2
705    0
706   -1
707   -2
708   -3
709   -4
710   -4
711   -4
712   -4
713   -4
714   -3
715   -2
716   -2
717   -1
718    0
719    0
720    1
721    2
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:01:14 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:01:14 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:01:14 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:01:14 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:01:15 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:01:15 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:01:15 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:02:26 - INFO - Logging started
2025-02-27 18:03:00 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:03:01 - INFO - Future price shape : (200, 1)
2025-02-27 18:03:01 - INFO - Future Prices : [[ 1]
 [ 0]
 [-1]
 [-2]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 2]
 [ 4]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 3]
 [ 1]
 [ 0]
 [-1]
 [-2]
 [-3]
 [-4]
 [-5]
 [-5]
 [-4]
 [-3]
 [-1]
 [-1]
 [ 0]
 [ 2]
 [ 4]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 4]
 [ 3]
 [ 2]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [-1]
 [-2]
 [-3]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-4]
 [-2]
 [-1]
 [ 0]
 [ 2]
 [ 2]
 [ 3]
 [ 3]
 [ 4]
 [ 4]
 [ 4]
 [ 4]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 4]
 [ 4]
 [ 4]
 [ 4]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 4]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-2]
 [-2]
 [-3]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [-1]
 [-3]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-5]
 [-4]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [-1]]
2025-02-27 18:03:01 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:03:01 - INFO - Previous Slope shape : (188,)
2025-02-27 18:03:01 - INFO - prev slope : 1578    1
1579    0
1580   -1
1581   -2
1582   -3
1583   -3
1584   -3
1585   -2
1586   -2
1587   -1
1588   -1
1589    0
1590    0
1591    0
1592    0
1593    0
1594    0
1595    0
1596    1
1597    1
1598    1
1599    1
1600    1
1601    1
1602    1
1603    0
1604    1
1605    1
1606    1
1607    1
1608    1
1609    0
1610    2
1611    4
1612    5
1613    5
1614    5
1615    5
1616    5
1617    5
1618    5
1619    5
1620    5
1621    5
1622    5
1623    5
1624    5
1625    3
1626    1
1627    0
1628   -1
1629   -2
1630   -3
1631   -4
1632   -5
1633   -5
1634   -4
1635   -3
1636   -1
1637   -1
1638    0
1639    2
1640    4
1641    5
1642    5
1643    5
1644    5
1645    5
1646    5
1647    4
1648    3
1649    2
1650    1
1651    0
1652    0
1653   -1
1654   -1
1655   -1
1656   -1
1657   -1
1658   -1
1659   -1
1660   -1
1661   -1
1662    0
1663    1
1664    1
1665    1
1666    1
1667    1
1668    1
1669    1
1670    1
1671    1
1672    0
1673    0
1674    0
1675    1
1676    1
1677    1
1678    1
1679    1
1680    2
1681    2
1682    2
1683    1
1684   -1
1685   -2
1686   -3
1687   -5
1688   -5
1689   -5
1690   -5
1691   -5
1692   -5
1693   -5
1694   -5
1695   -4
1696   -2
1697   -1
1698    0
1699    2
1700    2
1701    3
1702    3
1703    4
1704    4
1705    4
1706    4
1707    5
1708    5
1709    5
1710    5
1711    5
1712    5
1713    5
1714    4
1715    4
1716    4
1717    4
1718    5
1719    5
1720    5
1721    5
1722    5
1723    4
1724    2
1725    1
1726    1
1727    1
1728    0
1729    1
1730    1
1731    1
1732    0
1733    0
1734   -2
1735   -2
1736   -3
1737   -2
1738   -2
1739   -1
1740    0
1741    1
1742    2
1743    2
1744    2
1745    1
1746   -1
1747   -3
1748   -5
1749   -5
1750   -5
1751   -5
1752   -5
1753   -5
1754   -5
1755   -5
1756   -4
1757   -2
1758   -1
1759   -1
1760   -1
1761   -1
1762   -1
1763   -1
1764   -1
1765   -1
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:03:04 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:03:04 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:03:04 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:03:04 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:03:05 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:03:05 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:03:05 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:03:05 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-27 18:03:06 - INFO - Output shape : torch.Size([256, 188, 11])
2025-02-27 18:09:10 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:09:10 - INFO - Future price shape : (200, 1)
2025-02-27 18:09:10 - INFO - Future Prices : [[ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-3]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-3]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-2]
 [-3]
 [-3]
 [-3]
 [-3]
 [-3]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]]
2025-02-27 18:09:10 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:09:10 - INFO - Previous Slope shape : (188,)
2025-02-27 18:09:10 - INFO - prev slope : 5874    1
5875    1
5876    0
5877    0
5878    0
5879   -1
5880    0
5881    0
5882    0
5883    0
5884    0
5885    0
5886    0
5887    0
5888    0
5889    0
5890    1
5891    1
5892    1
5893    1
5894    2
5895    2
5896    2
5897    2
5898    1
5899    0
5900    0
5901    0
5902   -1
5903   -1
5904   -1
5905   -1
5906   -1
5907   -1
5908   -1
5909   -1
5910   -1
5911   -1
5912    0
5913    0
5914    0
5915    0
5916    1
5917    1
5918    1
5919    1
5920    1
5921    2
5922    2
5923    2
5924    1
5925    1
5926    1
5927    1
5928    1
5929    0
5930   -1
5931   -1
5932   -2
5933   -2
5934   -2
5935   -3
5936   -3
5937   -3
5938   -3
5939   -2
5940   -2
5941   -1
5942   -1
5943   -1
5944   -1
5945   -1
5946   -1
5947   -1
5948   -1
5949   -1
5950   -1
5951    0
5952    0
5953    1
5954    1
5955    1
5956    1
5957    1
5958    1
5959    1
5960    1
5961    1
5962    0
5963    0
5964    0
5965    0
5966    0
5967    1
5968    1
5969    1
5970    1
5971    1
5972    1
5973    1
5974    1
5975    1
5976    0
5977    0
5978    0
5979   -1
5980   -1
5981   -1
5982    0
5983    0
5984    0
5985    1
5986    1
5987    1
5988    1
5989    1
5990    1
5991    1
5992    1
5993    0
5994    0
5995    0
5996    1
5997    1
5998    0
5999    0
6000   -1
6001   -1
6002   -1
6003   -2
6004   -2
6005   -3
6006   -3
6007   -3
6008   -3
6009   -2
6010   -2
6011   -1
6012   -1
6013    0
6014    0
6015    1
6016    1
6017    1
6018    1
6019    0
6020    0
6021   -1
6022   -1
6023   -2
6024   -2
6025   -2
6026   -2
6027   -2
6028   -1
6029   -1
6030   -1
6031   -1
6032   -1
6033   -1
6034   -1
6035   -1
6036   -1
6037   -1
6038   -1
6039   -1
6040   -2
6041   -3
6042   -3
6043   -3
6044   -3
6045   -3
6046   -3
6047   -3
6048   -3
6049   -2
6050   -2
6051   -1
6052   -1
6053   -1
6054   -1
6055   -1
6056    0
6057    0
6058    0
6059    0
6060    0
6061    0
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:09:14 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:09:14 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:09:14 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:09:14 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:09:16 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:09:16 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:09:16 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:10:23 - INFO - Logging started
2025-02-27 18:10:58 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:10:58 - INFO - Future price shape : (200, 1)
2025-02-27 18:10:58 - INFO - Future Prices : [[ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]]
2025-02-27 18:10:58 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:10:58 - INFO - Previous Slope shape : (188,)
2025-02-27 18:10:58 - INFO - prev slope : 4590    1
4591    1
4592    2
4593    2
4594    2
4595    2
4596    2
4597    2
4598    2
4599    1
4600    1
4601    0
4602    0
4603   -1
4604   -1
4605   -1
4606   -2
4607   -2
4608   -2
4609   -2
4610   -1
4611   -1
4612   -1
4613    0
4614    0
4615    1
4616    1
4617    1
4618    1
4619    1
4620    1
4621    1
4622    1
4623    1
4624    1
4625    1
4626    1
4627    1
4628    1
4629    0
4630    0
4631    0
4632    0
4633    0
4634    0
4635    0
4636    0
4637    1
4638    1
4639    1
4640    1
4641    0
4642    0
4643   -1
4644   -1
4645   -2
4646   -1
4647   -1
4648   -1
4649   -1
4650    0
4651    0
4652    1
4653    1
4654    1
4655    2
4656    2
4657    2
4658    2
4659    2
4660    2
4661    2
4662    2
4663    2
4664    2
4665    2
4666    2
4667    2
4668    1
4669    1
4670    1
4671    0
4672    0
4673    0
4674    0
4675    0
4676    0
4677   -1
4678   -1
4679   -1
4680   -1
4681   -1
4682    0
4683    0
4684    0
4685    1
4686    1
4687    1
4688    1
4689    0
4690    0
4691   -1
4692   -1
4693   -1
4694   -1
4695   -1
4696   -1
4697   -1
4698   -1
4699   -1
4700   -1
4701   -1
4702   -1
4703   -1
4704   -1
4705   -1
4706   -1
4707   -1
4708   -2
4709   -2
4710   -2
4711   -1
4712   -1
4713    0
4714    0
4715    0
4716    0
4717    0
4718    0
4719   -1
4720   -1
4721   -1
4722   -1
4723   -1
4724    0
4725    0
4726    1
4727    1
4728    2
4729    2
4730    2
4731    2
4732    2
4733    1
4734    1
4735    1
4736    1
4737    0
4738    0
4739    0
4740    0
4741    1
4742    1
4743    1
4744    1
4745    1
4746    1
4747    1
4748    1
4749    1
4750    1
4751    1
4752    1
4753    1
4754    1
4755    1
4756    0
4757    0
4758    0
4759    1
4760    1
4761    1
4762    1
4763    1
4764    1
4765    2
4766    1
4767    1
4768    1
4769    2
4770    1
4771    2
4772    2
4773    2
4774    2
4775    1
4776    2
4777    2
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:11:02 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:11:02 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:11:02 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:11:02 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:11:02 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:11:02 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:11:02 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:11:03 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-27 18:11:03 - INFO - Output shape : torch.Size([256, 188, 11])
2025-02-27 18:11:03 - INFO - Output shape after view : torch.Size([256, 188, 1, 11])
2025-02-27 18:23:39 - INFO - Logging started
2025-02-27 18:27:50 - INFO - y batch shape : torch.Size([256, 200, 1, 11])
2025-02-27 18:29:29 - INFO - Logging started
2025-02-27 18:30:04 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:30:04 - INFO - Future price shape : (200, 1)
2025-02-27 18:30:04 - INFO - Future Prices : [[ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-2]
 [-3]
 [-3]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-2]
 [-2]
 [-3]
 [-4]
 [-4]
 [-3]
 [-3]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-3]
 [-4]
 [-4]
 [-4]
 [-4]
 [-4]
 [-3]
 [-3]
 [-3]
 [-2]
 [-1]
 [-1]
 [ 0]
 [-1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]]
2025-02-27 18:30:04 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:30:04 - INFO - Previous Slope shape : (188,)
2025-02-27 18:30:04 - INFO - prev slope : 7478    1
7479    1
7480    1
7481    1
7482    0
7483   -1
7484   -3
7485   -3
7486   -3
7487   -2
7488   -2
7489   -1
7490   -1
7491    0
7492    0
7493    0
7494    0
7495    0
7496    0
7497    0
7498    0
7499    1
7500    1
7501    1
7502    2
7503    1
7504    1
7505    0
7506    0
7507    0
7508    0
7509    0
7510    0
7511    0
7512    0
7513    0
7514    0
7515   -1
7516   -1
7517   -1
7518   -1
7519    0
7520    0
7521    0
7522    0
7523    0
7524    0
7525    1
7526    1
7527    1
7528    1
7529    1
7530    0
7531    1
7532    1
7533    0
7534    0
7535    0
7536    0
7537    1
7538    1
7539    1
7540    1
7541    1
7542    1
7543    1
7544    1
7545    0
7546    0
7547   -1
7548   -1
7549   -1
7550   -2
7551   -2
7552   -2
7553   -2
7554   -2
7555   -2
7556   -1
7557   -1
7558    0
7559    1
7560    1
7561    1
7562    1
7563    1
7564    1
7565    1
7566    1
7567    0
7568    0
7569    0
7570    0
7571    0
7572    0
7573    0
7574    0
7575    0
7576    0
7577    0
7578    0
7579    0
7580    0
7581    1
7582    1
7583    1
7584    0
7585    0
7586    0
7587    0
7588   -1
7589   -1
7590   -1
7591   -1
7592   -2
7593   -3
7594   -3
7595   -3
7596   -3
7597   -3
7598   -2
7599   -2
7600   -1
7601    0
7602    0
7603   -1
7604   -1
7605   -1
7606   -2
7607   -2
7608   -2
7609   -2
7610   -2
7611   -2
7612   -1
7613   -2
7614   -2
7615   -3
7616   -4
7617   -4
7618   -3
7619   -3
7620   -2
7621   -1
7622   -1
7623    0
7624    0
7625    0
7626    0
7627    1
7628    1
7629    1
7630    1
7631    0
7632    0
7633    0
7634   -1
7635   -1
7636   -3
7637   -4
7638   -4
7639   -4
7640   -4
7641   -4
7642   -3
7643   -3
7644   -3
7645   -2
7646   -1
7647   -1
7648    0
7649   -1
7650    0
7651   -1
7652   -1
7653   -1
7654    0
7655    0
7656    0
7657    1
7658    1
7659    1
7660    1
7661    0
7662    0
7663    0
7664    0
7665   -1
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:30:07 - INFO - y batch shape : torch.Size([256, 200, 1, 11])
2025-02-27 18:30:07 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:30:07 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:30:07 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:30:07 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:30:08 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:30:08 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:30:08 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:30:08 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-27 18:30:08 - INFO - Output shape : torch.Size([256, 188, 11])
2025-02-27 18:30:08 - INFO - Output shape after view : torch.Size([256, 188, 1, 11])
2025-02-27 18:30:08 - INFO - Output shape : torch.Size([256, 188, 1, 11])
2025-02-27 18:30:08 - INFO - Output shape after view : torch.Size([48128, 11])
2025-02-27 18:30:08 - INFO - y batch shape after view : torch.Size([51200, 11])
2025-02-27 18:30:08 - INFO - y batch shape after argmax : torch.Size([51200])
2025-02-27 18:37:25 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:37:25 - INFO - Future price shape : (200, 1)
2025-02-27 18:37:25 - INFO - Future Prices : [[ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 3]
 [ 3]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-2]
 [-3]
 [-4]
 [-5]
 [-5]
 [-4]
 [-4]
 [-3]
 [-2]
 [-1]
 [ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 4]
 [ 4]
 [ 4]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-3]
 [-3]
 [-3]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-2]
 [-3]
 [-3]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [-1]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 3]
 [ 4]
 [ 3]
 [ 4]]
2025-02-27 18:37:25 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:37:25 - INFO - Previous Slope shape : (188,)
2025-02-27 18:37:25 - INFO - prev slope : 4059    1
4060    1
4061    1
4062    2
4063    3
4064    3
4065    3
4066    2
4067    2
4068    3
4069    3
4070    2
4071    1
4072    1
4073    0
4074   -1
4075   -2
4076   -3
4077   -4
4078   -5
4079   -5
4080   -4
4081   -4
4082   -3
4083   -2
4084   -1
4085    1
4086    2
4087    3
4088    4
4089    4
4090    4
4091    4
4092    3
4093    3
4094    2
4095    2
4096    1
4097    0
4098   -1
4099   -1
4100   -2
4101   -2
4102   -2
4103   -1
4104   -1
4105   -1
4106   -1
4107   -1
4108   -1
4109    0
4110    0
4111    0
4112    0
4113    0
4114    0
4115    0
4116    1
4117    1
4118    1
4119    1
4120    1
4121    2
4122    2
4123    2
4124    2
4125    1
4126    1
4127    0
4128    0
4129    0
4130   -1
4131   -1
4132   -2
4133   -2
4134   -2
4135   -3
4136   -3
4137   -3
4138   -2
4139   -2
4140   -2
4141   -1
4142   -1
4143    0
4144    1
4145    1
4146    1
4147    1
4148    1
4149    0
4150    0
4151   -1
4152   -2
4153   -3
4154   -3
4155   -2
4156   -2
4157   -1
4158    0
4159    1
4160    1
4161    1
4162    1
4163    1
4164    1
4165    0
4166    0
4167    0
4168    0
4169    0
4170    1
4171    1
4172    2
4173    2
4174    2
4175    1
4176    1
4177    0
4178   -1
4179   -1
4180   -1
4181   -1
4182   -1
4183    0
4184    0
4185    1
4186    2
4187    2
4188    2
4189    3
4190    3
4191    3
4192    3
4193    3
4194    3
4195    2
4196    2
4197    3
4198    3
4199    3
4200    2
4201    2
4202    2
4203    2
4204    1
4205    1
4206    1
4207    0
4208    0
4209    0
4210    1
4211    1
4212    2
4213    2
4214    3
4215    3
4216    2
4217    2
4218    2
4219    1
4220    0
4221   -1
4222   -2
4223   -2
4224   -2
4225   -1
4226   -1
4227   -1
4228    0
4229    0
4230    0
4231    0
4232    0
4233   -1
4234   -1
4235   -1
4236   -2
4237   -2
4238   -2
4239   -2
4240   -2
4241   -1
4242   -1
4243   -1
4244   -1
4245   -1
4246   -1
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:37:29 - INFO - y batch shape : torch.Size([256, 200, 1, 11])
2025-02-27 18:37:29 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:37:29 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:37:29 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:37:29 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:37:31 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:37:31 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:37:31 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:38:52 - INFO - Logging started
2025-02-27 18:39:27 - INFO - Full sequence shape : (212, 80)
2025-02-27 18:39:27 - INFO - Future price shape : (200, 1)
2025-02-27 18:39:27 - INFO - Future Prices : [[ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 2]
 [ 3]
 [ 5]
 [ 5]
 [ 5]
 [ 5]
 [ 4]
 [ 3]
 [ 1]
 [-1]
 [-2]
 [-2]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 5]
 [ 4]
 [ 4]
 [ 4]
 [ 3]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 3]
 [ 1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-2]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 2]
 [ 3]
 [ 3]
 [ 4]
 [ 4]
 [ 4]
 [ 5]
 [ 5]
 [ 4]
 [ 3]
 [ 3]
 [ 3]
 [ 2]
 [ 2]
 [ 1]
 [ 0]
 [-1]
 [ 0]
 [ 0]
 [-1]
 [-1]
 [-2]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 0]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 0]
 [ 1]
 [ 0]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 1]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 1]
 [ 0]
 [ 1]
 [ 0]
 [ 1]
 [ 2]
 [ 3]
 [ 3]
 [ 4]
 [ 4]
 [ 3]
 [ 2]
 [ 0]
 [-1]
 [-2]
 [-3]
 [-4]
 [-4]
 [-4]
 [-4]
 [-4]
 [-3]
 [-2]
 [-2]
 [-1]
 [ 0]
 [ 0]
 [ 1]
 [ 2]
 [ 2]
 [ 1]
 [ 1]
 [ 0]
 [-1]
 [-1]
 [-1]
 [-1]
 [-1]
 [ 0]
 [ 0]]
2025-02-27 18:39:27 - INFO - Future Prices Onehot shape : (200, 1, 11)
2025-02-27 18:39:27 - INFO - Previous Slope shape : (188,)
2025-02-27 18:39:27 - INFO - prev slope : 533    1
534    1
535    2
536    2
537    1
538    0
539   -1
540   -1
541   -1
542   -1
543    0
544    2
545    3
546    5
547    5
548    5
549    5
550    4
551    3
552    1
553   -1
554   -2
555   -2
556   -1
557   -1
558   -1
559   -1
560    0
561    1
562    0
563    1
564    1
565    1
566    1
567    2
568    3
569    4
570    5
571    5
572    4
573    4
574    4
575    3
576    2
577    2
578    1
579    1
580    1
581    1
582   -1
583   -2
584   -2
585   -2
586   -2
587   -1
588    0
589    0
590    1
591    1
592    2
593    3
594    1
595    0
596    0
597   -1
598   -1
599   -1
600   -1
601   -1
602   -2
603   -2
604   -2
605   -2
606   -2
607   -2
608   -2
609   -2
610   -1
611   -1
612    0
613    0
614    0
615    0
616    0
617    1
618    2
619    3
620    3
621    4
622    4
623    4
624    5
625    5
626    4
627    3
628    3
629    3
630    2
631    2
632    1
633    0
634   -1
635    0
636    0
637   -1
638   -1
639   -2
640   -1
641   -1
642   -1
643    0
644    0
645    0
646    1
647    1
648    2
649    2
650    1
651    1
652    1
653    1
654    1
655    1
656    1
657    1
658    0
659    0
660    0
661    0
662    0
663    1
664    1
665    2
666    2
667    2
668    2
669    2
670    2
671    1
672    1
673    1
674    0
675    0
676    1
677    0
678    1
679    1
680    1
681    1
682    1
683    1
684    1
685    1
686    2
687    2
688    2
689    2
690    2
691    1
692    1
693    1
694    0
695    1
696    0
697    1
698    2
699    3
700    3
701    4
702    4
703    3
704    2
705    0
706   -1
707   -2
708   -3
709   -4
710   -4
711   -4
712   -4
713   -4
714   -3
715   -2
716   -2
717   -1
718    0
719    0
720    1
Name: Close_Slope_Bin, dtype: category
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 18:39:31 - INFO - y batch shape : torch.Size([256, 200, 1, 11])
2025-02-27 18:39:31 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 18:39:31 - INFO - Decoder Input shape : torch.Size([256, 188, 1])
2025-02-27 18:39:31 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 18:39:31 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 18:39:31 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 18:39:31 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 18:39:31 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 18:39:31 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-27 18:39:32 - INFO - Output shape : torch.Size([256, 188, 11])
2025-02-27 22:42:51 - INFO - Full sequence shape : (213, 80)
2025-02-27 22:42:51 - INFO - Decoder sequence length : 188
2025-02-27 22:42:51 - INFO - Slope value shape : (188,)
2025-02-27 22:53:09 - INFO - Full sequence shape : (213, 80)
2025-02-27 22:53:09 - INFO - Decoder sequence length : 188
2025-02-27 22:53:09 - INFO - Slope value shape : (188,)
2025-02-27 22:56:28 - INFO - Full sequence shape : (213, 80)
2025-02-27 22:56:28 - INFO - Decoder sequence length : 188
2025-02-27 22:56:28 - INFO - Slope value shape : (188,)
2025-02-27 23:02:05 - INFO - Full sequence shape : (213, 80)
2025-02-27 23:02:05 - INFO - Decoder sequence length : 188
2025-02-27 23:02:05 - INFO - Slope value shape : (188,)
2025-02-27 23:02:05 - INFO - slope classes : tensor([11, 10, 10, 10, 10, 10, 10,  9,  9,  8,  8,  9,  9, 10, 11, 11, 11, 12,
        12, 12, 12, 11, 11, 11, 11, 11, 12, 11, 11, 11, 11, 11, 10, 10, 10, 11,
        11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11,
        12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 10, 10, 10, 11, 11, 12, 12, 11,
        10,  9,  9,  9,  9, 10, 12, 13, 15, 15, 15, 15, 14, 13, 11,  9,  8,  8,
         9,  9,  9,  9, 10, 11, 10, 11, 11, 11, 11, 12, 13, 14, 15, 15, 14, 14,
        14, 13, 12, 12, 11, 11, 11, 11,  9,  8,  8,  8,  8,  9, 10, 10, 11, 11,
        12, 13, 11, 10, 10,  9,  9,  9,  9,  9,  8,  8,  8,  8,  8,  8,  8,  8,
         9,  9, 10, 10, 10, 10, 10, 11, 12, 13, 13, 14, 14, 14, 15, 15, 14, 13,
        13, 13, 12, 12, 11, 10,  9, 10, 10,  9,  9,  8,  9,  9,  9, 10, 10, 10,
        11, 11, 12, 12, 11, 11, 11, 11])
2025-02-27 23:10:24 - INFO - Full sequence shape : (213, 80)
2025-02-27 23:10:24 - INFO - Decoder sequence length : 188
2025-02-27 23:10:24 - INFO - Slope value shape : (188,)
2025-02-27 23:10:24 - INFO - Slope values : [ 6  6  6  6  6  6  6  6  5  5  4  4  4  4  5  5  5  6  6  7  7  6  6  5
  5  4  4  3  3  2  2  3  3  4  4  5  5  6  6  7  7  7  7  7  7  8  8  9
 10 10 10 10 10 10 10 10 10  6  2  0  0  0  0  0  0  1  4  4  4  3  2  1
  0  0  1  2  3  5  7  8  9  8  8  8  7  7  7  8  9 10 10 10 10  9  8  7
  6  5  5  4  4  4  5  5  6  7  7  7  7  7  7  7  6  5  5  5  5  5  5  6
  6  5  5  4  4  4  3  3  3  3  3  4  5  5  5  6  6  6  6  5  6  6  6  6
  6  7  7  8  8  8  8  7  6  6  5  4  4  3  2  2  3  3  4  5  5  6  6  7
  7  7  7  7  7  7  8  8  7  7  7  7  7  7  6  6  6  5  5  4]
2025-02-27 23:10:24 - INFO - Target bin max : 5
2025-02-27 23:10:24 - INFO - slope classes : tensor([11, 11, 11, 11, 11, 11, 11, 11, 10, 10,  9,  9,  9,  9, 10, 10, 10, 11,
        11, 12, 12, 11, 11, 10, 10,  9,  9,  8,  8,  7,  7,  8,  8,  9,  9, 10,
        10, 11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 14, 15, 15, 15, 15, 15, 15,
        15, 15, 15, 11,  7,  5,  5,  5,  5,  5,  5,  6,  9,  9,  9,  8,  7,  6,
         5,  5,  6,  7,  8, 10, 12, 13, 14, 13, 13, 13, 12, 12, 12, 13, 14, 15,
        15, 15, 15, 14, 13, 12, 11, 10, 10,  9,  9,  9, 10, 10, 11, 12, 12, 12,
        12, 12, 12, 12, 11, 10, 10, 10, 10, 10, 10, 11, 11, 10, 10,  9,  9,  9,
         8,  8,  8,  8,  8,  9, 10, 10, 10, 11, 11, 11, 11, 10, 11, 11, 11, 11,
        11, 12, 12, 13, 13, 13, 13, 12, 11, 11, 10,  9,  9,  8,  7,  7,  8,  8,
         9, 10, 10, 11, 11, 12, 12, 12, 12, 12, 12, 12, 13, 13, 12, 12, 12, 12,
        12, 12, 11, 11, 11, 10, 10,  9])
2025-02-27 23:13:05 - INFO - Full sequence shape : (213, 80)
2025-02-27 23:13:05 - INFO - Decoder sequence length : 188
2025-02-27 23:13:05 - INFO - Slope value shape : (188,)
2025-02-27 23:13:05 - INFO - Slope values : [1, 2, 2, 2, 2, ..., 4, 3, 2, 0, -1]
Length: 188
Categories (11, int64): [-5, -4, -3, -2, ..., 2, 3, 4, 5]
2025-02-27 23:13:05 - INFO - Target bin max : 5
2025-02-27 23:19:05 - INFO - Full sequence shape : (213, 80)
2025-02-27 23:19:05 - INFO - Decoder sequence length : 188
2025-02-27 23:19:05 - INFO - Slope value shape : (188,)
2025-02-27 23:19:05 - INFO - Slope values : [ 1  1  0  0  0  0  1  1  1  2  2  1  1  1  0  0  0  0  0  1  1  2  2  2
  3  2  1  1  0  0  0 -1 -1 -1 -2 -2 -2 -2 -2 -2 -2 -1 -1  0  0  1  1  1
  1  0 -1 -2 -2 -2 -3 -3 -2 -1  0  1  2  2  2  2  2  2  1  1  1  0  0 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1
  0  0  0  1  1  1  1  2  2  2  2  2  1  1  0  0  0  0  1  1  2  2  2  2
  2  2  2  1  1  0  0 -1 -1 -1 -2 -2 -2 -2 -1 -1 -1  0  0  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  1  1  1  1  0  0 -1
 -1 -2 -1 -1 -1 -1  0  0  1  1  1  2  2  2  2  2  2  2  2  2]
2025-02-27 23:19:05 - INFO - Target bin max : 5
2025-02-27 23:19:05 - INFO - slope classes : tensor([6, 6, 5, 5, 5, 5, 6, 6, 6, 7, 7, 6, 6, 6, 5, 5, 5, 5, 5, 6, 6, 7, 7, 7,
        8, 7, 6, 6, 5, 5, 5, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 6, 6, 6,
        6, 5, 4, 3, 3, 3, 2, 2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 6, 6, 6, 5, 5, 4,
        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,
        5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 6, 6, 5, 5, 5, 5, 6, 6, 7, 7, 7, 7,
        7, 7, 7, 6, 6, 5, 5, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 5, 5, 4,
        4, 3, 4, 4, 4, 4, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7])
2025-02-27 23:19:05 - INFO - Slope tensor shape : torch.Size([188, 11])
2025-02-27 23:19:05 - INFO - Future price : 0
2025-02-27 23:19:05 - INFO - X shape : torch.Size([200, 51])
2025-02-27 23:19:05 - INFO - y shape : torch.Size([11])
2025-02-27 23:19:05 - INFO - y : tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])
2025-02-27 23:19:05 - INFO - y batch shape : torch.Size([256, 11])
2025-02-27 23:19:05 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-27 23:19:05 - INFO - Decoder Input shape : torch.Size([256, 188, 11])
2025-02-27 23:19:05 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-27 23:19:05 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-27 23:19:07 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-27 23:19:07 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-27 23:19:07 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-27 23:19:15 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-27 23:19:15 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-27 23:19:15 - INFO - Output shape : torch.Size([256, 11])
2025-02-27 23:19:15 - INFO - Output s : tensor([[ 0.7733,  1.9975, -2.0727,  ..., -0.5679, -1.3434,  1.0128],
        [-1.4051,  2.4111, -2.8790,  ...,  1.2058, -0.3690,  1.1593],
        [ 0.2135,  1.7528, -0.5914,  ...,  1.1754, -0.3235, -0.6868],
        ...,
        [ 0.7899,  1.1744, -1.5440,  ...,  1.2951, -0.8314,  0.2719],
        [ 0.8995,  2.3413, -1.0288,  ...,  2.5435,  1.4821, -1.3052],
        [ 2.5349,  1.6896, -1.6555,  ...,  1.0533,  1.5372,  0.5485]],
       device='mps:0', grad_fn=<LinearBackward0>)
2025-02-27 23:19:15 - INFO - Output shape : torch.Size([256, 11])
2025-02-27 23:19:15 - INFO - Output shape after view : torch.Size([256, 11])
2025-02-27 23:19:15 - INFO - y batch shape after view : torch.Size([256, 11])
2025-02-27 23:19:20 - INFO - Epoch : 1 , Batch [ 0 / 274 ] : Loss = 3.673310, Accuracy = 8.20%, MSE = 16.0195
2025-02-27 23:20:15 - INFO - Epoch : 1 , Batch [ 10 / 274 ] : Loss = 2.601182, Accuracy = 14.74%, MSE = 11.1087
2025-02-27 23:21:04 - INFO - Epoch : 1 , Batch [ 20 / 274 ] : Loss = 2.289024, Accuracy = 17.11%, MSE = 8.3129
2025-02-27 23:21:54 - INFO - Epoch : 1 , Batch [ 30 / 274 ] : Loss = 2.121520, Accuracy = 19.10%, MSE = 7.3625
2025-02-27 23:22:45 - INFO - Epoch : 1 , Batch [ 40 / 274 ] : Loss = 2.139085, Accuracy = 18.97%, MSE = 6.9399
2025-02-27 23:23:35 - INFO - Epoch : 1 , Batch [ 50 / 274 ] : Loss = 2.122232, Accuracy = 19.80%, MSE = 6.5966
2025-02-27 23:24:26 - INFO - Epoch : 1 , Batch [ 60 / 274 ] : Loss = 1.979952, Accuracy = 20.54%, MSE = 6.4235
2025-02-27 23:25:15 - INFO - Epoch : 1 , Batch [ 70 / 274 ] : Loss = 2.094305, Accuracy = 20.74%, MSE = 6.3006
2025-02-27 23:26:05 - INFO - Epoch : 1 , Batch [ 80 / 274 ] : Loss = 2.064741, Accuracy = 21.04%, MSE = 6.1491
2025-02-27 23:26:54 - INFO - Epoch : 1 , Batch [ 90 / 274 ] : Loss = 2.122792, Accuracy = 21.45%, MSE = 6.0569
2025-02-27 23:27:43 - INFO - Epoch : 1 , Batch [ 100 / 274 ] : Loss = 2.115958, Accuracy = 21.40%, MSE = 6.0548
2025-02-27 23:28:31 - INFO - Epoch : 1 , Batch [ 110 / 274 ] : Loss = 2.042853, Accuracy = 21.59%, MSE = 6.0185
2025-02-27 23:29:22 - INFO - Epoch : 1 , Batch [ 120 / 274 ] : Loss = 2.063997, Accuracy = 21.86%, MSE = 5.9647
2025-02-27 23:30:12 - INFO - Epoch : 1 , Batch [ 130 / 274 ] : Loss = 2.231676, Accuracy = 21.81%, MSE = 5.9542
2025-02-27 23:31:00 - INFO - Epoch : 1 , Batch [ 140 / 274 ] : Loss = 2.161940, Accuracy = 21.87%, MSE = 5.9594
2025-02-27 23:31:48 - INFO - Epoch : 1 , Batch [ 150 / 274 ] : Loss = 2.029031, Accuracy = 22.17%, MSE = 5.9158
2025-02-27 23:32:39 - INFO - Epoch : 1 , Batch [ 160 / 274 ] : Loss = 2.056640, Accuracy = 22.37%, MSE = 5.8757
2025-02-27 23:33:33 - INFO - Epoch : 1 , Batch [ 170 / 274 ] : Loss = 1.982526, Accuracy = 22.53%, MSE = 5.8264
2025-02-27 23:34:18 - INFO - Epoch : 1 , Batch [ 180 / 274 ] : Loss = 1.937086, Accuracy = 22.78%, MSE = 5.7589
2025-02-27 23:35:04 - INFO - Epoch : 1 , Batch [ 190 / 274 ] : Loss = 2.086035, Accuracy = 22.94%, MSE = 5.7473
2025-02-27 23:35:49 - INFO - Epoch : 1 , Batch [ 200 / 274 ] : Loss = 1.897213, Accuracy = 23.08%, MSE = 5.7137
2025-02-27 23:36:36 - INFO - Epoch : 1 , Batch [ 210 / 274 ] : Loss = 1.845768, Accuracy = 23.52%, MSE = 5.6519
2025-02-27 23:37:19 - INFO - Epoch : 1 , Batch [ 220 / 274 ] : Loss = 2.088334, Accuracy = 23.84%, MSE = 5.6278
2025-02-27 23:38:04 - INFO - Epoch : 1 , Batch [ 230 / 274 ] : Loss = 1.533470, Accuracy = 24.41%, MSE = 5.5816
2025-02-27 23:38:49 - INFO - Epoch : 1 , Batch [ 240 / 274 ] : Loss = 1.005160, Accuracy = 25.87%, MSE = 5.4793
2025-02-27 23:39:36 - INFO - Epoch : 1 , Batch [ 250 / 274 ] : Loss = 2.495316, Accuracy = 27.18%, MSE = 5.3814
2025-02-27 23:40:19 - INFO - Epoch : 1 , Batch [ 260 / 274 ] : Loss = 1.408523, Accuracy = 27.66%, MSE = 5.3797
2025-02-27 23:41:07 - INFO - Epoch : 1 , Batch [ 270 / 274 ] : Loss = 1.178745, Accuracy = 28.70%, MSE = 5.2677
2025-02-27 23:41:20 - INFO - Epoch 2: Train Loss=1.9899, Train Acc=28.93%, Train MSE=5.2255
2025-02-27 23:42:21 - INFO - Epoch 2: Val Loss=2.3664, Val Acc=3.31%, Val MSE=512.8807
2025-02-27 23:42:25 - INFO - Epoch : 2 , Batch [ 0 / 274 ] : Loss = 3.087433, Accuracy = 14.45%, MSE = 10.3320
2025-02-27 23:43:24 - INFO - Epoch : 2 , Batch [ 10 / 274 ] : Loss = 2.167457, Accuracy = 20.53%, MSE = 7.1026
2025-02-27 23:44:24 - INFO - Epoch : 2 , Batch [ 20 / 274 ] : Loss = 2.024725, Accuracy = 22.75%, MSE = 6.0195
2025-02-27 23:45:32 - INFO - Epoch : 2 , Batch [ 30 / 274 ] : Loss = 1.986255, Accuracy = 24.16%, MSE = 5.6409
2025-02-27 23:46:40 - INFO - Epoch : 2 , Batch [ 40 / 274 ] : Loss = 1.985800, Accuracy = 23.78%, MSE = 5.4931
2025-02-27 23:47:47 - INFO - Epoch : 2 , Batch [ 50 / 274 ] : Loss = 1.958228, Accuracy = 24.17%, MSE = 5.2822
2025-02-27 23:48:49 - INFO - Epoch : 2 , Batch [ 60 / 274 ] : Loss = 1.818178, Accuracy = 24.74%, MSE = 5.1660
2025-02-27 23:49:56 - INFO - Epoch : 2 , Batch [ 70 / 274 ] : Loss = 1.982615, Accuracy = 24.75%, MSE = 5.1415
2025-02-27 23:54:51 - INFO - Epoch : 2 , Batch [ 80 / 274 ] : Loss = 1.839007, Accuracy = 25.09%, MSE = 5.0608
2025-02-27 23:55:27 - INFO - Epoch : 2 , Batch [ 90 / 274 ] : Loss = 1.853709, Accuracy = 25.91%, MSE = 5.0256
2025-02-27 23:57:16 - INFO - Epoch : 2 , Batch [ 100 / 274 ] : Loss = 2.067408, Accuracy = 25.71%, MSE = 5.0919
2025-02-27 23:57:57 - INFO - Epoch : 2 , Batch [ 110 / 274 ] : Loss = 1.818902, Accuracy = 26.09%, MSE = 5.0229
2025-02-27 23:58:37 - INFO - Epoch : 2 , Batch [ 120 / 274 ] : Loss = 1.687288, Accuracy = 26.91%, MSE = 5.0055
2025-02-27 23:59:17 - INFO - Epoch : 2 , Batch [ 130 / 274 ] : Loss = 2.082736, Accuracy = 26.95%, MSE = 5.0717
2025-02-28 00:00:04 - INFO - Epoch : 2 , Batch [ 140 / 274 ] : Loss = 1.866885, Accuracy = 27.20%, MSE = 5.0976
2025-02-28 00:00:58 - INFO - Epoch : 2 , Batch [ 150 / 274 ] : Loss = 1.546658, Accuracy = 27.88%, MSE = 5.0856
2025-02-28 00:01:45 - INFO - Epoch : 2 , Batch [ 160 / 274 ] : Loss = 1.894660, Accuracy = 28.20%, MSE = 5.1255
2025-02-28 00:02:39 - INFO - Epoch : 2 , Batch [ 170 / 274 ] : Loss = 1.587116, Accuracy = 28.78%, MSE = 5.0432
2025-02-28 00:03:30 - INFO - Epoch : 2 , Batch [ 180 / 274 ] : Loss = 1.355694, Accuracy = 29.81%, MSE = 4.8981
2025-02-28 00:04:20 - INFO - Epoch : 2 , Batch [ 190 / 274 ] : Loss = 1.956841, Accuracy = 30.43%, MSE = 4.8346
2025-02-28 00:05:12 - INFO - Epoch : 2 , Batch [ 200 / 274 ] : Loss = 1.431800, Accuracy = 31.08%, MSE = 4.7944
2025-02-28 00:06:05 - INFO - Epoch : 2 , Batch [ 210 / 274 ] : Loss = 1.082713, Accuracy = 32.32%, MSE = 4.7036
2025-02-28 00:06:58 - INFO - Epoch : 2 , Batch [ 220 / 274 ] : Loss = 1.635414, Accuracy = 33.23%, MSE = 4.6585
2025-02-28 00:07:53 - INFO - Epoch : 2 , Batch [ 230 / 274 ] : Loss = 0.704114, Accuracy = 34.60%, MSE = 4.5397
2025-02-28 00:08:56 - INFO - Epoch : 2 , Batch [ 240 / 274 ] : Loss = 0.521297, Accuracy = 36.41%, MSE = 4.3801
2025-02-28 00:09:47 - INFO - Epoch : 2 , Batch [ 250 / 274 ] : Loss = 2.385503, Accuracy = 37.58%, MSE = 4.3107
2025-02-28 00:10:35 - INFO - Epoch : 2 , Batch [ 260 / 274 ] : Loss = 0.997912, Accuracy = 38.09%, MSE = 4.2600
2025-02-28 00:11:28 - INFO - Epoch : 2 , Batch [ 270 / 274 ] : Loss = 0.834233, Accuracy = 39.12%, MSE = 4.1620
2025-02-28 00:11:42 - INFO - Epoch 3: Train Loss=1.6739, Train Acc=39.37%, Train MSE=4.1224
2025-02-28 00:12:41 - INFO - Epoch 3: Val Loss=2.7090, Val Acc=3.75%, Val MSE=510.4181
2025-02-28 00:12:46 - INFO - Epoch : 3 , Batch [ 0 / 274 ] : Loss = 3.099797, Accuracy = 18.75%, MSE = 8.7539
2025-02-28 00:13:44 - INFO - Epoch : 3 , Batch [ 10 / 274 ] : Loss = 2.116945, Accuracy = 21.02%, MSE = 6.1932
2025-02-28 00:14:35 - INFO - Epoch : 3 , Batch [ 20 / 274 ] : Loss = 2.000936, Accuracy = 23.85%, MSE = 5.4191
2025-02-28 00:15:36 - INFO - Epoch : 3 , Batch [ 30 / 274 ] : Loss = 1.914007, Accuracy = 25.42%, MSE = 5.1210
2025-02-28 00:16:33 - INFO - Epoch : 3 , Batch [ 40 / 274 ] : Loss = 1.998451, Accuracy = 24.45%, MSE = 5.0678
2025-02-28 00:17:25 - INFO - Epoch : 3 , Batch [ 50 / 274 ] : Loss = 1.872909, Accuracy = 24.81%, MSE = 4.9337
2025-02-28 00:18:27 - INFO - Epoch : 3 , Batch [ 60 / 274 ] : Loss = 1.751928, Accuracy = 25.52%, MSE = 4.7950
2025-02-28 00:19:20 - INFO - Epoch : 3 , Batch [ 70 / 274 ] : Loss = 1.931399, Accuracy = 25.68%, MSE = 4.7716
2025-02-28 00:20:17 - INFO - Epoch : 3 , Batch [ 80 / 274 ] : Loss = 1.727328, Accuracy = 26.11%, MSE = 4.7003
2025-02-28 00:21:09 - INFO - Epoch : 3 , Batch [ 90 / 274 ] : Loss = 1.820702, Accuracy = 26.91%, MSE = 4.6496
2025-02-28 00:21:58 - INFO - Epoch : 3 , Batch [ 100 / 274 ] : Loss = 1.999181, Accuracy = 26.71%, MSE = 4.7280
2025-02-28 00:22:54 - INFO - Epoch : 3 , Batch [ 110 / 274 ] : Loss = 1.739758, Accuracy = 27.01%, MSE = 4.6693
2025-02-28 00:23:52 - INFO - Epoch : 3 , Batch [ 120 / 274 ] : Loss = 1.707628, Accuracy = 27.72%, MSE = 4.6261
2025-02-28 00:24:49 - INFO - Epoch : 3 , Batch [ 130 / 274 ] : Loss = 2.053037, Accuracy = 27.68%, MSE = 4.6950
2025-02-28 00:25:44 - INFO - Epoch : 3 , Batch [ 140 / 274 ] : Loss = 1.726883, Accuracy = 27.99%, MSE = 4.7617
2025-02-28 00:26:41 - INFO - Epoch : 3 , Batch [ 150 / 274 ] : Loss = 1.534048, Accuracy = 28.76%, MSE = 4.7405
2025-02-28 00:27:38 - INFO - Epoch : 3 , Batch [ 160 / 274 ] : Loss = 1.952509, Accuracy = 29.20%, MSE = 4.7309
2025-02-28 00:28:40 - INFO - Epoch : 3 , Batch [ 170 / 274 ] : Loss = 1.441540, Accuracy = 29.87%, MSE = 4.6322
2025-02-28 00:29:30 - INFO - Epoch : 3 , Batch [ 180 / 274 ] : Loss = 1.224454, Accuracy = 31.09%, MSE = 4.5014
2025-02-28 00:30:24 - INFO - Epoch : 3 , Batch [ 190 / 274 ] : Loss = 1.948015, Accuracy = 31.85%, MSE = 4.4525
2025-02-28 00:31:26 - INFO - Epoch : 3 , Batch [ 200 / 274 ] : Loss = 1.175479, Accuracy = 32.73%, MSE = 4.3965
2025-02-28 00:32:27 - INFO - Epoch : 3 , Batch [ 210 / 274 ] : Loss = 0.871849, Accuracy = 34.21%, MSE = 4.2805
2025-02-28 00:33:26 - INFO - Epoch : 3 , Batch [ 220 / 274 ] : Loss = 1.582930, Accuracy = 35.22%, MSE = 4.2109
2025-02-28 00:34:22 - INFO - Epoch : 3 , Batch [ 230 / 274 ] : Loss = 0.749018, Accuracy = 36.44%, MSE = 4.0917
2025-02-28 00:35:17 - INFO - Epoch : 3 , Batch [ 240 / 274 ] : Loss = 0.511800, Accuracy = 38.19%, MSE = 3.9449
2025-02-28 00:36:12 - INFO - Epoch : 3 , Batch [ 250 / 274 ] : Loss = 2.183296, Accuracy = 39.42%, MSE = 3.8775
2025-02-28 00:37:03 - INFO - Epoch : 3 , Batch [ 260 / 274 ] : Loss = 1.091853, Accuracy = 39.83%, MSE = 3.8376
2025-02-28 00:37:58 - INFO - Epoch : 3 , Batch [ 270 / 274 ] : Loss = 0.855338, Accuracy = 40.83%, MSE = 3.7345
2025-02-28 00:38:12 - INFO - Epoch 4: Train Loss=1.6160, Train Acc=41.07%, Train MSE=3.7024
2025-02-28 00:39:11 - INFO - Epoch 4: Val Loss=2.7280, Val Acc=3.24%, Val MSE=570.8510
2025-02-28 00:39:17 - INFO - Epoch : 4 , Batch [ 0 / 274 ] : Loss = 3.350610, Accuracy = 15.62%, MSE = 12.3672
2025-02-28 00:40:16 - INFO - Epoch : 4 , Batch [ 10 / 274 ] : Loss = 2.191321, Accuracy = 18.93%, MSE = 6.7905
2025-02-28 00:41:11 - INFO - Epoch : 4 , Batch [ 20 / 274 ] : Loss = 2.053375, Accuracy = 21.58%, MSE = 5.5670
2025-02-28 00:42:06 - INFO - Epoch : 4 , Batch [ 30 / 274 ] : Loss = 1.961652, Accuracy = 22.88%, MSE = 5.2300
2025-02-28 00:43:04 - INFO - Epoch : 4 , Batch [ 40 / 274 ] : Loss = 2.007982, Accuracy = 23.07%, MSE = 5.0624
2025-02-28 00:44:01 - INFO - Epoch : 4 , Batch [ 50 / 274 ] : Loss = 1.945967, Accuracy = 23.73%, MSE = 4.8644
2025-02-28 00:44:59 - INFO - Epoch : 4 , Batch [ 60 / 274 ] : Loss = 1.782394, Accuracy = 24.64%, MSE = 4.7259
2025-02-28 00:45:59 - INFO - Epoch : 4 , Batch [ 70 / 274 ] : Loss = 1.962524, Accuracy = 25.13%, MSE = 4.7008
2025-02-28 00:46:52 - INFO - Epoch : 4 , Batch [ 80 / 274 ] : Loss = 1.824452, Accuracy = 25.64%, MSE = 4.6469
2025-02-28 00:47:46 - INFO - Epoch : 4 , Batch [ 90 / 274 ] : Loss = 1.905584, Accuracy = 26.66%, MSE = 4.5704
2025-02-28 00:48:35 - INFO - Epoch : 4 , Batch [ 100 / 274 ] : Loss = 1.965708, Accuracy = 26.51%, MSE = 4.6276
2025-02-28 00:49:39 - INFO - Epoch : 4 , Batch [ 110 / 274 ] : Loss = 1.866091, Accuracy = 26.60%, MSE = 4.5842
2025-02-28 00:50:41 - INFO - Epoch : 4 , Batch [ 120 / 274 ] : Loss = 1.768419, Accuracy = 27.08%, MSE = 4.4979
2025-02-28 00:51:44 - INFO - Epoch : 4 , Batch [ 130 / 274 ] : Loss = 1.984306, Accuracy = 27.24%, MSE = 4.5251
2025-02-28 00:52:36 - INFO - Epoch : 4 , Batch [ 140 / 274 ] : Loss = 1.706369, Accuracy = 27.74%, MSE = 4.5775
2025-02-28 00:53:28 - INFO - Epoch : 4 , Batch [ 150 / 274 ] : Loss = 1.437193, Accuracy = 28.63%, MSE = 4.5309
2025-02-28 00:54:22 - INFO - Epoch : 4 , Batch [ 160 / 274 ] : Loss = 1.811479, Accuracy = 29.16%, MSE = 4.5141
2025-02-28 00:55:22 - INFO - Epoch : 4 , Batch [ 170 / 274 ] : Loss = 1.466209, Accuracy = 29.82%, MSE = 4.4631
2025-02-28 00:56:22 - INFO - Epoch : 4 , Batch [ 180 / 274 ] : Loss = 1.097728, Accuracy = 31.12%, MSE = 4.3304
2025-02-28 00:57:25 - INFO - Epoch : 4 , Batch [ 190 / 274 ] : Loss = 1.788956, Accuracy = 31.98%, MSE = 4.2720
2025-02-28 00:58:21 - INFO - Epoch : 4 , Batch [ 200 / 274 ] : Loss = 1.234362, Accuracy = 32.64%, MSE = 4.2600
2025-02-28 01:22:57 - INFO - y batch shape : torch.Size([256, 11])
2025-02-28 01:22:57 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 01:22:57 - INFO - Decoder Input shape : torch.Size([256, 188, 11])
2025-02-28 01:22:57 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 01:22:57 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 01:22:57 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 01:22:57 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 01:22:57 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 01:23:00 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-28 01:23:00 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-28 01:23:00 - INFO - Output shape : torch.Size([256, 11])
2025-02-28 01:23:01 - INFO - Output s : tensor([[ 0.4608, -1.3757, -0.8001,  ...,  0.5500, -0.1690, -0.2226],
        [ 0.3037,  0.7810,  0.6095,  ...,  2.4285,  0.6670,  0.4572],
        [ 1.3714,  1.4709,  1.3245,  ...,  2.4859,  1.2566,  1.5036],
        ...,
        [-0.7021, -1.2792,  1.8426,  ...,  2.0817,  0.5610, -0.1028],
        [ 1.4877, -0.1926,  0.4715,  ..., -1.2972, -1.0528,  1.3549],
        [-0.2228,  0.4148, -0.0988,  ...,  0.3648,  1.7727, -1.4666]],
       device='mps:0', grad_fn=<LinearBackward0>)
2025-02-28 01:23:01 - INFO - Output shape : torch.Size([256, 11])
2025-02-28 01:23:01 - INFO - Output shape after view : torch.Size([256, 11])
2025-02-28 01:23:01 - INFO - y batch shape after view : torch.Size([256, 11])
2025-02-28 01:23:04 - INFO - Epoch : 1 , Batch [ 0 / 274 ] : Loss = 3.377185, Accuracy = 10.55%, MSE = 12.8555
2025-02-28 01:24:00 - INFO - Epoch : 1 , Batch [ 10 / 274 ] : Loss = 2.338951, Accuracy = 17.97%, MSE = 6.8018
2025-02-28 01:24:53 - INFO - Epoch : 1 , Batch [ 20 / 274 ] : Loss = 2.143764, Accuracy = 18.94%, MSE = 6.2195
2025-02-28 01:29:47 - INFO - y batch shape : torch.Size([256, 11])
2025-02-28 01:29:51 - INFO - Output shape : torch.Size([256, 11])
2025-02-28 01:29:51 - INFO - Output shape after view : torch.Size([256, 11])
2025-02-28 01:29:51 - INFO - y batch shape after view : torch.Size([256, 11])
2025-02-28 01:29:55 - INFO - Epoch : 1 , Batch [ 0 / 274 ] : Loss = 2.194684, Accuracy = 15.62%, MSE = 6.3516
2025-02-28 01:31:00 - INFO - Epoch : 1 , Batch [ 10 / 274 ] : Loss = 2.145238, Accuracy = 18.47%, MSE = 5.7731
2025-02-28 01:31:57 - INFO - Epoch : 1 , Batch [ 20 / 274 ] : Loss = 2.119605, Accuracy = 18.92%, MSE = 5.7920
2025-02-28 01:32:56 - INFO - Epoch : 1 , Batch [ 30 / 274 ] : Loss = 2.168310, Accuracy = 19.44%, MSE = 5.7954
2025-02-28 01:33:58 - INFO - Epoch : 1 , Batch [ 40 / 274 ] : Loss = 2.131890, Accuracy = 20.26%, MSE = 5.8607
2025-02-28 01:34:59 - INFO - Epoch : 1 , Batch [ 50 / 274 ] : Loss = 2.082082, Accuracy = 20.57%, MSE = 5.7502
2025-02-28 01:35:58 - INFO - Epoch : 1 , Batch [ 60 / 274 ] : Loss = 2.161146, Accuracy = 20.81%, MSE = 5.7288
2025-02-28 01:36:53 - INFO - Epoch : 1 , Batch [ 70 / 274 ] : Loss = 2.136018, Accuracy = 20.91%, MSE = 5.7190
2025-02-28 01:37:53 - INFO - Epoch : 1 , Batch [ 80 / 274 ] : Loss = 2.117072, Accuracy = 20.99%, MSE = 5.7706
2025-02-28 01:38:55 - INFO - Epoch : 1 , Batch [ 90 / 274 ] : Loss = 2.105720, Accuracy = 21.06%, MSE = 5.7542
2025-02-28 01:39:51 - INFO - Epoch : 1 , Batch [ 100 / 274 ] : Loss = 2.079216, Accuracy = 21.08%, MSE = 5.7863
2025-02-28 01:40:43 - INFO - Epoch : 1 , Batch [ 110 / 274 ] : Loss = 2.078228, Accuracy = 21.31%, MSE = 5.8268
2025-02-28 01:41:36 - INFO - Epoch : 1 , Batch [ 120 / 274 ] : Loss = 2.048137, Accuracy = 21.57%, MSE = 5.8840
2025-02-28 01:42:32 - INFO - Epoch : 1 , Batch [ 130 / 274 ] : Loss = 2.018288, Accuracy = 21.82%, MSE = 5.8627
2025-02-28 01:43:30 - INFO - Epoch : 1 , Batch [ 140 / 274 ] : Loss = 2.031209, Accuracy = 21.90%, MSE = 5.8461
2025-02-28 01:44:31 - INFO - Epoch : 1 , Batch [ 150 / 274 ] : Loss = 1.974390, Accuracy = 22.04%, MSE = 5.8355
2025-02-28 01:45:32 - INFO - Epoch : 1 , Batch [ 160 / 274 ] : Loss = 1.912138, Accuracy = 22.31%, MSE = 5.8223
2025-02-28 01:46:35 - INFO - Epoch : 1 , Batch [ 170 / 274 ] : Loss = 1.959259, Accuracy = 22.66%, MSE = 5.7963
2025-02-28 01:47:25 - INFO - Epoch : 1 , Batch [ 180 / 274 ] : Loss = 1.951787, Accuracy = 23.00%, MSE = 5.7904
2025-02-28 01:48:16 - INFO - Epoch : 1 , Batch [ 190 / 274 ] : Loss = 1.981965, Accuracy = 23.22%, MSE = 5.7817
2025-02-28 01:49:11 - INFO - Epoch : 1 , Batch [ 200 / 274 ] : Loss = 1.923795, Accuracy = 23.46%, MSE = 5.7738
2025-02-28 01:50:06 - INFO - Epoch : 1 , Batch [ 210 / 274 ] : Loss = 1.805382, Accuracy = 23.65%, MSE = 5.7431
2025-02-28 01:51:04 - INFO - Epoch : 1 , Batch [ 220 / 274 ] : Loss = 2.005063, Accuracy = 23.75%, MSE = 5.7406
2025-02-28 01:52:10 - INFO - Epoch : 1 , Batch [ 230 / 274 ] : Loss = 2.108571, Accuracy = 23.78%, MSE = 5.7326
2025-02-28 01:53:12 - INFO - Epoch : 1 , Batch [ 240 / 274 ] : Loss = 1.882994, Accuracy = 23.97%, MSE = 5.7224
2025-02-28 01:54:01 - INFO - Epoch : 1 , Batch [ 250 / 274 ] : Loss = 2.012333, Accuracy = 24.07%, MSE = 5.7219
2025-02-28 01:54:47 - INFO - Epoch : 1 , Batch [ 260 / 274 ] : Loss = 1.963483, Accuracy = 24.11%, MSE = 5.6997
2025-02-28 01:55:32 - INFO - Epoch : 1 , Batch [ 270 / 274 ] : Loss = 2.024079, Accuracy = 24.16%, MSE = 5.6960
2025-02-28 01:55:43 - INFO - Epoch 2: Train Loss=2.0376, Train Acc=24.18%, Train MSE=5.6869
2025-02-28 01:56:42 - INFO - Epoch 2: Val Loss=2.4267, Val Acc=19.16%, Val MSE=5.2275
2025-02-28 01:56:45 - INFO - Epoch : 2 , Batch [ 0 / 274 ] : Loss = 2.464617, Accuracy = 17.97%, MSE = 8.4258
2025-02-28 01:57:32 - INFO - Epoch : 2 , Batch [ 10 / 274 ] : Loss = 2.096467, Accuracy = 22.44%, MSE = 5.1289
2025-02-28 01:58:20 - INFO - Epoch : 2 , Batch [ 20 / 274 ] : Loss = 2.021114, Accuracy = 22.27%, MSE = 5.3452
2025-02-28 01:59:08 - INFO - Epoch : 2 , Batch [ 30 / 274 ] : Loss = 2.029797, Accuracy = 22.58%, MSE = 5.3934
2025-02-28 01:59:57 - INFO - Epoch : 2 , Batch [ 40 / 274 ] : Loss = 2.045331, Accuracy = 22.82%, MSE = 5.3497
2025-02-28 02:00:45 - INFO - Epoch : 2 , Batch [ 50 / 274 ] : Loss = 1.981521, Accuracy = 23.03%, MSE = 5.3528
2025-02-28 02:01:34 - INFO - Epoch : 2 , Batch [ 60 / 274 ] : Loss = 2.081279, Accuracy = 23.16%, MSE = 5.3439
2025-02-28 02:02:21 - INFO - Epoch : 2 , Batch [ 70 / 274 ] : Loss = 2.072576, Accuracy = 23.33%, MSE = 5.3346
2025-02-28 02:03:11 - INFO - Epoch : 2 , Batch [ 80 / 274 ] : Loss = 2.180365, Accuracy = 23.33%, MSE = 5.3769
2025-02-28 02:04:04 - INFO - Epoch : 2 , Batch [ 90 / 274 ] : Loss = 1.977388, Accuracy = 23.36%, MSE = 5.3695
2025-02-28 02:04:52 - INFO - Epoch : 2 , Batch [ 100 / 274 ] : Loss = 2.000995, Accuracy = 23.39%, MSE = 5.3930
2025-02-28 02:05:39 - INFO - Epoch : 2 , Batch [ 110 / 274 ] : Loss = 1.958755, Accuracy = 23.44%, MSE = 5.5029
2025-02-28 02:06:27 - INFO - Epoch : 2 , Batch [ 120 / 274 ] : Loss = 2.114743, Accuracy = 23.48%, MSE = 5.5357
2025-02-28 02:07:16 - INFO - Epoch : 2 , Batch [ 130 / 274 ] : Loss = 1.967070, Accuracy = 23.62%, MSE = 5.5728
2025-02-28 02:08:07 - INFO - Epoch : 2 , Batch [ 140 / 274 ] : Loss = 1.927300, Accuracy = 23.78%, MSE = 5.5553
2025-02-28 02:08:59 - INFO - Epoch : 2 , Batch [ 150 / 274 ] : Loss = 1.957241, Accuracy = 23.88%, MSE = 5.5760
2025-02-28 02:09:54 - INFO - Epoch : 2 , Batch [ 160 / 274 ] : Loss = 1.974401, Accuracy = 24.00%, MSE = 5.5733
2025-02-28 02:10:51 - INFO - Epoch : 2 , Batch [ 170 / 274 ] : Loss = 2.081838, Accuracy = 23.92%, MSE = 5.5605
2025-02-28 02:11:51 - INFO - Epoch : 2 , Batch [ 180 / 274 ] : Loss = 2.065548, Accuracy = 23.87%, MSE = 5.5740
2025-02-28 02:12:51 - INFO - Epoch : 2 , Batch [ 190 / 274 ] : Loss = 2.046723, Accuracy = 23.83%, MSE = 5.5880
2025-02-28 02:13:49 - INFO - Epoch : 2 , Batch [ 200 / 274 ] : Loss = 2.045433, Accuracy = 23.85%, MSE = 5.5966
2025-02-28 02:14:47 - INFO - Epoch : 2 , Batch [ 210 / 274 ] : Loss = 2.014538, Accuracy = 23.86%, MSE = 5.6194
2025-02-28 02:15:42 - INFO - Epoch : 2 , Batch [ 220 / 274 ] : Loss = 1.964554, Accuracy = 23.90%, MSE = 5.6317
2025-02-28 02:16:40 - INFO - Epoch : 2 , Batch [ 230 / 274 ] : Loss = 2.001227, Accuracy = 23.91%, MSE = 5.6315
2025-02-28 02:17:27 - INFO - Epoch : 2 , Batch [ 240 / 274 ] : Loss = 2.053562, Accuracy = 23.87%, MSE = 5.6542
2025-02-28 02:18:24 - INFO - Epoch : 2 , Batch [ 250 / 274 ] : Loss = 2.082098, Accuracy = 23.76%, MSE = 5.6932
2025-02-28 02:19:34 - INFO - Epoch : 2 , Batch [ 260 / 274 ] : Loss = 2.113638, Accuracy = 23.69%, MSE = 5.7138
2025-02-28 02:20:41 - INFO - Epoch : 2 , Batch [ 270 / 274 ] : Loss = 2.027461, Accuracy = 23.65%, MSE = 5.7220
2025-02-28 02:20:58 - INFO - Epoch 3: Train Loss=2.0413, Train Acc=23.66%, Train MSE=5.7202
2025-02-28 02:21:57 - INFO - Epoch 3: Val Loss=2.3209, Val Acc=21.57%, Val MSE=4.8558
2025-02-28 02:22:02 - INFO - Epoch : 3 , Batch [ 0 / 274 ] : Loss = 2.088034, Accuracy = 19.14%, MSE = 6.2109
2025-02-28 02:23:11 - INFO - Epoch : 3 , Batch [ 10 / 274 ] : Loss = 2.080210, Accuracy = 20.38%, MSE = 6.6832
2025-02-28 02:24:17 - INFO - Epoch : 3 , Batch [ 20 / 274 ] : Loss = 2.002554, Accuracy = 22.94%, MSE = 6.2965
2025-02-28 02:28:19 - INFO - Epoch : 3 , Batch [ 30 / 274 ] : Loss = 1.957501, Accuracy = 23.56%, MSE = 6.4381
2025-02-28 02:44:54 - INFO - Epoch : 3 , Batch [ 40 / 274 ] : Loss = 2.042530, Accuracy = 23.13%, MSE = 6.4548
2025-02-28 03:01:16 - INFO - Epoch : 3 , Batch [ 50 / 274 ] : Loss = 2.055239, Accuracy = 23.14%, MSE = 6.3817
2025-02-28 03:01:58 - INFO - Epoch : 3 , Batch [ 60 / 274 ] : Loss = 1.995612, Accuracy = 23.32%, MSE = 6.4905
2025-02-28 03:02:44 - INFO - Epoch : 3 , Batch [ 70 / 274 ] : Loss = 2.062330, Accuracy = 23.25%, MSE = 6.4721
2025-02-28 03:03:31 - INFO - Epoch : 3 , Batch [ 80 / 274 ] : Loss = 2.121208, Accuracy = 22.87%, MSE = 6.3830
2025-02-28 03:20:27 - INFO - Epoch : 3 , Batch [ 90 / 274 ] : Loss = 2.123275, Accuracy = 22.71%, MSE = 6.1945
2025-02-28 03:21:12 - INFO - Epoch : 3 , Batch [ 100 / 274 ] : Loss = 2.133163, Accuracy = 22.66%, MSE = 6.0807
2025-02-28 03:22:04 - INFO - Epoch : 3 , Batch [ 110 / 274 ] : Loss = 2.079802, Accuracy = 22.62%, MSE = 5.9647
2025-02-28 03:38:04 - INFO - Epoch : 3 , Batch [ 120 / 274 ] : Loss = 2.069262, Accuracy = 22.43%, MSE = 5.8991
2025-02-28 03:53:55 - INFO - Epoch : 3 , Batch [ 130 / 274 ] : Loss = 2.095489, Accuracy = 22.39%, MSE = 5.8005
2025-02-28 03:54:33 - INFO - Epoch : 3 , Batch [ 140 / 274 ] : Loss = 2.064926, Accuracy = 22.37%, MSE = 5.7420
2025-02-28 03:55:15 - INFO - Epoch : 3 , Batch [ 150 / 274 ] : Loss = 2.119073, Accuracy = 22.31%, MSE = 5.6762
2025-02-28 04:04:06 - INFO - Epoch : 3 , Batch [ 160 / 274 ] : Loss = 2.181011, Accuracy = 22.27%, MSE = 5.6593
2025-02-28 04:20:55 - INFO - Epoch : 3 , Batch [ 170 / 274 ] : Loss = 2.049390, Accuracy = 22.28%, MSE = 5.6243
2025-02-28 04:21:40 - INFO - Epoch : 3 , Batch [ 180 / 274 ] : Loss = 2.071271, Accuracy = 22.26%, MSE = 5.5868
2025-02-28 04:54:45 - INFO - Epoch : 3 , Batch [ 190 / 274 ] : Loss = 2.094406, Accuracy = 22.22%, MSE = 5.5808
2025-02-28 04:55:27 - INFO - Epoch : 3 , Batch [ 200 / 274 ] : Loss = 2.098607, Accuracy = 22.16%, MSE = 5.5632
2025-02-28 04:56:06 - INFO - Epoch : 3 , Batch [ 210 / 274 ] : Loss = 2.062968, Accuracy = 22.20%, MSE = 5.5240
2025-02-28 04:56:45 - INFO - Epoch : 3 , Batch [ 220 / 274 ] : Loss = 2.147770, Accuracy = 22.17%, MSE = 5.5176
2025-02-28 05:05:39 - INFO - Epoch : 3 , Batch [ 230 / 274 ] : Loss = 2.091731, Accuracy = 22.09%, MSE = 5.4940
2025-02-28 05:23:14 - INFO - Epoch : 3 , Batch [ 240 / 274 ] : Loss = 2.092811, Accuracy = 22.04%, MSE = 5.4751
2025-02-28 05:24:04 - INFO - Epoch : 3 , Batch [ 250 / 274 ] : Loss = 2.086669, Accuracy = 21.99%, MSE = 5.4556
2025-02-28 05:24:54 - INFO - Epoch : 3 , Batch [ 260 / 274 ] : Loss = 2.000749, Accuracy = 21.95%, MSE = 5.4348
2025-02-28 05:29:42 - INFO - Epoch : 3 , Batch [ 270 / 274 ] : Loss = 2.090101, Accuracy = 21.95%, MSE = 5.4236
2025-02-28 05:29:52 - INFO - Epoch 4: Train Loss=2.0843, Train Acc=21.96%, Train MSE=5.4297
2025-02-28 05:46:04 - INFO - Epoch 4: Val Loss=2.3106, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 06:02:14 - INFO - Epoch : 4 , Batch [ 0 / 274 ] : Loss = 2.147363, Accuracy = 19.14%, MSE = 5.5469
2025-02-28 06:02:58 - INFO - Epoch : 4 , Batch [ 10 / 274 ] : Loss = 2.173329, Accuracy = 20.53%, MSE = 5.1278
2025-02-28 06:03:46 - INFO - Epoch : 4 , Batch [ 20 / 274 ] : Loss = 2.047113, Accuracy = 21.30%, MSE = 4.9782
2025-02-28 06:06:21 - INFO - Epoch : 4 , Batch [ 30 / 274 ] : Loss = 2.034474, Accuracy = 21.57%, MSE = 4.9252
2025-02-28 06:22:17 - INFO - Epoch : 4 , Batch [ 40 / 274 ] : Loss = 2.143409, Accuracy = 21.45%, MSE = 4.9566
2025-02-28 06:40:34 - INFO - Epoch : 4 , Batch [ 50 / 274 ] : Loss = 2.053223, Accuracy = 21.54%, MSE = 4.9666
2025-02-28 06:41:24 - INFO - Epoch : 4 , Batch [ 60 / 274 ] : Loss = 2.079499, Accuracy = 21.64%, MSE = 4.9157
2025-02-28 06:42:14 - INFO - Epoch : 4 , Batch [ 70 / 274 ] : Loss = 2.208509, Accuracy = 21.79%, MSE = 4.9414
2025-02-28 06:43:09 - INFO - Epoch : 4 , Batch [ 80 / 274 ] : Loss = 2.105284, Accuracy = 21.86%, MSE = 4.8957
2025-02-28 06:43:59 - INFO - Epoch : 4 , Batch [ 90 / 274 ] : Loss = 1.991934, Accuracy = 21.81%, MSE = 4.8931
2025-02-28 07:00:45 - INFO - Epoch : 4 , Batch [ 100 / 274 ] : Loss = 2.102010, Accuracy = 21.82%, MSE = 4.8976
2025-02-28 07:01:33 - INFO - Epoch : 4 , Batch [ 110 / 274 ] : Loss = 2.162888, Accuracy = 21.65%, MSE = 4.9263
2025-02-28 07:02:12 - INFO - Epoch : 4 , Batch [ 120 / 274 ] : Loss = 2.152422, Accuracy = 21.72%, MSE = 4.9626
2025-02-28 07:02:51 - INFO - Epoch : 4 , Batch [ 130 / 274 ] : Loss = 2.153403, Accuracy = 21.64%, MSE = 4.9632
2025-02-28 07:07:41 - INFO - Epoch : 4 , Batch [ 140 / 274 ] : Loss = 2.089528, Accuracy = 21.50%, MSE = 4.9732
2025-02-28 07:23:33 - INFO - Epoch : 4 , Batch [ 150 / 274 ] : Loss = 2.065490, Accuracy = 21.64%, MSE = 4.9768
2025-02-28 07:41:04 - INFO - Epoch : 4 , Batch [ 160 / 274 ] : Loss = 2.129111, Accuracy = 21.55%, MSE = 4.9902
2025-02-28 07:41:43 - INFO - Epoch : 4 , Batch [ 170 / 274 ] : Loss = 2.119163, Accuracy = 21.55%, MSE = 4.9961
2025-02-28 07:42:25 - INFO - Epoch : 4 , Batch [ 180 / 274 ] : Loss = 2.139715, Accuracy = 21.48%, MSE = 5.0172
2025-02-28 07:58:52 - INFO - Epoch : 4 , Batch [ 190 / 274 ] : Loss = 2.060297, Accuracy = 21.54%, MSE = 5.0072
2025-02-28 07:59:29 - INFO - Epoch : 4 , Batch [ 200 / 274 ] : Loss = 2.110732, Accuracy = 21.61%, MSE = 5.0383
2025-02-28 08:00:12 - INFO - Epoch : 4 , Batch [ 210 / 274 ] : Loss = 2.171984, Accuracy = 21.63%, MSE = 5.0687
2025-02-28 08:08:18 - INFO - Epoch : 4 , Batch [ 220 / 274 ] : Loss = 2.187992, Accuracy = 21.62%, MSE = 5.0667
2025-02-28 08:24:16 - INFO - Epoch : 4 , Batch [ 230 / 274 ] : Loss = 2.092956, Accuracy = 21.61%, MSE = 5.0523
2025-02-28 08:33:39 - INFO - Epoch : 4 , Batch [ 240 / 274 ] : Loss = 2.059346, Accuracy = 21.69%, MSE = 5.0624
2025-02-28 08:34:39 - INFO - Epoch : 4 , Batch [ 250 / 274 ] : Loss = 2.086074, Accuracy = 21.72%, MSE = 5.0527
2025-02-28 08:35:28 - INFO - Epoch : 4 , Batch [ 260 / 274 ] : Loss = 2.074085, Accuracy = 21.71%, MSE = 5.0419
2025-02-28 08:36:12 - INFO - Epoch : 4 , Batch [ 270 / 274 ] : Loss = 2.037509, Accuracy = 21.71%, MSE = 5.0477
2025-02-28 08:36:24 - INFO - Epoch 5: Train Loss=2.0961, Train Acc=21.71%, Train MSE=5.0433
2025-02-28 08:37:22 - INFO - Epoch 5: Val Loss=2.3044, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 08:37:25 - INFO - Epoch : 5 , Batch [ 0 / 274 ] : Loss = 2.223242, Accuracy = 18.36%, MSE = 4.5000
2025-02-28 08:38:13 - INFO - Epoch : 5 , Batch [ 10 / 274 ] : Loss = 2.065965, Accuracy = 21.09%, MSE = 5.0657
2025-02-28 08:39:07 - INFO - Epoch : 5 , Batch [ 20 / 274 ] : Loss = 2.165729, Accuracy = 21.19%, MSE = 5.3000
2025-02-28 08:39:57 - INFO - Epoch : 5 , Batch [ 30 / 274 ] : Loss = 2.043290, Accuracy = 21.74%, MSE = 5.1610
2025-02-28 08:40:46 - INFO - Epoch : 5 , Batch [ 40 / 274 ] : Loss = 2.173102, Accuracy = 22.00%, MSE = 5.1055
2025-02-28 08:41:37 - INFO - Epoch : 5 , Batch [ 50 / 274 ] : Loss = 2.092562, Accuracy = 21.93%, MSE = 5.1243
2025-02-28 08:42:26 - INFO - Epoch : 5 , Batch [ 60 / 274 ] : Loss = 2.102063, Accuracy = 21.79%, MSE = 5.1653
2025-02-28 08:43:14 - INFO - Epoch : 5 , Batch [ 70 / 274 ] : Loss = 2.130787, Accuracy = 22.04%, MSE = 5.1495
2025-02-28 08:44:06 - INFO - Epoch : 5 , Batch [ 80 / 274 ] : Loss = 2.155927, Accuracy = 22.14%, MSE = 5.2125
2025-02-28 08:44:57 - INFO - Epoch : 5 , Batch [ 90 / 274 ] : Loss = 2.108250, Accuracy = 22.22%, MSE = 5.2914
2025-02-28 08:45:50 - INFO - Epoch : 5 , Batch [ 100 / 274 ] : Loss = 1.969112, Accuracy = 22.37%, MSE = 5.2301
2025-02-28 08:46:43 - INFO - Epoch : 5 , Batch [ 110 / 274 ] : Loss = 2.029619, Accuracy = 22.46%, MSE = 5.1807
2025-02-28 08:47:40 - INFO - Epoch : 5 , Batch [ 120 / 274 ] : Loss = 2.136971, Accuracy = 22.42%, MSE = 5.1807
2025-02-28 08:48:39 - INFO - Epoch : 5 , Batch [ 130 / 274 ] : Loss = 2.093453, Accuracy = 22.34%, MSE = 5.1572
2025-02-28 08:49:35 - INFO - Epoch : 5 , Batch [ 140 / 274 ] : Loss = 2.026147, Accuracy = 22.36%, MSE = 5.1354
2025-02-28 08:50:29 - INFO - Epoch : 5 , Batch [ 150 / 274 ] : Loss = 2.081383, Accuracy = 22.34%, MSE = 5.1455
2025-02-28 08:51:25 - INFO - Epoch : 5 , Batch [ 160 / 274 ] : Loss = 2.119830, Accuracy = 22.32%, MSE = 5.2080
2025-02-28 08:52:17 - INFO - Epoch : 5 , Batch [ 170 / 274 ] : Loss = 2.066235, Accuracy = 22.26%, MSE = 5.2126
2025-02-28 08:53:12 - INFO - Epoch : 5 , Batch [ 180 / 274 ] : Loss = 2.062556, Accuracy = 22.22%, MSE = 5.1891
2025-02-28 08:54:13 - INFO - Epoch : 5 , Batch [ 190 / 274 ] : Loss = 2.130116, Accuracy = 22.12%, MSE = 5.1730
2025-02-28 08:55:15 - INFO - Epoch : 5 , Batch [ 200 / 274 ] : Loss = 2.008689, Accuracy = 22.06%, MSE = 5.1498
2025-02-28 08:56:14 - INFO - Epoch : 5 , Batch [ 210 / 274 ] : Loss = 2.074788, Accuracy = 22.10%, MSE = 5.1353
2025-02-28 08:57:13 - INFO - Epoch : 5 , Batch [ 220 / 274 ] : Loss = 1.982574, Accuracy = 22.08%, MSE = 5.1149
2025-02-28 08:58:13 - INFO - Epoch : 5 , Batch [ 230 / 274 ] : Loss = 2.061632, Accuracy = 22.14%, MSE = 5.1143
2025-02-28 08:59:07 - INFO - Epoch : 5 , Batch [ 240 / 274 ] : Loss = 2.106791, Accuracy = 22.15%, MSE = 5.1107
2025-02-28 09:00:13 - INFO - Epoch : 5 , Batch [ 250 / 274 ] : Loss = 2.018105, Accuracy = 22.18%, MSE = 5.0981
2025-02-28 09:01:20 - INFO - Epoch : 5 , Batch [ 260 / 274 ] : Loss = 2.108290, Accuracy = 22.18%, MSE = 5.0846
2025-02-28 09:02:27 - INFO - Epoch : 5 , Batch [ 270 / 274 ] : Loss = 2.039916, Accuracy = 22.19%, MSE = 5.0765
2025-02-28 09:02:46 - INFO - Epoch 6: Train Loss=2.0713, Train Acc=22.19%, Train MSE=5.0726
2025-02-28 09:03:45 - INFO - Epoch 6: Val Loss=2.2308, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 09:03:52 - INFO - Epoch : 6 , Batch [ 0 / 274 ] : Loss = 2.098582, Accuracy = 22.27%, MSE = 4.6523
2025-02-28 09:05:01 - INFO - Epoch : 6 , Batch [ 10 / 274 ] : Loss = 2.047246, Accuracy = 23.19%, MSE = 4.9268
2025-02-28 09:09:55 - INFO - Epoch : 6 , Batch [ 20 / 274 ] : Loss = 2.047511, Accuracy = 22.82%, MSE = 4.7826
2025-02-28 09:10:41 - INFO - Epoch : 6 , Batch [ 30 / 274 ] : Loss = 2.130264, Accuracy = 22.78%, MSE = 4.8232
2025-02-28 09:32:54 - INFO - Epoch : 6 , Batch [ 40 / 274 ] : Loss = 2.144839, Accuracy = 22.59%, MSE = 4.9986
2025-02-28 09:33:40 - INFO - Epoch : 6 , Batch [ 50 / 274 ] : Loss = 2.033488, Accuracy = 22.27%, MSE = 5.0166
2025-02-28 09:34:20 - INFO - Epoch : 6 , Batch [ 60 / 274 ] : Loss = 2.143221, Accuracy = 22.34%, MSE = 5.0222
2025-02-28 09:34:58 - INFO - Epoch : 6 , Batch [ 70 / 274 ] : Loss = 2.007613, Accuracy = 22.51%, MSE = 4.9899
2025-02-28 09:35:38 - INFO - Epoch : 6 , Batch [ 80 / 274 ] : Loss = 2.076434, Accuracy = 22.59%, MSE = 4.9601
2025-02-28 09:36:20 - INFO - Epoch : 6 , Batch [ 90 / 274 ] : Loss = 2.037576, Accuracy = 22.66%, MSE = 4.9393
2025-02-28 09:37:02 - INFO - Epoch : 6 , Batch [ 100 / 274 ] : Loss = 2.075100, Accuracy = 22.66%, MSE = 4.9322
2025-02-28 09:37:43 - INFO - Epoch : 6 , Batch [ 110 / 274 ] : Loss = 1.936768, Accuracy = 22.67%, MSE = 4.9080
2025-02-28 09:38:27 - INFO - Epoch : 6 , Batch [ 120 / 274 ] : Loss = 1.983234, Accuracy = 22.64%, MSE = 4.9365
2025-02-28 09:39:11 - INFO - Epoch : 6 , Batch [ 130 / 274 ] : Loss = 2.036065, Accuracy = 22.46%, MSE = 4.9772
2025-02-28 09:39:59 - INFO - Epoch : 6 , Batch [ 140 / 274 ] : Loss = 2.100660, Accuracy = 22.38%, MSE = 5.0709
2025-02-28 09:40:50 - INFO - Epoch : 6 , Batch [ 150 / 274 ] : Loss = 2.059672, Accuracy = 22.39%, MSE = 5.0884
2025-02-28 09:41:48 - INFO - Epoch : 6 , Batch [ 160 / 274 ] : Loss = 2.033991, Accuracy = 22.48%, MSE = 5.0659
2025-02-28 09:42:43 - INFO - Epoch : 6 , Batch [ 170 / 274 ] : Loss = 2.117292, Accuracy = 22.47%, MSE = 5.0547
2025-02-28 09:43:42 - INFO - Epoch : 6 , Batch [ 180 / 274 ] : Loss = 2.046880, Accuracy = 22.50%, MSE = 5.0574
2025-02-28 09:44:42 - INFO - Epoch : 6 , Batch [ 190 / 274 ] : Loss = 2.011267, Accuracy = 22.56%, MSE = 5.0439
2025-02-28 09:45:38 - INFO - Epoch : 6 , Batch [ 200 / 274 ] : Loss = 2.062783, Accuracy = 22.59%, MSE = 5.0765
2025-02-28 09:46:36 - INFO - Epoch : 6 , Batch [ 210 / 274 ] : Loss = 2.060292, Accuracy = 22.62%, MSE = 5.0945
2025-02-28 09:47:29 - INFO - Epoch : 6 , Batch [ 220 / 274 ] : Loss = 2.162809, Accuracy = 22.63%, MSE = 5.0828
2025-02-28 09:48:24 - INFO - Epoch : 6 , Batch [ 230 / 274 ] : Loss = 1.998101, Accuracy = 22.66%, MSE = 5.0831
2025-02-28 09:49:12 - INFO - Epoch : 6 , Batch [ 240 / 274 ] : Loss = 2.053974, Accuracy = 22.65%, MSE = 5.0920
2025-02-28 09:50:01 - INFO - Epoch : 6 , Batch [ 250 / 274 ] : Loss = 2.035789, Accuracy = 22.70%, MSE = 5.0847
2025-02-28 09:50:53 - INFO - Epoch : 6 , Batch [ 260 / 274 ] : Loss = 2.070727, Accuracy = 22.63%, MSE = 5.1050
2025-02-28 09:51:50 - INFO - Epoch : 6 , Batch [ 270 / 274 ] : Loss = 2.023082, Accuracy = 22.69%, MSE = 5.1242
2025-02-28 09:52:08 - INFO - Epoch 7: Train Loss=2.0564, Train Acc=22.70%, Train MSE=5.1202
2025-02-28 09:53:07 - INFO - Epoch 7: Val Loss=2.1607, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 09:53:12 - INFO - Epoch : 7 , Batch [ 0 / 274 ] : Loss = 2.112449, Accuracy = 24.22%, MSE = 4.7656
2025-02-28 09:54:14 - INFO - Epoch : 7 , Batch [ 10 / 274 ] : Loss = 2.072966, Accuracy = 21.73%, MSE = 6.0504
2025-02-28 09:55:22 - INFO - Epoch : 7 , Batch [ 20 / 274 ] : Loss = 1.996597, Accuracy = 21.35%, MSE = 5.7786
2025-02-28 09:56:28 - INFO - Epoch : 7 , Batch [ 30 / 274 ] : Loss = 1.993767, Accuracy = 21.56%, MSE = 5.5575
2025-02-28 09:57:35 - INFO - Epoch : 7 , Batch [ 40 / 274 ] : Loss = 2.027524, Accuracy = 21.56%, MSE = 5.4302
2025-02-28 09:58:38 - INFO - Epoch : 7 , Batch [ 50 / 274 ] : Loss = 1.939084, Accuracy = 21.81%, MSE = 5.4445
2025-02-28 09:59:46 - INFO - Epoch : 7 , Batch [ 60 / 274 ] : Loss = 1.999860, Accuracy = 22.16%, MSE = 5.4957
2025-02-28 10:03:31 - INFO - Epoch : 7 , Batch [ 70 / 274 ] : Loss = 1.978219, Accuracy = 22.43%, MSE = 5.4293
2025-02-28 10:04:33 - INFO - Epoch : 7 , Batch [ 80 / 274 ] : Loss = 2.129992, Accuracy = 22.19%, MSE = 5.3640
2025-02-28 10:14:08 - INFO - Epoch : 7 , Batch [ 90 / 274 ] : Loss = 2.017468, Accuracy = 22.16%, MSE = 5.3775
2025-02-28 10:15:12 - INFO - Epoch : 7 , Batch [ 100 / 274 ] : Loss = 2.148448, Accuracy = 22.13%, MSE = 5.4167
2025-02-28 10:16:14 - INFO - Epoch : 7 , Batch [ 110 / 274 ] : Loss = 2.014693, Accuracy = 22.02%, MSE = 5.4471
2025-02-28 10:24:09 - INFO - Epoch : 7 , Batch [ 120 / 274 ] : Loss = 2.009645, Accuracy = 21.99%, MSE = 5.4153
2025-02-28 10:40:24 - INFO - Epoch : 7 , Batch [ 130 / 274 ] : Loss = 2.063285, Accuracy = 21.93%, MSE = 5.4084
2025-02-28 10:41:17 - INFO - Epoch : 7 , Batch [ 140 / 274 ] : Loss = 2.057466, Accuracy = 21.90%, MSE = 5.4594
2025-02-28 10:42:14 - INFO - Epoch : 7 , Batch [ 150 / 274 ] : Loss = 1.918986, Accuracy = 21.83%, MSE = 5.5295
2025-02-28 10:59:40 - INFO - Epoch : 7 , Batch [ 160 / 274 ] : Loss = 2.066101, Accuracy = 21.88%, MSE = 5.4721
2025-02-28 11:00:36 - INFO - Epoch : 7 , Batch [ 170 / 274 ] : Loss = 2.000419, Accuracy = 21.91%, MSE = 5.4463
2025-02-28 11:01:31 - INFO - Epoch : 7 , Batch [ 180 / 274 ] : Loss = 2.099537, Accuracy = 21.94%, MSE = 5.4573
2025-02-28 11:03:20 - INFO - Epoch : 7 , Batch [ 190 / 274 ] : Loss = 2.095050, Accuracy = 21.86%, MSE = 5.4443
2025-02-28 11:05:47 - INFO - Epoch : 7 , Batch [ 200 / 274 ] : Loss = 2.025784, Accuracy = 21.92%, MSE = 5.4353
2025-02-28 11:06:52 - INFO - Epoch : 7 , Batch [ 210 / 274 ] : Loss = 2.059450, Accuracy = 21.91%, MSE = 5.4192
2025-02-28 11:07:43 - INFO - Epoch : 7 , Batch [ 220 / 274 ] : Loss = 2.031316, Accuracy = 21.89%, MSE = 5.4696
2025-02-28 11:08:36 - INFO - Epoch : 7 , Batch [ 230 / 274 ] : Loss = 2.061694, Accuracy = 21.86%, MSE = 5.4789
2025-02-28 11:09:25 - INFO - Epoch : 7 , Batch [ 240 / 274 ] : Loss = 2.067261, Accuracy = 21.85%, MSE = 5.4903
2025-02-28 11:10:16 - INFO - Epoch : 7 , Batch [ 250 / 274 ] : Loss = 1.992054, Accuracy = 21.87%, MSE = 5.4780
2025-02-28 11:11:13 - INFO - Epoch : 7 , Batch [ 260 / 274 ] : Loss = 1.976582, Accuracy = 21.84%, MSE = 5.5191
2025-02-28 11:12:17 - INFO - Epoch : 7 , Batch [ 270 / 274 ] : Loss = 2.037355, Accuracy = 21.83%, MSE = 5.5732
2025-02-28 11:12:34 - INFO - Epoch 8: Train Loss=2.0324, Train Acc=21.85%, Train MSE=5.5528
2025-02-28 11:13:34 - INFO - Epoch 8: Val Loss=2.1481, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 11:13:40 - INFO - Epoch : 8 , Batch [ 0 / 274 ] : Loss = 2.143669, Accuracy = 19.92%, MSE = 4.2969
2025-02-28 11:14:47 - INFO - Epoch : 8 , Batch [ 10 / 274 ] : Loss = 1.994734, Accuracy = 21.95%, MSE = 4.8658
2025-02-28 11:15:48 - INFO - Epoch : 8 , Batch [ 20 / 274 ] : Loss = 1.976477, Accuracy = 21.89%, MSE = 5.8677
2025-02-28 11:16:43 - INFO - Epoch : 8 , Batch [ 30 / 274 ] : Loss = 1.950140, Accuracy = 22.20%, MSE = 5.5302
2025-02-28 11:17:39 - INFO - Epoch : 8 , Batch [ 40 / 274 ] : Loss = 1.996189, Accuracy = 22.22%, MSE = 5.4739
2025-02-28 11:18:34 - INFO - Epoch : 8 , Batch [ 50 / 274 ] : Loss = 2.045462, Accuracy = 22.23%, MSE = 5.8786
2025-02-28 11:19:33 - INFO - Epoch : 8 , Batch [ 60 / 274 ] : Loss = 2.079728, Accuracy = 22.11%, MSE = 5.7767
2025-02-28 11:20:31 - INFO - Epoch : 8 , Batch [ 70 / 274 ] : Loss = 1.947743, Accuracy = 21.92%, MSE = 5.7178
2025-02-28 11:21:32 - INFO - Epoch : 8 , Batch [ 80 / 274 ] : Loss = 2.026348, Accuracy = 21.75%, MSE = 5.6301
2025-02-28 11:22:30 - INFO - Epoch : 8 , Batch [ 90 / 274 ] : Loss = 2.075216, Accuracy = 21.73%, MSE = 5.6657
2025-02-28 11:23:28 - INFO - Epoch : 8 , Batch [ 100 / 274 ] : Loss = 2.033076, Accuracy = 21.60%, MSE = 5.6294
2025-02-28 11:24:27 - INFO - Epoch : 8 , Batch [ 110 / 274 ] : Loss = 2.069579, Accuracy = 21.67%, MSE = 5.6147
2025-02-28 11:25:26 - INFO - Epoch : 8 , Batch [ 120 / 274 ] : Loss = 2.032576, Accuracy = 21.70%, MSE = 5.5967
2025-02-28 11:26:25 - INFO - Epoch : 8 , Batch [ 130 / 274 ] : Loss = 2.073248, Accuracy = 21.76%, MSE = 5.5888
2025-02-28 11:27:24 - INFO - Epoch : 8 , Batch [ 140 / 274 ] : Loss = 1.990618, Accuracy = 21.79%, MSE = 5.6058
2025-02-28 11:28:16 - INFO - Epoch : 8 , Batch [ 150 / 274 ] : Loss = 2.011132, Accuracy = 21.75%, MSE = 5.5779
2025-02-28 11:29:14 - INFO - Epoch : 8 , Batch [ 160 / 274 ] : Loss = 1.997349, Accuracy = 21.77%, MSE = 5.5249
2025-02-28 11:30:18 - INFO - Epoch : 8 , Batch [ 170 / 274 ] : Loss = 1.953442, Accuracy = 21.79%, MSE = 5.4888
2025-02-28 11:31:19 - INFO - Epoch : 8 , Batch [ 180 / 274 ] : Loss = 2.063768, Accuracy = 21.78%, MSE = 5.4629
2025-02-28 11:32:12 - INFO - Epoch : 8 , Batch [ 190 / 274 ] : Loss = 2.106503, Accuracy = 21.78%, MSE = 5.4559
2025-02-28 11:33:02 - INFO - Epoch : 8 , Batch [ 200 / 274 ] : Loss = 2.066777, Accuracy = 21.68%, MSE = 5.4729
2025-02-28 11:33:53 - INFO - Epoch : 8 , Batch [ 210 / 274 ] : Loss = 2.023382, Accuracy = 21.67%, MSE = 5.4733
2025-02-28 11:34:47 - INFO - Epoch : 8 , Batch [ 220 / 274 ] : Loss = 2.064864, Accuracy = 21.70%, MSE = 5.4765
2025-02-28 11:35:45 - INFO - Epoch : 8 , Batch [ 230 / 274 ] : Loss = 2.050604, Accuracy = 21.66%, MSE = 5.5686
2025-02-28 11:36:46 - INFO - Epoch : 8 , Batch [ 240 / 274 ] : Loss = 2.060550, Accuracy = 21.64%, MSE = 5.7293
2025-02-28 11:37:38 - INFO - Epoch : 8 , Batch [ 250 / 274 ] : Loss = 1.959351, Accuracy = 21.62%, MSE = 5.7297
2025-02-28 11:38:32 - INFO - Epoch : 8 , Batch [ 260 / 274 ] : Loss = 2.001867, Accuracy = 21.64%, MSE = 5.7286
2025-02-28 11:39:29 - INFO - Epoch : 8 , Batch [ 270 / 274 ] : Loss = 2.056529, Accuracy = 21.61%, MSE = 5.7598
2025-02-28 11:39:44 - INFO - Epoch 9: Train Loss=2.0224, Train Acc=21.60%, Train MSE=5.7677
2025-02-28 11:40:41 - INFO - Epoch 9: Val Loss=2.1682, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 11:40:46 - INFO - Epoch : 9 , Batch [ 0 / 274 ] : Loss = 2.325980, Accuracy = 19.53%, MSE = 5.3242
2025-02-28 11:41:44 - INFO - Epoch : 9 , Batch [ 10 / 274 ] : Loss = 2.025294, Accuracy = 20.49%, MSE = 6.6619
2025-02-28 11:42:53 - INFO - Epoch : 9 , Batch [ 20 / 274 ] : Loss = 2.076613, Accuracy = 20.89%, MSE = 6.0766
2025-02-28 11:43:56 - INFO - Epoch : 9 , Batch [ 30 / 274 ] : Loss = 1.942949, Accuracy = 21.27%, MSE = 5.8356
2025-02-28 11:44:49 - INFO - Epoch : 9 , Batch [ 40 / 274 ] : Loss = 1.968541, Accuracy = 21.69%, MSE = 5.9684
2025-02-28 11:45:42 - INFO - Epoch : 9 , Batch [ 50 / 274 ] : Loss = 2.021431, Accuracy = 21.71%, MSE = 6.1186
2025-02-28 11:46:34 - INFO - Epoch : 9 , Batch [ 60 / 274 ] : Loss = 2.060891, Accuracy = 21.61%, MSE = 6.1473
2025-02-28 11:47:29 - INFO - Epoch : 9 , Batch [ 70 / 274 ] : Loss = 1.943540, Accuracy = 21.80%, MSE = 6.1883
2025-02-28 11:48:31 - INFO - Epoch : 9 , Batch [ 80 / 274 ] : Loss = 1.898719, Accuracy = 21.79%, MSE = 6.1660
2025-02-28 11:49:25 - INFO - Epoch : 9 , Batch [ 90 / 274 ] : Loss = 1.985597, Accuracy = 22.04%, MSE = 6.1465
2025-02-28 11:50:21 - INFO - Epoch : 9 , Batch [ 100 / 274 ] : Loss = 1.972425, Accuracy = 22.08%, MSE = 6.1755
2025-02-28 11:51:14 - INFO - Epoch : 9 , Batch [ 110 / 274 ] : Loss = 1.967937, Accuracy = 22.08%, MSE = 6.1798
2025-02-28 11:52:17 - INFO - Epoch : 9 , Batch [ 120 / 274 ] : Loss = 2.019498, Accuracy = 22.12%, MSE = 6.1180
2025-02-28 11:53:18 - INFO - Epoch : 9 , Batch [ 130 / 274 ] : Loss = 2.014537, Accuracy = 21.98%, MSE = 6.0904
2025-02-28 11:54:13 - INFO - Epoch : 9 , Batch [ 140 / 274 ] : Loss = 1.984496, Accuracy = 21.93%, MSE = 6.1168
2025-02-28 11:55:09 - INFO - Epoch : 9 , Batch [ 150 / 274 ] : Loss = 1.988950, Accuracy = 21.94%, MSE = 6.0970
2025-02-28 11:56:08 - INFO - Epoch : 9 , Batch [ 160 / 274 ] : Loss = 1.976013, Accuracy = 21.94%, MSE = 6.1802
2025-02-28 11:57:14 - INFO - Epoch : 9 , Batch [ 170 / 274 ] : Loss = 2.021403, Accuracy = 21.94%, MSE = 6.1582
2025-02-28 11:58:19 - INFO - Epoch : 9 , Batch [ 180 / 274 ] : Loss = 2.023468, Accuracy = 21.89%, MSE = 6.1390
2025-02-28 11:59:28 - INFO - Epoch : 9 , Batch [ 190 / 274 ] : Loss = 1.976957, Accuracy = 21.89%, MSE = 6.1345
2025-02-28 12:00:41 - INFO - Epoch : 9 , Batch [ 200 / 274 ] : Loss = 1.888969, Accuracy = 21.94%, MSE = 6.2242
2025-02-28 12:01:47 - INFO - Epoch : 9 , Batch [ 210 / 274 ] : Loss = 2.044573, Accuracy = 21.99%, MSE = 6.1740
2025-02-28 12:02:52 - INFO - Epoch : 9 , Batch [ 220 / 274 ] : Loss = 2.059673, Accuracy = 21.93%, MSE = 6.2331
2025-02-28 12:03:57 - INFO - Epoch : 9 , Batch [ 230 / 274 ] : Loss = 2.119558, Accuracy = 21.88%, MSE = 6.2129
2025-02-28 12:04:54 - INFO - Epoch : 9 , Batch [ 240 / 274 ] : Loss = 2.157806, Accuracy = 21.84%, MSE = 6.1872
2025-02-28 12:05:53 - INFO - Epoch : 9 , Batch [ 250 / 274 ] : Loss = 2.141692, Accuracy = 21.82%, MSE = 6.1242
2025-02-28 12:06:54 - INFO - Epoch : 9 , Batch [ 260 / 274 ] : Loss = 2.112898, Accuracy = 21.79%, MSE = 6.0877
2025-02-28 12:08:01 - INFO - Epoch : 9 , Batch [ 270 / 274 ] : Loss = 2.145382, Accuracy = 21.75%, MSE = 6.0598
2025-02-28 12:08:18 - INFO - Epoch 10: Train Loss=2.0098, Train Acc=21.74%, Train MSE=6.0588
2025-02-28 12:09:17 - INFO - Epoch 10: Val Loss=2.1421, Val Acc=19.44%, Val MSE=5.1873
2025-02-28 12:09:23 - INFO - Epoch : 10 , Batch [ 0 / 274 ] : Loss = 2.126122, Accuracy = 22.66%, MSE = 5.2305
2025-02-28 12:10:30 - INFO - Epoch : 10 , Batch [ 10 / 274 ] : Loss = 2.177639, Accuracy = 21.98%, MSE = 5.2106
2025-02-28 12:11:32 - INFO - Epoch : 10 , Batch [ 20 / 274 ] : Loss = 2.146288, Accuracy = 20.72%, MSE = 5.1362
2025-02-28 12:12:37 - INFO - Epoch : 10 , Batch [ 30 / 274 ] : Loss = 2.143917, Accuracy = 20.48%, MSE = 4.9934
2025-02-28 12:13:39 - INFO - Epoch : 10 , Batch [ 40 / 274 ] : Loss = 2.206080, Accuracy = 20.59%, MSE = 5.0498
2025-02-28 13:28:54 - INFO - Full sequence shape : (213, 81)
2025-02-28 13:28:54 - INFO - Decoder sequence length : 188
2025-02-28 13:28:54 - INFO - Slope value shape : (188,)
2025-02-28 13:28:54 - INFO - Slope values : [-1 -1  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0
  1  1  0  0  0  0  0  0  0 -1 -1 -1  0  0  0  0  0  1  1  1  1  1  1  0
  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1  0]
2025-02-28 13:28:54 - INFO - Target bin max : 1
2025-02-28 13:28:54 - INFO - slope classes : tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1,
        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1])
2025-02-28 13:28:54 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-02-28 13:28:54 - INFO - Future price : 0
2025-02-28 13:28:54 - INFO - X shape : torch.Size([200, 51])
2025-02-28 13:28:54 - INFO - y shape : torch.Size([3])
2025-02-28 13:28:54 - INFO - y : tensor([0., 1., 0.])
2025-02-28 13:28:54 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 13:28:54 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-02-28 13:28:54 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 13:28:54 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 13:28:55 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 13:28:55 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 13:28:55 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 13:29:04 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-28 13:29:04 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-28 13:29:05 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 13:29:05 - INFO - Output s : tensor([[-2.1076e+00, -9.5042e-01,  5.5860e-01],
        [ 6.5478e-01,  6.4743e-01, -6.9386e-01],
        [-4.0967e-01,  3.4421e-01, -5.7022e-01],
        [-5.2019e-01,  1.4166e+00, -3.0410e-02],
        [-9.4610e-01,  3.5575e-01,  1.5336e+00],
        [ 2.6550e-01,  2.1711e+00, -2.3575e-01],
        [ 6.3521e-01,  2.2187e-01, -9.0735e-01],
        [-7.9358e-01,  1.0246e+00,  6.5889e-01],
        [ 1.5146e+00, -4.8412e-01,  3.4470e+00],
        [-1.7759e+00,  1.8189e+00,  1.5626e+00],
        [ 2.4028e-01, -1.6962e+00,  2.6442e+00],
        [ 5.7076e-01,  3.4470e-01,  5.4830e-01],
        [-1.1915e+00, -2.7396e-01, -8.7219e-01],
        [-7.6665e-01,  1.1230e-01,  3.0061e-01],
        [-1.1127e+00, -3.9931e-01, -1.6677e-01],
        [ 5.0112e-01,  1.5783e+00,  6.2497e-01],
        [-1.7455e-01,  1.5161e+00, -5.5228e-01],
        [-3.7286e-01, -9.1840e-01,  1.1813e+00],
        [-1.1697e+00, -1.2375e+00,  6.9145e-01],
        [-1.3127e+00, -1.0048e+00, -2.0578e-01],
        [-5.0584e-01,  1.4039e-01, -1.1205e+00],
        [-1.1740e+00, -8.5621e-01,  2.7001e-01],
        [-6.8099e-01, -9.3673e-01,  1.1434e+00],
        [-3.7733e-01, -1.0145e+00,  1.5951e-01],
        [-8.5321e-01, -4.4053e-01,  3.3793e-04],
        [ 1.1734e-01,  5.8173e-01,  3.8720e-01],
        [-5.6248e-01, -7.5719e-03, -7.0000e-01],
        [ 5.4175e-01,  1.2169e+00, -7.3553e-01],
        [-2.8485e-01, -9.4929e-01,  2.7689e+00],
        [-5.9024e-01, -3.9568e-01,  8.9263e-01],
        [-1.5438e+00,  1.9837e-01, -1.8893e-01],
        [-1.3498e+00,  1.0605e+00, -4.5441e-01],
        [ 8.3507e-01, -2.3567e+00,  3.7747e+00],
        [-7.1155e-01, -2.8569e+00,  3.6213e+00],
        [ 3.4686e-01,  9.5140e-01, -8.1027e-01],
        [-1.0408e+00, -6.6389e-01,  2.6178e+00],
        [-1.4528e+00,  4.9869e-01, -1.0805e+00],
        [-2.7764e+00,  2.4905e+00,  1.3921e+00],
        [-6.0615e-02, -5.8893e-01, -1.4642e+00],
        [-2.4471e+00,  1.7628e-01,  8.1139e-01],
        [-1.6381e+00,  9.7203e-02,  3.5979e-01],
        [ 9.7202e-02,  1.2025e+00, -1.1866e+00],
        [-5.3881e-02,  4.2204e-01, -6.9428e-02],
        [ 6.6670e-02,  7.2076e-01,  3.5091e-01],
        [-1.2738e+00,  7.3734e-01,  1.1514e+00],
        [-8.7887e-01,  2.3876e-01,  4.7861e-01],
        [-2.0491e+00, -6.0125e-03,  6.6002e-01],
        [-1.5353e+00, -1.5245e+00,  1.1445e+00],
        [-7.4143e-01, -7.9259e-01, -9.0356e-01],
        [-1.1574e+00,  2.1662e+00,  5.4576e-01],
        [-1.6117e+00,  7.7643e-01,  4.4879e-01],
        [-9.3532e-01, -3.5473e-01,  5.8808e-01],
        [-4.2058e-01,  1.4687e+00,  1.7795e+00],
        [-1.0494e+00, -1.9784e+00,  3.2477e+00],
        [ 1.5343e-02, -1.9958e+00,  3.6159e+00],
        [ 8.9084e-02, -1.6896e+00,  3.8508e+00],
        [ 5.5985e-01, -9.4696e-01, -1.8416e+00],
        [ 7.3902e-01,  2.6831e-01, -5.4072e-01],
        [-2.1338e+00,  5.5915e-01,  1.7140e+00],
        [-1.0240e+00,  7.9804e-01,  1.0616e+00],
        [-1.8686e+00, -1.7261e+00,  2.9687e+00],
        [-1.1264e+00,  9.4210e-01, -1.3313e+00],
        [-2.1057e+00, -7.1415e-01,  7.5900e-01],
        [ 1.3716e+00, -4.6786e-01,  2.6150e+00],
        [-1.7056e+00,  2.1217e-01,  4.7207e-01],
        [-3.7371e-01, -8.9664e-01,  6.9995e-01],
        [-9.4026e-01,  1.3960e+00, -1.3853e+00],
        [-1.1741e+00,  5.3262e-01,  7.5699e-01],
        [-4.2028e-01,  3.1709e-01, -3.2653e-01],
        [-7.8240e-01,  2.0668e-01, -8.0678e-01],
        [-2.2114e+00, -1.4430e+00,  4.2942e-01],
        [-1.9864e+00,  1.3579e+00,  9.8791e-01],
        [ 2.2159e-01,  5.4169e-01,  7.1343e-02],
        [-4.2870e-01,  7.5251e-01,  1.1448e+00],
        [-1.1840e+00,  3.3829e-01, -8.5359e-01],
        [-1.0050e+00,  9.6335e-01,  7.9950e-02],
        [-1.1897e+00, -3.2953e-01,  2.3229e-01],
        [-1.7295e+00,  4.1530e-01,  1.2558e+00],
        [-1.7334e+00,  8.1097e-01,  7.5509e-01],
        [ 4.5240e-01, -1.8962e+00,  3.2830e+00],
        [-1.6351e+00,  2.1322e+00,  7.0886e-01],
        [-8.9079e-01,  5.0331e-01,  2.2297e-01],
        [-4.8453e-01,  5.2839e-01,  1.1478e+00],
        [ 5.6674e-01,  1.5571e+00, -1.2816e+00],
        [-7.5434e-01,  6.3037e-01, -9.6174e-01],
        [-1.2246e+00,  6.6154e-01,  7.3736e-01],
        [-1.2846e+00, -1.2393e+00,  1.0661e+00],
        [-2.1946e+00,  8.7899e-01,  1.0732e+00],
        [-1.5483e+00,  1.3592e+00,  7.6587e-01],
        [-3.9022e+00, -9.7721e-01,  1.5639e+00],
        [-1.4558e+00,  5.4211e-01, -9.1082e-01],
        [-1.1591e+00, -1.4588e+00,  6.9047e-01],
        [-7.1129e-03,  6.3271e-01,  9.1849e-01],
        [-2.5154e-01,  2.3651e-01,  6.0027e-01],
        [ 2.6772e-02,  2.0556e+00,  5.8512e-01],
        [ 8.8189e-01,  2.2224e+00,  2.9668e-01],
        [-2.9756e+00,  1.0985e+00,  1.2500e+00],
        [-1.1526e+00,  4.5146e-01,  4.8426e-01],
        [-1.8299e-01,  1.6979e-02,  3.8559e-01],
        [-6.9692e-01, -3.0056e-02, -1.4287e+00],
        [-4.8237e-01,  1.9316e+00,  4.2259e-01],
        [-1.6854e+00,  3.4294e-01,  1.2178e+00],
        [-4.5949e-01,  1.2989e+00,  8.1546e-01],
        [-6.1456e-01,  1.1157e+00,  4.5083e-01],
        [-1.8184e+00,  5.4676e-02, -3.7463e-01],
        [-3.4473e-02,  2.3360e+00,  1.0383e+00],
        [-1.2488e+00, -8.0504e-01, -3.3385e-01],
        [ 9.9285e-01,  2.7964e-02,  3.2102e+00],
        [-8.8749e-01,  1.4524e-01,  1.1298e+00],
        [-1.2368e+00,  4.3841e-01,  1.5420e-01],
        [-4.0389e-01,  5.7566e-01,  7.2260e-01],
        [-1.1846e+00,  7.1946e-01, -6.3639e-01],
        [ 3.4075e-01, -1.0634e-01,  2.9594e+00],
        [ 1.3938e-01,  8.6284e-01,  3.6041e-01],
        [ 1.0856e+00, -4.2679e-01,  2.6861e+00],
        [-1.6407e-01,  8.0421e-01, -9.4834e-01],
        [ 6.1353e-01,  8.4488e-01,  1.0507e+00],
        [ 8.2888e-01,  7.2367e-01,  1.9940e+00],
        [-2.0063e+00,  7.9645e-01, -1.9738e-01],
        [ 8.5331e-01, -7.2715e-01,  3.0311e+00],
        [-1.7589e+00, -5.2183e-02, -9.8661e-02],
        [-2.0395e+00,  3.5329e-01,  8.0146e-01],
        [-1.5918e+00, -1.1552e+00,  7.8834e-01],
        [-1.5299e+00, -2.6291e-02,  7.9996e-03],
        [-1.7351e+00, -2.4550e-01, -2.5847e-01],
        [-1.4295e+00, -7.6457e-01,  4.6208e-02],
        [-9.9583e-01, -9.3556e-02, -1.4403e+00],
        [-1.4871e+00, -4.5311e-01,  6.1936e-01],
        [-4.1802e-01,  1.1688e-01, -7.8595e-01],
        [-1.1523e+00,  2.4491e-02, -2.9351e-01],
        [-5.9093e-01,  1.9105e+00, -5.1995e-01],
        [ 7.7721e-01, -1.3593e+00,  3.3858e+00],
        [-9.1518e-01,  8.0275e-01,  9.5890e-01],
        [-2.2750e-01, -1.8266e+00,  3.3384e+00],
        [ 2.7730e-01,  1.2208e+00,  9.7127e-01],
        [-9.6178e-01,  1.0667e-01, -3.4303e-01],
        [-1.8531e+00,  1.3073e+00,  8.1337e-01],
        [-3.7563e-01,  1.6337e+00,  9.2823e-02],
        [-2.3652e-01, -6.3453e-01,  1.4022e+00],
        [-5.8575e-01,  1.3603e+00,  8.6228e-01],
        [-1.0755e+00,  1.0203e+00,  2.0356e+00],
        [-1.4232e-01,  1.4331e+00,  1.3271e+00],
        [-2.1230e+00,  7.4861e-02, -1.2452e-02],
        [ 9.0349e-01, -9.5652e-01,  4.5628e+00],
        [-2.2304e+00,  2.3288e-01, -5.6406e-01],
        [-2.9795e+00, -5.0247e-01,  9.0094e-01],
        [ 8.4436e-02, -5.5918e-01,  1.7609e+00],
        [-5.6111e-01,  1.4484e+00, -3.2487e-02],
        [-9.0813e-01, -8.9094e-01,  3.8813e-01],
        [-3.8774e-01,  1.2441e+00, -1.8984e-02],
        [-1.0867e+00,  1.3102e-01,  8.1510e-01],
        [-3.1448e-01,  7.9313e-01,  4.2150e-01],
        [ 2.6091e-01,  1.3438e+00,  5.8426e-01],
        [ 8.8848e-02,  7.4068e-01, -1.7365e-01],
        [-6.8654e-01,  6.5887e-01,  3.0759e-01],
        [-6.2399e-01,  7.2245e-01, -9.3489e-01],
        [-6.6849e-01, -6.9032e-01,  5.6437e-01],
        [ 6.2416e-01, -1.4460e+00,  3.6953e+00],
        [ 5.4436e-01,  7.1316e-01, -7.0758e-01],
        [-4.2682e-01, -5.6119e-01, -9.4868e-01],
        [ 4.0838e-01,  1.0266e+00, -3.6393e-02],
        [-1.6750e+00,  5.5569e-01, -4.6852e-01],
        [-3.4000e-01,  1.1393e-02, -3.2556e-01],
        [-1.4162e+00, -1.0592e-01, -5.5849e-01],
        [-8.1143e-02, -6.5380e-01,  3.7908e+00],
        [-1.5845e+00, -7.8558e-01,  7.4579e-01],
        [-5.7949e-01, -1.7874e-01,  7.8726e-02],
        [ 8.2644e-01,  2.7601e-01,  2.2413e+00],
        [-7.6657e-01,  7.3718e-01,  8.5442e-01],
        [-1.2293e+00,  1.2686e+00,  6.9218e-01],
        [-3.0668e+00,  1.2112e+00,  1.9379e+00],
        [-1.0283e+00,  7.3783e-02,  4.7118e-01],
        [ 6.2753e-01,  1.8159e-01, -5.7493e-02],
        [ 5.1686e-01,  1.8144e-01,  5.9596e-01],
        [-2.5118e-01,  1.6571e+00, -4.7668e-01],
        [-1.0675e+00,  7.8012e-01,  1.3190e+00],
        [-6.5581e-01,  1.7151e+00,  1.2235e-01],
        [-1.2727e+00,  1.0199e+00, -3.7245e-01],
        [-5.3483e-01,  1.9798e-01,  6.0294e-02],
        [-6.5466e-01, -5.8260e-01, -6.7863e-02],
        [-2.0800e+00,  8.5495e-01, -4.5226e-01],
        [-1.3194e+00,  3.9475e-01,  9.6383e-01],
        [-1.4202e+00, -3.5983e-01, -3.8564e-01],
        [-6.4434e-01,  5.9005e-01, -9.5175e-03],
        [ 2.3191e+00, -3.0830e-01,  1.9059e+00],
        [-1.5486e+00, -6.9397e-01,  1.1689e+00],
        [-1.1583e-01,  8.8415e-01, -5.3962e-01],
        [-8.2606e-01,  3.2652e-01,  7.2071e-01],
        [-2.3516e-01,  1.5153e+00,  1.3564e+00],
        [-1.4435e+00,  2.7176e-01, -1.1169e+00],
        [-5.9393e-01,  4.0855e-01,  1.1614e+00],
        [-2.5566e-01, -3.0864e-01, -1.0007e+00],
        [-4.3250e-01, -1.1936e-01, -2.0762e+00],
        [-1.2474e+00, -1.1510e-01, -6.3027e-01],
        [-2.9958e-01,  1.1443e+00,  8.0111e-01],
        [-8.9575e-01,  1.0949e+00,  1.4288e-01],
        [-6.2063e-01, -4.7976e-01,  3.5073e-01],
        [-1.3387e+00, -1.3569e-01,  3.9756e-01],
        [-1.2003e+00,  2.6125e-01,  1.1733e+00],
        [-7.6686e-01, -6.1771e-01,  1.2550e+00],
        [-9.2869e-01,  1.2860e+00, -3.0063e-01],
        [-2.1060e+00, -3.4463e-01,  1.6100e-01],
        [-1.1780e+00,  9.6719e-01, -7.2679e-01],
        [ 4.8665e-01,  5.3106e-01,  7.5528e-01],
        [-3.4465e-01,  4.5430e-01, -8.2599e-01],
        [-2.5772e+00,  6.9912e-02,  1.6443e+00],
        [-1.6195e+00, -1.2202e+00,  1.3170e+00],
        [-5.9418e-02,  1.7779e+00, -1.2226e-01],
        [-4.6616e-01,  4.5179e-01,  7.0613e-02],
        [-1.9716e+00,  4.3298e-01,  2.3367e-01],
        [-5.7289e-01, -1.9030e-01,  4.6143e-01],
        [ 8.4843e-01,  1.7002e+00,  8.1340e-01],
        [-1.8614e+00,  1.4330e+00, -6.7990e-02],
        [-1.7615e+00,  1.5598e+00,  1.3511e+00],
        [-3.9001e-01,  1.8860e+00,  9.9850e-01],
        [-7.7928e-01, -1.1278e-01,  1.0219e+00],
        [-7.7354e-01,  1.6714e-01,  7.6557e-01],
        [ 1.5366e+00, -1.4794e+00,  3.0347e+00],
        [-3.0104e-01, -2.5577e+00,  2.4587e+00],
        [-8.2459e-02,  1.1530e+00, -2.0418e-01],
        [-3.8833e-01,  7.1258e-01,  3.3778e-02],
        [-3.9731e-01,  9.9659e-01,  9.6430e-01],
        [-1.5785e+00,  4.3423e-01,  4.0778e-01],
        [ 2.7502e-01, -3.1958e-01,  1.8847e-01],
        [-5.4880e-01,  4.6072e-01, -7.4082e-02],
        [ 2.7235e-01,  1.0413e+00,  9.3630e-01],
        [-1.0816e+00, -5.6754e-02,  8.9485e-01],
        [-9.5284e-01,  1.2289e+00,  2.9550e-01],
        [ 4.4752e-01, -7.8623e-01,  3.4619e+00],
        [-3.0337e-01,  1.8782e-01,  2.4715e+00],
        [-1.0320e+00,  1.0259e+00, -4.0645e-02],
        [-8.3947e-01, -9.5097e-01,  1.1676e+00],
        [-1.6191e+00, -1.7092e+00,  3.2329e+00],
        [-1.3585e+00,  5.6294e-01,  4.2731e-01],
        [-4.2916e-01, -6.3644e-01, -3.4364e-01],
        [-1.5230e-01, -9.6440e-01, -6.8531e-02],
        [-1.2641e+00,  2.0710e-01,  2.6401e-01],
        [ 5.9057e-01,  7.4106e-01, -1.5597e+00],
        [ 8.3220e-01, -3.7264e-01, -5.9845e-01],
        [-2.1414e+00, -9.5881e-02,  2.1518e-01],
        [ 5.4558e-01, -3.5572e-01,  4.7433e-01],
        [-3.4205e+00, -7.6851e-01,  1.6248e+00],
        [-1.7614e-03, -7.8799e-03,  1.6674e+00],
        [-3.0674e-01,  3.5585e-01,  9.6178e-01],
        [-2.6178e+00, -1.5361e+00,  2.6847e-01],
        [-5.6161e-01, -1.6283e-01, -2.0644e-01],
        [-1.4141e+00,  8.5438e-01,  6.8718e-01],
        [-4.7360e-01, -2.0969e+00,  3.1209e+00],
        [-8.8464e-01,  9.1711e-01,  1.1671e+00],
        [-1.5287e+00, -7.3192e-01, -1.3033e+00],
        [-1.0985e+00,  6.8275e-01,  1.3757e+00],
        [-5.6778e-01, -7.7949e-01,  9.8963e-01],
        [-1.0728e-01, -7.9537e-02, -2.1997e-01],
        [-1.1710e+00,  3.6385e-01,  2.3483e-01],
        [-3.0829e-01,  1.4426e+00, -5.6626e-01],
        [-1.0764e+00,  2.6490e-02, -4.8903e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-02-28 13:48:01 - INFO - y batch shape : torch.Size([256, 3])
2025-02-28 13:48:01 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 13:48:01 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-02-28 13:48:01 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 13:48:01 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 13:48:01 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 13:48:01 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 13:48:01 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 13:48:07 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-28 13:48:07 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-28 13:48:07 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 13:48:08 - INFO - Output s : tensor([[ 9.6786e-01,  5.3600e-01, -3.2233e+00],
        [ 3.3919e-01, -6.1838e-01, -3.4581e+00],
        [ 1.6052e+00, -3.3088e-02, -1.6561e+00],
        [ 1.7868e+00,  1.8955e+00, -1.1580e+00],
        [ 1.9805e+00,  1.0674e+00, -9.5854e-01],
        [ 9.1873e-01,  2.7752e-01, -2.9250e+00],
        [ 2.2452e+00,  1.0779e+00, -1.8760e+00],
        [ 1.1071e+00,  2.2317e-01, -2.8102e+00],
        [ 1.4113e+00,  7.4547e-01, -8.3680e-01],
        [ 4.8051e-01,  5.4769e-01, -2.7320e+00],
        [ 1.3315e+00,  5.2043e-01, -3.0544e+00],
        [ 1.2117e+00,  1.7847e+00, -2.3514e+00],
        [ 1.1266e+00, -2.2947e+00, -4.7750e-01],
        [ 6.0780e-01, -1.4165e-01, -3.2528e+00],
        [ 3.6265e-01, -7.5179e-01, -9.4500e-01],
        [ 1.0717e+00,  9.5797e-01, -2.4458e+00],
        [ 1.6069e+00,  1.3608e+00, -2.2682e+00],
        [ 1.5112e+00,  4.8800e-01, -2.1570e+00],
        [ 1.5248e+00, -4.1232e-01, -1.9319e+00],
        [ 1.4659e+00,  1.6672e+00, -2.1348e+00],
        [ 1.7949e+00,  5.1187e-02, -2.3444e+00],
        [ 1.4105e+00,  1.7254e+00, -2.2431e+00],
        [ 1.9735e+00,  4.7886e-01, -3.2376e+00],
        [-2.0789e-01, -2.9884e-01, -2.8933e+00],
        [ 4.5098e-01,  5.0665e-01, -2.8611e+00],
        [ 3.3501e-01,  2.3958e-01, -3.3694e+00],
        [ 3.0701e+00,  1.5668e+00, -1.0304e+00],
        [ 1.9684e+00,  7.1384e-01, -2.4229e+00],
        [ 4.1478e-01,  1.8669e+00, -1.9950e+00],
        [ 1.5652e+00, -1.5552e-01, -2.2285e+00],
        [ 1.7738e+00, -1.6627e+00, -1.3097e+00],
        [ 8.7082e-01,  4.1752e-01, -2.9163e+00],
        [ 2.3035e+00,  1.6197e+00, -2.1613e+00],
        [ 5.0111e-01,  1.2251e+00, -2.2508e+00],
        [ 1.0041e+00,  6.1116e-01, -1.8789e+00],
        [-8.8829e-01,  5.2528e-01, -1.2990e+00],
        [ 1.4745e+00,  1.1722e-01, -3.1336e+00],
        [ 8.8248e-01,  1.9038e+00, -3.4855e+00],
        [ 4.4310e-02, -9.2193e-01, -2.6615e+00],
        [ 8.3530e-01,  4.6434e-01, -2.0735e+00],
        [ 2.0811e+00,  4.7135e-01, -2.5925e+00],
        [ 1.6418e-01,  2.1746e-02, -1.8145e+00],
        [ 2.8683e+00,  1.0257e-01, -4.4644e+00],
        [-2.0167e-01,  7.6962e-01, -2.9428e+00],
        [ 1.8421e+00,  7.7914e-01, -1.8169e+00],
        [ 1.2761e+00,  9.9413e-01, -1.5399e+00],
        [ 1.8013e+00,  1.5818e+00, -1.5399e+00],
        [ 5.2708e-01,  7.7439e-01, -3.0360e+00],
        [ 1.8489e+00,  1.2596e-01, -2.6205e+00],
        [ 5.3342e-01,  1.7890e+00, -1.2308e+00],
        [ 4.0809e-01,  2.8856e-01, -2.8693e+00],
        [ 1.4136e+00, -2.2739e+00, -1.1500e+00],
        [ 7.2573e-01,  1.3252e+00, -1.5676e+00],
        [-4.2068e-01,  6.7880e-01, -2.8943e+00],
        [ 6.2472e-01, -1.5732e-01, -1.9516e+00],
        [ 2.7849e+00,  4.5928e-01, -2.4225e+00],
        [ 7.8877e-01,  4.5166e-01, -1.6115e+00],
        [ 1.5647e+00,  2.0606e+00, -1.8255e+00],
        [ 2.0121e+00,  1.4881e+00, -2.5309e+00],
        [ 5.3915e-01,  1.0147e+00, -2.9062e+00],
        [ 1.6694e+00,  1.2952e+00, -2.7143e+00],
        [ 9.5019e-01,  1.3428e+00, -2.5553e+00],
        [ 6.6195e-01, -7.8214e-01, -1.1017e+00],
        [ 1.0833e+00,  9.1809e-01, -2.7589e+00],
        [ 1.8794e+00,  6.5739e-01, -3.8740e+00],
        [ 8.0512e-01, -1.8008e-01, -2.7261e+00],
        [ 8.2637e-01, -5.2181e-01, -2.7331e+00],
        [ 1.3191e+00,  2.6317e-01, -3.1555e+00],
        [ 3.1813e-01,  1.0125e+00, -1.9916e+00],
        [ 1.8471e+00,  5.5409e-01, -3.2523e+00],
        [-6.1965e-02, -4.0369e-01, -2.9800e+00],
        [ 1.4550e+00, -1.9463e-01, -2.1566e+00],
        [ 1.7936e+00,  2.9837e-01, -3.1289e+00],
        [ 9.5755e-01,  1.1913e+00, -2.4934e+00],
        [ 1.4232e+00, -1.3173e-01, -2.7336e+00],
        [ 1.7784e+00,  1.6495e+00, -1.7057e+00],
        [ 1.3599e+00,  9.9666e-01, -2.8371e+00],
        [ 2.6791e-01,  1.2929e+00, -2.5444e+00],
        [ 1.7470e+00,  7.9811e-01, -2.5500e+00],
        [ 1.1766e+00,  1.9330e+00, -1.5577e+00],
        [ 1.5459e+00, -1.5914e+00, -2.1372e+00],
        [ 1.4758e+00,  8.5964e-01, -2.5655e+00],
        [ 1.4081e+00,  5.7053e-01, -1.6977e+00],
        [-4.8286e-01,  1.2364e+00, -3.7445e+00],
        [-3.5579e-01,  1.2678e-01, -7.0088e-01],
        [ 7.6463e-01, -1.0420e+00, -7.2875e-01],
        [ 1.7366e+00,  2.0155e-01, -2.4062e+00],
        [ 1.4408e+00,  1.1097e+00, -2.2729e+00],
        [ 2.6805e+00, -2.8335e-01, -2.4309e+00],
        [ 5.1932e-01, -1.2409e-01, -3.3660e+00],
        [ 5.4822e-01,  4.3139e-01, -2.0990e+00],
        [ 2.2239e+00,  1.2952e+00, -2.3119e+00],
        [ 7.6075e-01, -6.3007e-01, -3.2912e+00],
        [-1.2816e+00, -6.0431e-01, -3.8557e+00],
        [ 1.3128e+00,  2.0103e-01, -2.7250e+00],
        [ 8.1368e-01,  1.3461e-01, -3.0011e+00],
        [-3.5650e-01,  9.0898e-01, -2.4800e+00],
        [ 1.8994e+00, -3.9928e-01, -3.4882e+00],
        [ 2.0381e+00, -1.1963e-01, -2.0189e+00],
        [ 1.1232e+00,  1.2171e+00, -3.2178e+00],
        [ 6.6241e-01,  6.0103e-01, -1.3117e+00],
        [ 1.6252e+00,  6.1558e-01, -3.6019e+00],
        [ 2.1423e+00,  9.5951e-01, -2.2527e+00],
        [ 2.1568e+00,  1.1199e+00, -2.4755e+00],
        [ 8.8041e-01,  9.8327e-01, -2.4187e+00],
        [ 1.2056e+00,  2.4901e-01, -1.5641e+00],
        [-1.4833e-01, -1.8831e+00,  2.5609e-01],
        [ 7.5443e-01,  7.8025e-01, -3.2387e+00],
        [ 1.7971e+00,  1.4309e+00, -2.4937e+00],
        [ 1.6545e+00,  6.0429e-01, -2.1234e+00],
        [ 9.4987e-01, -2.6148e+00, -9.5689e-01],
        [ 6.2384e-01,  4.1710e-01, -2.8335e+00],
        [ 2.0602e+00, -2.9448e+00, -1.5542e+00],
        [ 9.8207e-01, -6.8977e-02, -2.7962e+00],
        [ 1.8089e+00,  1.4365e-01, -3.1915e+00],
        [ 1.3893e+00, -1.7259e-01, -3.0310e+00],
        [ 8.1131e-01,  1.0352e-01, -3.8251e+00],
        [ 1.5108e+00, -1.7014e-01, -4.5014e-01],
        [ 4.6899e-01, -1.4786e+00, -1.7407e+00],
        [ 2.0230e+00, -1.1503e+00, -1.5558e+00],
        [-8.2091e-01,  4.7125e-01, -2.4228e+00],
        [ 2.0969e+00,  2.0889e+00, -2.7273e+00],
        [ 1.2704e+00,  1.4395e-01, -2.1011e+00],
        [ 1.2166e+00,  1.9956e+00, -2.6749e+00],
        [ 1.0057e+00, -1.9636e-01, -2.3516e+00],
        [ 1.4232e+00,  7.3468e-02, -3.0868e+00],
        [ 5.3952e-01, -7.3201e-01, -2.4936e+00],
        [ 1.7194e+00,  3.0664e-01, -1.9589e+00],
        [ 1.4956e+00,  9.9741e-01, -3.1145e+00],
        [ 7.1322e-01,  6.5136e-01, -1.6820e+00],
        [ 8.3687e-01,  1.1166e-01, -2.3292e+00],
        [ 1.7045e+00, -3.7950e-02, -1.7108e+00],
        [ 1.3484e+00, -5.7964e-01, -2.2863e+00],
        [ 9.6554e-01, -2.0214e+00, -1.5024e+00],
        [ 1.7587e+00, -9.4091e-01, -1.4733e+00],
        [ 9.0109e-01, -1.0955e-01, -3.1273e+00],
        [ 1.6642e+00, -4.2331e-01, -2.0828e+00],
        [ 1.9236e-01,  1.7283e+00, -4.6826e+00],
        [ 1.0366e+00,  1.7059e+00, -2.7499e+00],
        [ 4.6235e-01, -1.0115e+00, -2.7037e+00],
        [-3.5903e-01, -7.3501e-01, -1.4217e-01],
        [ 2.1336e+00,  4.7437e-01, -2.2716e+00],
        [-5.5429e-01,  1.1761e+00, -2.8620e+00],
        [ 2.4651e+00,  1.9651e-02, -3.5356e+00],
        [ 2.2513e-01,  1.4050e-02, -2.1710e+00],
        [ 2.3131e+00, -3.4527e-01, -1.8876e+00],
        [ 9.9772e-01,  9.5892e-01, -3.0272e+00],
        [ 9.7119e-01,  1.2553e+00, -2.3668e+00],
        [ 1.8164e+00, -2.8109e-01, -2.9053e+00],
        [ 5.6526e-01, -4.6961e-03, -1.7689e+00],
        [ 1.9006e+00, -2.5954e+00, -1.2191e+00],
        [ 1.0415e+00,  1.0583e+00, -2.2619e+00],
        [ 1.4865e+00,  1.8147e-01, -2.3471e+00],
        [ 1.4773e+00,  1.2446e+00, -2.7248e+00],
        [ 1.7753e+00, -1.3286e+00, -5.8147e-01],
        [ 1.9081e+00, -8.0278e-01, -1.1258e+00],
        [ 1.2565e+00,  1.5544e+00, -3.5082e+00],
        [ 1.5368e+00,  5.8037e-01, -2.5132e+00],
        [ 1.4531e+00,  3.5395e-01, -3.0066e+00],
        [ 1.3211e+00,  2.0085e-01, -2.3267e+00],
        [ 8.2661e-01,  1.1216e+00, -1.7001e+00],
        [ 1.9112e+00,  7.4105e-01, -2.3083e+00],
        [ 2.4008e+00,  6.7828e-01, -2.8013e+00],
        [ 1.7809e+00, -2.0155e+00, -4.9397e-01],
        [ 1.7185e+00,  3.9820e-01, -3.1950e+00],
        [ 1.5366e+00, -5.2526e-01, -2.1485e+00],
        [ 7.8948e-01, -6.3153e-02, -2.3909e+00],
        [ 5.1788e-01, -5.5346e-01, -2.2699e+00],
        [ 8.6271e-01, -3.6777e-01, -2.5095e+00],
        [ 8.4437e-01,  1.7974e+00, -1.4484e+00],
        [ 1.1060e+00,  5.4332e-01, -2.9682e+00],
        [ 1.1446e+00,  1.3898e+00, -1.9618e+00],
        [ 1.2644e+00, -3.2127e-01, -2.4530e+00],
        [ 1.5724e+00,  7.6995e-01, -1.5324e+00],
        [ 1.9523e+00,  1.3800e+00, -8.5250e-01],
        [ 1.2955e+00,  5.8311e-01, -2.5870e+00],
        [ 9.3936e-01,  3.0624e-01, -2.5750e+00],
        [ 1.1107e+00,  2.1948e-01, -2.5969e+00],
        [ 7.9800e-01,  9.1528e-01, -1.0445e+00],
        [ 1.0695e+00, -3.4077e-01, -2.4277e+00],
        [ 1.0146e+00,  1.1160e+00, -3.0927e+00],
        [ 2.4272e+00,  1.7528e+00, -2.3890e+00],
        [ 1.8324e+00,  1.0741e+00, -2.2497e+00],
        [ 2.2907e+00, -2.1889e+00, -5.6651e-01],
        [-6.2230e-01,  1.6710e+00, -2.3240e+00],
        [ 1.3425e+00,  1.1916e+00, -1.9558e+00],
        [ 8.2280e-01,  2.5430e-01, -3.3355e+00],
        [ 1.2491e+00, -1.2543e+00,  1.1219e+00],
        [ 2.0025e+00,  5.6885e-02, -1.9475e+00],
        [ 2.4341e+00,  2.3659e+00, -3.0395e+00],
        [ 8.3953e-01, -4.5928e-01, -2.5655e+00],
        [ 2.2288e+00, -6.5943e-02, -2.1442e+00],
        [ 1.8190e-01, -1.4790e-01, -2.9904e+00],
        [ 1.2296e+00, -1.6966e+00, -6.4389e-01],
        [ 8.9231e-01,  1.9241e-01, -2.7682e+00],
        [ 9.0187e-01,  9.4881e-01, -2.7468e+00],
        [ 1.3487e+00,  8.0726e-01, -2.0894e+00],
        [ 2.9079e+00,  8.0930e-01, -1.8568e+00],
        [-6.2995e-01,  2.6447e-01, -9.8677e-01],
        [ 4.1515e-01,  2.0671e+00, -2.7417e+00],
        [-4.1179e-02, -1.5146e+00, -1.3523e+00],
        [ 1.2610e+00, -5.4867e-01, -2.2817e+00],
        [ 1.3428e+00,  1.9761e+00, -3.3631e+00],
        [ 5.4976e-01,  1.8364e-01, -2.6353e+00],
        [ 1.3499e+00,  3.9100e-02, -2.5087e+00],
        [ 3.0869e-01,  9.1211e-01, -3.4754e+00],
        [ 3.5489e-01,  2.3731e+00, -2.7037e+00],
        [ 4.9149e-01,  5.5237e-01, -1.9681e+00],
        [ 2.2619e-01,  3.0469e+00, -2.3809e+00],
        [ 1.4727e+00,  6.8330e-01, -2.9676e+00],
        [ 1.8839e+00,  1.1438e+00, -3.9022e+00],
        [ 8.2979e-01,  5.4823e-01, -2.5255e+00],
        [ 5.0589e-01, -1.2732e-01, -3.6322e+00],
        [ 1.7419e+00,  2.8732e-01, -2.8650e+00],
        [ 1.9715e+00,  9.6956e-02, -3.2865e+00],
        [ 2.9066e+00,  1.4837e+00, -2.3988e+00],
        [ 1.0909e+00,  3.8479e-02, -2.1491e+00],
        [ 9.8672e-01, -1.6944e-01, -2.9213e+00],
        [ 1.1371e+00, -8.9919e-01, -1.5847e+00],
        [ 1.0477e+00,  4.8976e-02, -1.9181e+00],
        [ 1.4415e+00, -5.1764e-01, -2.3796e+00],
        [ 1.1784e+00,  1.9608e+00, -1.4399e+00],
        [ 1.4568e+00,  1.9057e+00, -2.5651e+00],
        [ 1.5806e+00,  5.7930e-01, -3.0000e+00],
        [ 9.5680e-01,  7.8074e-01, -1.4837e+00],
        [ 1.5380e+00,  1.9639e+00, -2.0415e+00],
        [ 2.2640e+00, -5.0732e-01, -1.7803e+00],
        [ 1.9463e+00, -2.6710e-01, -2.7288e+00],
        [ 3.4938e+00, -9.6428e-04, -2.7249e+00],
        [-4.1324e-01,  1.4901e+00, -1.9613e+00],
        [ 1.8033e+00,  2.9463e-01, -3.9005e+00],
        [ 2.7316e+00,  8.4813e-01, -2.8209e+00],
        [-1.3212e+00, -4.3646e-01, -3.1296e+00],
        [ 2.2757e+00,  9.3624e-01, -1.8831e+00],
        [ 1.5490e+00,  8.8474e-01, -2.8265e+00],
        [ 1.7849e+00,  1.0626e+00, -3.1393e+00],
        [ 7.4750e-01, -6.1814e-01, -2.0851e+00],
        [ 1.1939e+00,  6.7141e-02, -2.2544e+00],
        [ 9.2930e-01,  7.8770e-01, -2.6608e+00],
        [ 1.1631e+00,  6.5448e-01, -1.9800e+00],
        [ 1.8278e+00,  9.4813e-01, -2.5486e+00],
        [ 1.0070e+00,  1.6949e+00, -1.9849e+00],
        [ 8.7329e-01,  2.1515e+00, -3.1819e+00],
        [ 1.9915e+00, -9.2909e-02, -1.4646e+00],
        [ 1.2989e+00,  1.2046e+00, -2.2424e+00],
        [ 3.3608e-01,  1.4010e+00, -3.2431e+00],
        [ 2.0628e+00,  1.9829e+00, -2.8120e+00],
        [ 1.9962e+00, -2.5829e-01, -3.1196e+00],
        [ 4.7018e-01, -9.6449e-01,  4.8383e-01],
        [ 2.0943e+00, -5.6227e-01, -3.0495e+00],
        [ 2.3206e+00,  4.7400e-01, -1.8682e+00],
        [ 3.7206e-01,  1.9329e+00, -2.7216e+00],
        [ 6.6261e-01,  4.3386e-01, -2.5388e+00],
        [ 6.4545e-01, -1.7473e-01, -2.3916e+00],
        [ 3.0578e-02,  2.3608e-02, -1.7672e+00],
        [-8.8390e-01, -4.3720e-02, -1.3756e+00]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-02-28 13:48:08 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 13:48:08 - INFO - Output shape after view : torch.Size([256, 3])
2025-02-28 13:48:08 - INFO - y batch shape after view : torch.Size([256, 3])
2025-02-28 13:48:14 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.596880, Accuracy = 24.22%, MSE = 0.9922
2025-02-28 13:49:13 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 0.899155, Accuracy = 45.21%, MSE = 0.7131
2025-02-28 13:50:09 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 1.073582, Accuracy = 56.12%, MSE = 0.5465
2025-02-28 13:51:07 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 0.747673, Accuracy = 57.51%, MSE = 0.5247
2025-02-28 13:52:05 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 0.687294, Accuracy = 62.85%, MSE = 0.4469
2025-02-28 13:53:05 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 0.669695, Accuracy = 66.19%, MSE = 0.3987
2025-02-28 13:54:01 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 0.679460, Accuracy = 68.28%, MSE = 0.3679
2025-02-28 13:54:57 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 0.778809, Accuracy = 69.89%, MSE = 0.3447
2025-02-28 13:55:21 - INFO - Epoch 2: Train Loss=1.1027, Train Acc=70.48%, Train MSE=0.3353
2025-02-28 14:27:44 - INFO - y batch shape : torch.Size([256, 3])
2025-02-28 14:27:44 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 14:27:44 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-02-28 14:27:44 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 14:27:44 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 14:27:46 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 14:27:46 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 14:27:46 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 14:27:59 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-28 14:27:59 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-28 14:28:02 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 14:28:05 - INFO - Output s : tensor([[ 1.4776e-01,  3.6699e-01,  1.1826e+00],
        [ 1.5131e+00,  7.7922e-01,  1.8355e+00],
        [-1.6623e+00,  2.2267e+00,  3.4122e+00],
        [-9.8606e-01,  4.4342e-02,  1.1759e+00],
        [ 1.1404e-01,  8.8073e-01,  9.8387e-01],
        [ 3.4088e-01,  1.5163e+00,  9.0319e-01],
        [ 7.9321e-01,  9.1856e-01, -4.7107e-02],
        [-1.4457e-01,  2.7038e+00,  2.3313e+00],
        [-5.7740e-02,  2.8957e-01,  1.5631e+00],
        [ 1.1072e-01,  2.4951e-01,  8.0795e-01],
        [-6.8604e-01,  1.1842e+00,  8.5333e-01],
        [-1.6736e+00, -3.3041e-01,  9.2432e-01],
        [ 1.4988e+00,  7.1427e-01,  5.6942e-01],
        [ 5.4587e-01,  4.0826e-01,  1.4002e+00],
        [-2.1487e+00,  8.6807e-01,  1.4923e+00],
        [ 4.5053e-02, -2.0201e-01, -4.1364e-01],
        [-8.6211e-01,  1.8574e+00,  2.7924e+00],
        [-8.6885e-01,  6.4267e-01,  1.2711e+00],
        [ 5.4072e-02,  2.0597e+00,  6.4404e-01],
        [-5.4483e-01,  3.1922e-01, -3.0565e-01],
        [-1.0419e-01,  5.6400e-01,  1.3581e+00],
        [-1.2265e-01,  1.2372e-01, -8.0761e-01],
        [ 7.6719e-01,  2.3430e-02,  1.1415e+00],
        [-5.9901e-01,  2.3273e+00,  1.6791e+00],
        [ 1.0835e+00,  1.8356e+00,  9.9532e-01],
        [ 4.8762e-02,  1.4521e+00,  1.8631e+00],
        [ 1.3701e+00, -2.5700e-01, -2.3359e-01],
        [ 1.6551e+00,  7.9718e-01,  4.5761e-01],
        [ 7.7870e-01, -1.5501e-01,  4.7244e-01],
        [-6.0018e-01,  1.2883e+00, -2.3775e-01],
        [ 5.0277e-01,  1.8011e+00,  9.9627e-01],
        [ 4.3064e-01,  6.7224e-02,  2.4155e-01],
        [ 8.5961e-01, -9.8027e-02,  1.8631e+00],
        [ 3.5129e-01, -1.2643e-01,  6.4684e-01],
        [-1.3030e-01,  1.8613e+00,  3.2470e-03],
        [-2.3672e+00,  1.9126e+00, -3.7606e-02],
        [ 7.5160e-01,  1.3995e-01,  9.6468e-01],
        [-1.3573e+00,  1.5040e+00,  8.6866e-01],
        [ 3.2871e-01, -2.6719e-01,  5.3876e-01],
        [-2.5292e-01,  7.3148e-01,  5.8441e-01],
        [ 9.5770e-01,  1.5331e+00,  2.6088e-01],
        [ 8.3326e-01,  6.8802e-01,  9.7186e-01],
        [-1.5165e-01,  1.4329e+00,  1.0157e+00],
        [ 6.5253e-01,  2.4629e-01, -3.5527e-01],
        [-1.0623e+00,  1.4100e+00,  2.0303e+00],
        [-2.4915e-01,  1.5916e+00,  2.3743e+00],
        [-1.5986e+00,  5.1463e-01,  1.4364e+00],
        [ 4.2074e-01, -8.5005e-01,  4.2615e-01],
        [ 1.4680e+00,  2.2906e+00,  1.2590e+00],
        [ 5.2799e-01,  2.8718e-01,  1.4043e+00],
        [ 9.4413e-01,  1.0205e+00, -1.8614e-01],
        [-3.7883e-01,  2.2594e+00,  2.1064e+00],
        [-1.9406e-01,  6.5519e-01,  9.5688e-01],
        [ 4.9022e-01, -1.3259e-01, -8.4133e-01],
        [-1.5705e+00,  1.1428e+00,  5.6395e-01],
        [ 1.4584e-01,  1.0853e+00,  2.0148e+00],
        [ 3.6690e-01,  6.5644e-01, -2.0060e-01],
        [-6.1990e-01,  1.2128e+00,  2.0328e+00],
        [-9.5856e-01,  1.6923e+00,  6.4524e-01],
        [ 2.2615e-01,  6.7522e-01, -1.0315e-01],
        [ 5.1767e-01,  1.0942e+00,  1.3734e+00],
        [ 6.8439e-01, -1.0286e+00, -1.8501e-01],
        [ 7.4815e-01,  3.6021e-01, -3.7795e-01],
        [ 9.7781e-01,  1.4677e+00,  1.9617e-01],
        [-8.7478e-01,  8.9655e-01,  3.2239e+00],
        [-4.6117e-01,  1.5011e+00,  2.6978e+00],
        [-1.5667e-01,  6.3640e-01,  1.9980e+00],
        [ 4.9465e-01,  7.7951e-01,  3.9446e-01],
        [ 7.9064e-01,  1.1540e+00,  2.1581e-01],
        [-9.2567e-01,  3.8989e-01,  7.5095e-01],
        [ 1.2036e-01,  1.6601e+00,  8.2842e-01],
        [ 2.7264e-02,  3.6341e-01,  9.4581e-01],
        [ 4.1676e-01,  7.7608e-01,  5.0017e-01],
        [-1.0548e+00,  1.1612e+00,  2.0259e+00],
        [-1.6634e+00,  1.2472e+00,  2.7871e+00],
        [-1.3416e+00,  8.0624e-01, -1.4851e-01],
        [-1.3238e+00,  6.6458e-02,  3.0945e-01],
        [-9.2602e-01,  2.0192e+00,  4.0051e-01],
        [ 2.6459e-01,  1.0386e+00,  7.5912e-01],
        [-2.3876e-01, -2.3884e-01,  5.0996e-01],
        [-1.4112e+00,  3.0289e-01,  5.8165e-01],
        [-1.0913e-01,  8.1214e-01, -4.6978e-01],
        [ 2.3738e+00,  1.3022e+00,  7.6453e-01],
        [ 1.9403e-01,  1.0623e+00,  5.5683e-01],
        [ 1.4474e-01,  4.6118e-01,  5.3859e-01],
        [ 2.7809e-01,  4.0668e-01,  6.4260e-01],
        [ 7.7035e-02, -3.9056e-01,  1.2048e+00],
        [-2.9331e+00,  7.4611e-01,  1.9757e+00],
        [-1.0502e+00,  2.5116e+00,  5.1721e-01],
        [ 1.1965e-01,  1.0058e+00,  5.0210e-01],
        [ 3.5957e-03,  1.9968e+00,  3.4119e+00],
        [-2.8827e-02,  1.8696e+00,  2.2913e+00],
        [ 7.2615e-01,  1.5497e+00,  6.9119e-01],
        [ 1.7749e-01, -1.6202e-02,  7.1697e-01],
        [ 1.1205e-01,  1.0311e+00,  6.1518e-01],
        [ 3.3962e-01, -3.9440e-01, -1.7193e-01],
        [ 1.3909e+00,  2.7906e-01,  5.2987e-01],
        [-4.8756e-01,  6.7710e-01,  1.7056e+00],
        [-7.6751e-01, -1.5346e+00,  1.7360e+00],
        [ 1.3655e+00,  1.7744e+00,  8.1439e-01],
        [ 6.8762e-01,  1.7562e+00,  6.8905e-01],
        [-1.5702e+00,  1.3221e+00,  1.5154e+00],
        [-3.6910e-02,  3.5168e-01,  4.7922e-02],
        [ 5.2370e-01,  1.1235e+00, -1.5537e-02],
        [-4.4885e-01,  2.0203e-01,  3.0032e-01],
        [-4.1513e-01,  1.2806e+00,  1.4049e+00],
        [ 6.2360e-01, -5.8676e-02,  9.0366e-01],
        [-8.5627e-02,  7.8494e-01,  1.2776e+00],
        [-1.1685e+00,  1.4365e+00, -4.5724e-01],
        [ 2.1299e-01,  1.3795e-01,  1.4743e+00],
        [ 1.4326e+00,  3.9518e-01,  1.4519e-02],
        [-1.3951e+00,  1.4722e+00,  1.8544e+00],
        [-1.0755e-01,  2.1701e+00,  1.7898e+00],
        [ 4.3937e-01,  5.7044e-01, -2.7203e-01],
        [-3.9742e-01, -2.6206e-01,  9.4183e-01],
        [-4.7876e-02,  4.4804e-01, -6.9529e-01],
        [-3.4192e-01,  1.0564e+00,  1.2635e+00],
        [ 1.0020e+00,  6.0138e-01,  4.4690e-01],
        [ 2.6141e-01,  1.3843e+00,  8.9928e-01],
        [ 1.8111e-01,  2.0791e+00,  2.1288e+00],
        [ 4.8693e-01,  3.8456e-01, -1.9784e+00],
        [-2.6163e-01, -1.8169e-01, -3.0542e-01],
        [-2.4704e+00,  2.0593e-01,  2.3360e+00],
        [ 1.7215e+00,  2.0909e-01,  1.3979e+00],
        [ 3.9067e-01,  1.5880e-01,  1.4357e+00],
        [ 5.2911e-02,  8.5731e-01,  1.5567e-01],
        [ 2.0343e+00,  9.3127e-01,  1.5762e+00],
        [ 2.8516e-01,  9.8412e-01, -5.4335e-02],
        [ 4.7217e-01, -3.3943e-01,  8.4368e-01],
        [ 4.1635e-01,  2.2940e-01,  1.2185e+00],
        [ 4.6505e-02,  6.4448e-01,  1.2373e+00],
        [ 1.4698e-01,  1.7815e+00,  2.7988e+00],
        [-1.3287e+00,  5.6779e-01,  1.0795e+00],
        [ 3.6263e-01,  5.0760e-01,  8.7970e-01],
        [ 4.0690e-01,  8.2594e-01,  7.0709e-01],
        [-1.2243e-01,  6.0772e-02,  1.7748e+00],
        [ 2.6645e-01,  2.4903e-01,  4.0394e-01],
        [ 1.3049e+00,  4.1218e-01,  9.9731e-01],
        [ 1.7264e+00, -4.1972e-01,  1.4122e+00],
        [-1.0585e+00,  7.3933e-01,  5.5642e-01],
        [-6.9679e-01,  1.7112e+00,  1.5615e+00],
        [ 1.5753e+00,  2.8801e-01,  1.1623e+00],
        [ 6.0997e-01,  6.8131e-01,  7.5523e-01],
        [-1.0143e+00,  5.4656e-01,  2.2711e+00],
        [ 1.0031e+00,  7.0819e-01,  6.9592e-01],
        [-4.7488e-01,  3.3147e-01,  2.6740e-01],
        [-1.0091e+00,  2.0529e-02,  2.9486e+00],
        [ 1.2284e+00,  1.3688e+00,  3.9655e-01],
        [-6.9433e-01,  2.3149e+00,  2.7195e+00],
        [ 4.7746e-01,  1.6938e+00,  1.4717e+00],
        [ 3.6335e-01,  1.5316e+00,  1.0867e+00],
        [ 9.1013e-01,  1.1785e+00,  7.7976e-01],
        [ 1.2544e+00, -1.2684e+00, -8.8161e-01],
        [-4.2531e-01, -3.4242e-01,  6.4408e-01],
        [ 3.8321e-02,  3.4366e-01, -9.9584e-01],
        [-6.0912e-01,  2.8337e-01,  1.7108e+00],
        [ 8.4787e-02,  1.5939e+00,  3.3285e-01],
        [ 2.2340e-01,  5.4013e-01,  1.9264e-01],
        [-8.8331e-01, -2.2674e-01,  2.0716e+00],
        [-1.0840e+00,  2.4560e+00,  1.4173e+00],
        [ 5.8891e-01,  1.8184e+00,  4.5070e-01],
        [ 1.8938e+00,  5.8633e-01,  1.3179e+00],
        [ 3.7699e-01,  4.9723e-01, -8.7347e-01],
        [-2.9289e-01,  9.9247e-01,  5.5700e-01],
        [-2.3747e+00, -4.2692e-01,  2.6567e-01],
        [ 8.2522e-01,  5.3507e-01,  3.8497e-01],
        [ 1.3612e+00, -2.3454e-01,  6.1205e-01],
        [-1.3621e+00,  1.6458e+00,  3.5797e+00],
        [ 8.0341e-02,  4.7537e-01,  5.9492e-01],
        [ 6.8692e-01,  3.8405e-01,  9.3168e-01],
        [-5.2502e-01,  5.1515e-01,  5.1640e-01],
        [-1.1871e+00,  1.0470e+00,  1.1891e+00],
        [-2.2254e-01,  9.5902e-01,  5.1146e-01],
        [ 2.4370e-02,  8.6469e-01,  8.0444e-01],
        [-4.1695e-01,  1.2208e+00,  6.5954e-01],
        [-4.8507e-01,  2.1173e+00,  1.3139e+00],
        [ 9.5755e-01,  2.5806e-01,  1.6989e+00],
        [ 9.3002e-01,  3.0067e-01,  4.2021e-02],
        [ 6.4468e-01,  7.8470e-02,  7.9216e-01],
        [-7.9855e-01,  7.7304e-01,  1.0239e+00],
        [-1.0432e+00,  1.8744e+00,  1.0946e+00],
        [ 2.6986e-01,  4.2109e-01,  5.1591e-01],
        [ 2.4615e-01,  2.2339e+00,  2.0922e-02],
        [ 1.2606e+00,  1.2855e+00,  5.7171e-01],
        [ 8.2486e-01,  9.8552e-01,  8.7366e-01],
        [ 9.7854e-01, -6.0496e-01,  3.9655e-01],
        [-7.2510e-01,  1.9794e+00, -1.2127e+00],
        [ 2.8919e-01,  8.8678e-02,  1.0140e+00],
        [-3.4041e-01,  1.9894e-01, -3.8637e-01],
        [ 9.2189e-01, -2.0144e-02,  2.2132e-01],
        [-4.1066e-01,  3.8384e-01,  1.5019e+00],
        [ 1.5887e+00,  4.8185e-01, -1.7491e-01],
        [-8.6464e-02,  2.0205e+00,  1.8399e+00],
        [-1.5657e+00,  3.1724e-01,  1.7700e+00],
        [-2.0343e-01,  2.3604e+00, -9.2628e-01],
        [-4.7763e-01,  9.2842e-02,  6.9014e-01],
        [ 1.4183e+00, -2.0230e-01, -7.4883e-01],
        [-1.1515e+00,  1.8088e+00,  1.1089e+00],
        [ 7.0854e-01,  4.8917e-01, -1.5816e+00],
        [ 1.3780e+00,  9.9010e-01,  7.5435e-01],
        [ 8.1512e-01,  6.8111e-01,  4.9628e-01],
        [ 7.4276e-01,  1.0124e+00,  4.9684e-01],
        [ 4.4613e-02,  6.5154e-02, -4.9469e-01],
        [ 1.3883e-01,  6.0848e-01,  1.0284e+00],
        [ 2.6778e-01,  1.4383e-02,  2.1973e+00],
        [ 2.0624e-01,  8.5422e-01,  2.0027e-01],
        [-3.0630e-01,  3.1898e+00,  2.2953e+00],
        [ 1.1369e-01,  9.1989e-01, -4.0428e-01],
        [ 1.3170e+00,  4.5338e-02,  1.1923e+00],
        [ 4.6405e-01,  9.5411e-02, -1.0219e+00],
        [-4.8391e-01,  2.5838e-01,  1.1467e+00],
        [ 9.1801e-01,  2.8413e-01,  1.3569e+00],
        [-1.2212e+00,  1.7024e+00,  3.2028e+00],
        [-3.6633e-01,  5.0101e-01,  2.0812e-01],
        [-9.7321e-02,  6.8711e-01,  1.2785e+00],
        [-1.6506e+00,  1.9196e+00,  1.3307e+00],
        [-8.1795e-01,  1.4057e+00,  1.3906e+00],
        [-5.3072e-01,  2.7500e-01,  2.4145e-01],
        [-1.2100e+00,  1.0925e+00,  6.1325e-01],
        [ 1.0665e+00,  4.7395e-01,  1.7649e-01],
        [ 6.8199e-01,  1.2468e+00,  3.7402e-01],
        [ 2.9673e-01,  1.0180e+00,  9.6469e-01],
        [ 1.6157e+00,  1.1756e+00,  4.5277e-01],
        [-1.3341e-02,  1.9721e+00,  3.3693e+00],
        [-8.0876e-01, -9.8651e-01,  7.3466e-01],
        [-7.5953e-01,  9.5565e-01,  1.2030e+00],
        [-4.8489e-01,  9.0435e-01,  3.8642e-01],
        [ 4.0532e-01,  3.5851e-01, -1.1657e+00],
        [-4.3746e-02,  9.4735e-01,  1.7171e+00],
        [ 7.0990e-01,  1.4380e+00,  1.1168e+00],
        [ 6.8842e-01,  1.5934e+00,  7.4213e-01],
        [ 1.3544e+00,  1.0858e+00,  5.3211e-01],
        [-6.5102e-01,  5.6939e-01,  1.1471e+00],
        [-5.0292e-01,  1.0784e+00,  4.5373e-01],
        [-1.4494e+00,  8.0599e-03,  8.6030e-01],
        [-4.0621e-01,  5.8351e-01,  2.4884e+00],
        [-5.2627e-02,  1.1695e+00,  1.7554e-01],
        [ 9.1949e-01,  3.9637e-01,  6.1046e-01],
        [ 4.9197e-01,  1.2917e-01,  8.8946e-01],
        [-6.2792e-01, -2.1620e-01,  1.2212e+00],
        [ 1.5624e+00,  2.2768e-02, -1.0653e+00],
        [-9.2350e-01, -2.4002e-01, -1.0687e-01],
        [-2.5679e-01, -3.5687e-01, -3.6080e-01],
        [ 6.0750e-01,  9.8555e-01,  4.2465e-01],
        [ 1.6859e+00, -3.7734e-01,  1.0126e+00],
        [-1.7234e-01,  1.3388e+00,  7.0506e-01],
        [ 4.0061e-01,  1.7096e+00,  1.4510e-01],
        [-1.0006e+00,  6.2727e-01,  1.2296e-01],
        [ 1.0727e-01,  4.5848e-01,  5.0674e-01],
        [ 6.9340e-01,  5.9646e-01,  7.5577e-01],
        [ 1.4333e-01,  5.0070e-01,  2.4739e-01],
        [ 1.0457e+00,  8.0073e-01, -1.0109e+00],
        [-2.0387e+00,  7.8761e-01,  6.6922e-01],
        [-3.5154e-01,  5.7595e-01,  1.6229e+00],
        [ 1.6858e-01, -1.2452e-01, -5.7238e-01],
        [-2.4576e-01,  3.0716e-02,  1.3269e-02]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-02-28 14:28:05 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 14:28:05 - INFO - Output shape after view : torch.Size([256, 3])
2025-02-28 14:28:05 - INFO - y batch shape after view : torch.Size([256, 3])
2025-02-28 14:28:17 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.242938, Accuracy = 37.89%, MSE = 0.8555
2025-02-28 14:30:41 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 2.015957, Accuracy = 47.76%, MSE = 0.6523
2025-02-28 14:32:31 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 0.793696, Accuracy = 46.54%, MSE = 0.6719
2025-02-28 14:34:18 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 1.661670, Accuracy = 52.34%, MSE = 0.5941
2025-02-28 14:36:31 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 0.680514, Accuracy = 58.74%, MSE = 0.5015
2025-02-28 14:38:52 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 0.697273, Accuracy = 61.51%, MSE = 0.4637
2025-02-28 14:41:17 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 0.667186, Accuracy = 64.34%, MSE = 0.4225
2025-02-28 14:43:25 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 0.688677, Accuracy = 66.54%, MSE = 0.3912
2025-02-28 14:44:00 - INFO - Epoch 2: Train Loss=1.6088, Train Acc=67.14%, Train MSE=0.3813
2025-02-28 14:44:17 - INFO - Epoch 2: Val Loss=0.6384, Val Acc=80.26%
2025-02-28 14:44:17 - INFO - Classification Report :               precision    recall  f1-score   support

          -1       0.00      0.00      0.00       439
           0       0.80      1.00      0.89      3826
           1       0.00      0.00      0.00       502

    accuracy                           0.80      4767
   macro avg       0.27      0.33      0.30      4767
weighted avg       0.64      0.80      0.71      4767

2025-02-28 14:44:27 - INFO - Epoch : 2 , Batch [ 0 / 75 ] : Loss = 0.590015, Accuracy = 82.42%, MSE = 0.1758
2025-02-28 14:46:27 - INFO - Epoch : 2 , Batch [ 10 / 75 ] : Loss = 0.722911, Accuracy = 79.90%, MSE = 0.2010
2025-02-28 14:48:40 - INFO - Epoch : 2 , Batch [ 20 / 75 ] : Loss = 0.630936, Accuracy = 79.45%, MSE = 0.2055
2025-02-28 14:50:37 - INFO - Epoch : 2 , Batch [ 30 / 75 ] : Loss = 0.741583, Accuracy = 79.28%, MSE = 0.2072
2025-02-28 14:52:59 - INFO - Epoch : 2 , Batch [ 40 / 75 ] : Loss = 0.727671, Accuracy = 79.20%, MSE = 0.2080
2025-02-28 14:55:56 - INFO - Epoch : 2 , Batch [ 50 / 75 ] : Loss = 0.669919, Accuracy = 79.20%, MSE = 0.2080
2025-02-28 14:58:55 - INFO - Epoch : 2 , Batch [ 60 / 75 ] : Loss = 0.561451, Accuracy = 79.27%, MSE = 0.2073
2025-02-28 15:01:21 - INFO - Epoch : 2 , Batch [ 70 / 75 ] : Loss = 0.726914, Accuracy = 79.43%, MSE = 0.2057
2025-02-28 15:02:19 - INFO - Epoch 3: Train Loss=0.6607, Train Acc=79.32%, Train MSE=0.2073
2025-02-28 15:02:38 - INFO - Epoch 3: Val Loss=0.6332, Val Acc=80.26%
2025-02-28 15:02:38 - INFO - Classification Report :               precision    recall  f1-score   support

          -1       0.00      0.00      0.00       439
           0       0.80      1.00      0.89      3826
           1       0.00      0.00      0.00       502

    accuracy                           0.80      4767
   macro avg       0.27      0.33      0.30      4767
weighted avg       0.64      0.80      0.71      4767

2025-02-28 15:02:47 - INFO - Epoch : 3 , Batch [ 0 / 75 ] : Loss = 0.702221, Accuracy = 77.34%, MSE = 0.2266
2025-02-28 15:04:26 - INFO - Epoch : 3 , Batch [ 10 / 75 ] : Loss = 0.645784, Accuracy = 80.08%, MSE = 0.1992
2025-02-28 15:06:35 - INFO - Epoch : 3 , Batch [ 20 / 75 ] : Loss = 0.622330, Accuracy = 79.82%, MSE = 0.2018
2025-02-28 15:09:03 - INFO - Epoch : 3 , Batch [ 30 / 75 ] : Loss = 0.732883, Accuracy = 79.67%, MSE = 0.2033
2025-02-28 15:11:20 - INFO - Epoch : 3 , Batch [ 40 / 75 ] : Loss = 0.701550, Accuracy = 79.62%, MSE = 0.2038
2025-02-28 15:13:40 - INFO - Epoch : 3 , Batch [ 50 / 75 ] : Loss = 0.738025, Accuracy = 79.50%, MSE = 0.2050
2025-02-28 15:16:53 - INFO - Epoch : 3 , Batch [ 60 / 75 ] : Loss = 0.591622, Accuracy = 79.40%, MSE = 0.2060
2025-02-28 15:26:29 - INFO - y batch shape : torch.Size([256, 3])
2025-02-28 15:26:29 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 15:26:29 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-02-28 15:26:29 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 15:26:29 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 15:26:40 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 15:26:40 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 15:26:40 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 15:27:56 - INFO - Logging started
2025-02-28 15:28:31 - INFO - Full sequence shape : (213, 81)
2025-02-28 15:28:31 - INFO - Decoder sequence length : 188
2025-02-28 15:28:31 - INFO - Slope value shape : (188,)
2025-02-28 15:28:31 - INFO - Slope values : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1
 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0]
2025-02-28 15:28:31 - INFO - Target bin max : 1
2025-02-28 15:28:31 - INFO - slope classes : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
2025-02-28 15:28:31 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-02-28 15:28:31 - INFO - Future price : 1
2025-02-28 15:28:31 - INFO - X shape : torch.Size([200, 51])
2025-02-28 15:28:31 - INFO - y shape : torch.Size([3])
2025-02-28 15:28:31 - INFO - y : tensor([0., 0., 1.])
2025-02-28 15:28:31 - INFO - y batch shape : torch.Size([256, 3])
2025-02-28 15:28:31 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 15:28:31 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-02-28 15:28:31 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 15:28:31 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 15:28:31 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 15:28:31 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 15:28:31 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 15:28:32 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-28 15:28:32 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-28 15:28:32 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 15:28:33 - INFO - Output s : tensor([[ 2.6432e-01,  8.3553e-02,  4.5354e-01],
        [ 1.3955e-01, -7.4913e-01,  5.0787e-01],
        [-2.2744e+00, -1.2164e-01,  6.2463e-01],
        [-1.0368e+00, -4.1439e-01,  3.5510e-01],
        [ 1.5436e-01,  4.3673e-01,  5.3703e-03],
        [-8.5535e-01, -1.0746e+00,  1.4134e+00],
        [-8.1201e-01,  7.1353e-01,  1.9970e+00],
        [-4.2706e-01, -1.1515e+00,  1.3687e+00],
        [-1.3982e+00,  5.5715e-01,  9.5010e-01],
        [-3.2441e-01, -7.6786e-01,  1.1292e+00],
        [-3.9358e-01, -7.4712e-01,  9.9970e-01],
        [-6.4625e-01, -1.2662e-01,  7.2737e-01],
        [-9.4039e-01,  2.6152e-02,  7.9556e-01],
        [-1.5136e+00, -4.0379e-01,  9.7098e-01],
        [ 1.2283e+00,  2.5446e-01,  1.4756e+00],
        [-7.3232e-01, -9.9507e-01,  7.4877e-01],
        [ 1.1067e-01, -4.3113e-01, -5.4707e-01],
        [-1.3287e+00, -1.4361e+00,  1.1395e+00],
        [-3.1591e-01, -9.2437e-01,  1.3657e+00],
        [-4.2926e-01,  7.2318e-01,  6.8197e-01],
        [-1.5727e+00, -1.0786e+00,  8.5796e-01],
        [-4.4676e-01, -1.0070e+00,  1.0851e+00],
        [-2.1254e+00,  2.3537e-01,  1.5165e+00],
        [-6.5505e-01,  2.3453e-01,  9.1348e-01],
        [-1.4639e+00, -1.4850e+00,  1.3230e-01],
        [-2.3806e+00, -6.4200e-02,  2.0860e-01],
        [ 2.5664e+00,  7.2992e-02,  6.7739e-01],
        [-9.4704e-01,  5.9866e-01,  8.6769e-02],
        [-1.0585e+00, -1.2052e+00,  1.5058e+00],
        [-8.2276e-01,  8.1241e-01, -2.9916e-01],
        [-8.8740e-01, -4.6766e-01,  1.1144e+00],
        [-1.0307e+00, -1.3012e-01,  2.1171e-01],
        [-6.4142e-01,  2.8336e-01,  2.5614e-01],
        [-1.8562e+00, -8.8707e-01,  1.0838e+00],
        [-1.6286e+00,  4.4062e-01,  1.3625e+00],
        [ 1.1149e+00,  6.2698e-01,  1.2468e+00],
        [ 1.6050e-01, -1.1395e+00,  2.0193e+00],
        [-1.5282e+00, -1.9687e-01,  1.8547e+00],
        [-5.6997e-01, -3.0515e-01,  8.2282e-01],
        [-5.4578e-01, -2.6920e-01,  8.5396e-01],
        [ 2.0979e+00,  4.1392e-01,  1.1411e+00],
        [-9.5805e-01, -3.6549e-01,  5.5579e-01],
        [-5.5590e-01,  1.5506e-01,  1.1528e+00],
        [-2.3682e+00,  2.4082e-01, -2.1576e-02],
        [-5.7645e-02,  8.9878e-02,  6.4162e-01],
        [ 7.9258e-01,  2.5923e-01,  2.1246e+00],
        [-2.0474e+00, -3.3576e-01,  1.0362e+00],
        [-6.6823e-01, -7.7695e-01,  2.4320e-01],
        [-6.7762e-02,  4.7218e-01,  1.3553e+00],
        [ 1.5852e+00, -3.9948e-01,  1.4653e+00],
        [-1.1281e+00, -6.5366e-01,  1.0682e+00],
        [-9.7332e-01, -4.9870e-01,  1.8053e+00],
        [-2.1943e+00, -2.9972e-01,  5.8965e-01],
        [-1.4885e+00, -8.2150e-01,  2.5471e+00],
        [-1.0077e+00,  3.9307e-01,  7.9592e-01],
        [-3.0007e-01, -3.8471e-01,  1.3692e+00],
        [-1.1580e+00, -6.4328e-01,  2.1641e-01],
        [-2.0049e+00, -6.1818e-01,  4.0839e-01],
        [-1.1144e+00,  2.1018e-01,  9.6288e-01],
        [-7.7664e-01,  2.4365e-01, -4.7265e-01],
        [-2.2262e+00, -4.0885e-01, -4.1681e-02],
        [-1.5014e+00,  4.3329e-01,  1.6935e+00],
        [-1.2608e+00,  1.7210e-01,  2.0175e+00],
        [-1.6524e+00,  5.0422e-01,  1.8047e-01],
        [-1.0020e+00, -6.9291e-01,  1.3798e+00],
        [-1.3858e+00,  3.0977e-01, -5.6620e-01],
        [-1.1613e+00, -5.7545e-01,  5.8204e-01],
        [-1.9146e+00,  5.8143e-01,  1.2593e-01],
        [-8.8178e-01, -5.2201e-01,  5.2544e-01],
        [-2.5072e-01, -5.4347e-01,  2.1830e-01],
        [-1.1770e+00,  2.5607e-01, -1.0841e-01],
        [-3.1344e-01, -3.8544e-01,  4.2355e-01],
        [-2.2222e+00,  1.4557e-01,  4.0285e-01],
        [-1.8959e+00,  9.9552e-01,  1.1969e+00],
        [-1.7866e+00, -4.6213e-01,  1.1416e+00],
        [ 9.0178e-01,  7.7940e-01,  5.1134e-01],
        [ 1.5717e+00, -1.7108e-02,  2.7076e+00],
        [ 2.1356e-01, -1.6176e+00,  1.3488e-01],
        [-8.5035e-01,  4.2370e-01,  4.0234e-01],
        [-3.3869e-01, -5.3183e-01,  1.6349e+00],
        [-1.8114e+00, -1.2376e-01,  1.3810e+00],
        [ 7.1265e-01, -2.9412e-01,  3.4054e+00],
        [-1.9172e+00, -2.1341e-01,  1.1416e+00],
        [-2.0887e+00,  4.8473e-01,  5.0035e-01],
        [-4.4240e-03,  7.4659e-01,  5.7206e-01],
        [-1.0124e-01, -4.3914e-01,  3.9833e-02],
        [-1.5025e+00,  7.6271e-01,  1.0752e+00],
        [-2.4725e-01,  6.9806e-01, -4.7629e-01],
        [ 2.0329e-01,  1.6179e-02,  2.2890e+00],
        [-8.8158e-01, -3.7331e-02,  8.7714e-01],
        [ 5.3067e-01, -7.9726e-01, -1.3324e-03],
        [-1.0742e+00,  1.3405e+00,  1.1566e+00],
        [-5.3525e-01,  6.6057e-01,  6.2061e-01],
        [-6.8439e-01,  6.3738e-01,  3.4581e-01],
        [-3.6349e-01, -1.4572e+00, -3.3256e-02],
        [ 9.7352e-01, -6.7990e-01,  2.2404e+00],
        [ 1.9877e-01, -9.5738e-01,  9.6943e-01],
        [-4.6136e-01,  9.5833e-01,  1.2093e+00],
        [-1.8824e+00,  6.8904e-02,  3.9471e-01],
        [-1.1970e+00,  8.3163e-02,  3.7304e-01],
        [-1.4981e+00, -1.3266e-01,  4.8560e-01],
        [-3.1584e-01, -9.4784e-01,  2.1343e+00],
        [-1.0166e+00, -4.0508e-01,  4.2283e-01],
        [-9.8154e-01,  2.2183e-01,  1.8896e-01],
        [-1.6355e+00, -8.2732e-01,  3.9542e-01],
        [-3.0922e+00, -3.6872e-01,  1.0640e+00],
        [-1.1290e+00,  9.4273e-02,  8.8956e-01],
        [-8.7915e-01, -1.0592e+00,  5.7946e-01],
        [-3.0186e+00,  7.0699e-01,  5.9878e-01],
        [-1.6948e+00, -6.1371e-01,  1.1865e+00],
        [-1.6660e+00, -1.3010e-01,  1.5047e+00],
        [-6.9684e-01,  2.4701e-01,  1.4274e+00],
        [-2.0944e+00, -8.6213e-01,  9.0441e-01],
        [-6.8714e-01,  2.3456e-01,  3.2091e-01],
        [-1.9757e+00,  1.8151e-01,  3.5340e-01],
        [-1.3435e+00, -5.2150e-01,  8.3217e-01],
        [ 2.1175e+00,  4.6169e-02,  1.2071e+00],
        [-1.6568e+00,  1.9224e-01,  3.7087e-01],
        [-1.4757e+00, -5.6357e-01, -5.2329e-03],
        [-1.2380e+00, -3.2159e-01,  7.0350e-01],
        [-1.1127e+00, -4.3952e-01,  6.9096e-01],
        [-6.3391e-01,  4.3946e-02,  1.5690e+00],
        [-2.4217e-01, -2.6310e-01, -1.6554e-01],
        [-1.6314e+00,  2.0432e-01,  5.9877e-01],
        [-1.5515e+00, -8.2268e-02,  1.2811e+00],
        [-9.2004e-01, -6.0121e-01,  1.2377e+00],
        [-1.7721e+00,  7.0134e-01,  1.5877e-01],
        [-1.7158e+00,  1.9507e-02,  1.0707e+00],
        [ 2.0793e-01,  6.0214e-02,  5.1801e-01],
        [-2.2192e+00, -1.0855e-01,  7.1343e-01],
        [-2.1169e+00, -1.5721e-01,  1.1341e+00],
        [-8.4811e-01, -1.3756e-01,  9.7450e-01],
        [ 1.0998e+00,  5.1009e-01,  1.6068e+00],
        [-1.8179e+00,  4.9911e-01,  1.8631e+00],
        [-1.2644e+00,  6.5594e-01,  1.0526e+00],
        [ 1.2298e-01,  4.3132e-01,  8.1714e-01],
        [-5.1442e-01, -2.4079e-02,  2.4961e-01],
        [-2.8358e-01,  2.8651e-01,  7.9615e-01],
        [-1.1627e+00,  8.4658e-01,  4.9039e-01],
        [-1.0551e+00,  3.7857e-01, -4.0488e-01],
        [-1.3722e+00, -4.7849e-01,  1.1180e+00],
        [-2.0232e+00,  4.1736e-01,  8.0160e-01],
        [-1.0578e+00, -3.4481e-01,  4.4657e-01],
        [-3.4057e-01,  1.7367e-01,  9.1157e-01],
        [-8.9058e-01, -3.8858e-01,  3.8661e-01],
        [-7.5718e-01,  9.7230e-01,  5.5929e-01],
        [ 9.9319e-02,  9.8835e-02,  1.3184e+00],
        [-6.1181e-01, -6.0064e-01,  1.5725e+00],
        [-1.1060e+00, -8.0260e-01,  9.3547e-01],
        [-1.2667e+00,  1.1409e+00,  1.3723e+00],
        [-1.6247e+00,  4.4441e-01,  1.7936e+00],
        [-2.1861e+00, -3.2167e-01,  1.5164e+00],
        [-1.5470e+00, -5.8471e-01,  6.0400e-01],
        [-2.4445e-01,  6.0722e-01,  1.6911e+00],
        [-8.6600e-01,  8.1915e-01,  1.2831e+00],
        [-1.2255e+00,  2.0090e-01,  2.8344e-01],
        [ 1.9436e-01,  8.1686e-01,  4.8182e-01],
        [-7.9658e-01,  7.1774e-01,  1.9040e-01],
        [-1.1828e+00,  1.9978e-01,  1.2907e+00],
        [-2.1719e-01, -8.7281e-01,  1.0492e+00],
        [-8.4517e-01,  7.1858e-01,  1.2345e+00],
        [-1.5752e+00, -4.6308e-01,  1.1184e+00],
        [-1.4762e+00,  3.4164e-01,  2.8649e-01],
        [-7.1930e-01, -6.0641e-01, -4.5377e-01],
        [-1.0917e+00,  3.5221e-01,  1.4224e+00],
        [ 3.4240e-01,  1.3087e+00,  1.1388e+00],
        [-2.0162e+00, -1.9912e+00,  1.6670e+00],
        [-4.9165e-01, -4.1915e-01, -3.0912e-01],
        [-1.3631e+00, -1.7460e+00,  7.3387e-01],
        [-5.1593e-01, -2.0194e-01, -1.5751e-03],
        [-1.3639e+00,  4.9539e-01,  2.2918e+00],
        [-1.6978e-01, -1.1518e+00,  8.4104e-01],
        [-1.6530e+00, -6.1514e-01,  1.1796e+00],
        [-1.8339e+00, -1.0573e+00,  1.7063e-01],
        [-9.6865e-01, -3.4426e-01,  1.1362e+00],
        [-1.4923e+00,  5.2916e-01,  5.8284e-01],
        [-8.6006e-02,  6.8413e-01, -9.7986e-03],
        [-6.8700e-01, -4.2314e-01,  1.4870e+00],
        [ 8.6200e-02, -7.7588e-01,  4.6242e-01],
        [ 4.2657e-01,  7.3838e-01,  4.5883e-01],
        [-1.8948e+00, -6.7127e-01,  7.0372e-02],
        [-3.7332e-01,  7.1727e-01,  1.2857e+00],
        [-4.7915e-01,  1.0494e+00,  1.5499e+00],
        [ 1.7772e+00,  1.3758e+00,  1.0255e+00],
        [-9.7901e-01, -1.3671e+00,  7.7599e-01],
        [-1.7054e+00, -9.1479e-02,  4.7995e-01],
        [-1.3482e+00,  1.2767e-01,  8.3442e-01],
        [-1.1126e+00,  1.4048e-01,  8.4773e-01],
        [-5.4145e-01,  1.1422e+00,  6.5675e-01],
        [-1.1268e+00,  4.1347e-01,  6.0607e-01],
        [-3.9793e-01,  3.3775e-01,  1.1372e-01],
        [-1.5868e+00, -3.0879e-01,  1.9731e+00],
        [ 2.3759e+00,  1.1987e+00,  1.3374e+00],
        [-1.4758e+00, -2.2736e-01,  4.4829e-01],
        [-7.6280e-01, -1.1063e+00,  1.1580e+00],
        [-8.8060e-01, -5.6273e-01,  2.0089e-01],
        [-1.1037e-01, -5.4982e-01,  2.4104e-01],
        [-2.5406e+00, -3.2484e-01,  1.2486e+00],
        [-5.1068e-01,  9.8378e-01, -6.6674e-01],
        [-9.4736e-01, -5.2194e-01,  6.8257e-01],
        [-1.7197e+00, -7.2549e-01, -4.4372e-01],
        [-6.0576e-01,  2.9170e-01,  9.2800e-01],
        [-1.1347e+00, -1.2824e+00,  7.8370e-01],
        [-2.5298e+00, -4.4731e-02,  4.2577e-01],
        [ 1.7170e+00, -1.8560e-01,  1.6750e+00],
        [-9.8525e-01, -1.4797e+00,  2.5403e-01],
        [-2.0387e+00, -1.4112e-01,  7.8977e-01],
        [-5.2131e-01, -8.7058e-01,  2.0667e+00],
        [-1.3441e+00, -7.4142e-01,  1.6410e+00],
        [-7.0889e-01, -2.0140e-01,  1.4189e+00],
        [-1.1520e+00,  3.3204e-01,  1.3812e+00],
        [-1.0322e+00,  5.4980e-01,  1.3216e+00],
        [-1.6577e+00, -9.3540e-01,  1.8043e+00],
        [-3.8168e-01, -2.7172e-01,  3.6961e-01],
        [-1.5989e-01, -2.6136e-01,  5.4704e-01],
        [-1.1175e+00, -3.1015e-01,  1.7617e+00],
        [-2.1921e+00, -5.3717e-01,  1.1630e+00],
        [-1.5671e+00,  1.8217e-02,  1.1910e+00],
        [ 7.0080e-01,  4.0743e-01,  2.1939e-01],
        [-3.7673e-01, -3.2709e-01,  1.0587e+00],
        [-5.3600e-01,  7.1102e-01,  1.3840e+00],
        [-2.1757e+00,  1.0934e-01,  1.8858e+00],
        [-4.7973e-01, -6.3886e-01,  1.4636e+00],
        [-5.7184e-01,  8.3058e-02,  7.9802e-01],
        [-9.0205e-01, -6.0543e-01,  8.1037e-01],
        [-1.2902e+00,  2.2733e-01,  1.7251e+00],
        [-5.3146e-01,  3.2154e-01,  1.7682e+00],
        [-7.9051e-01, -7.4276e-01,  2.0439e-01],
        [-6.7005e-01, -5.6216e-01,  1.6015e+00],
        [-1.4933e+00,  3.3286e-01,  1.2283e+00],
        [ 2.0525e+00,  4.3665e-01,  1.1386e+00],
        [-9.1869e-01, -9.1045e-01,  5.6408e-01],
        [-1.3215e+00, -3.8241e-01,  1.3628e+00],
        [-4.5568e-01,  3.7635e-01,  6.1302e-01],
        [ 1.1846e+00, -3.9084e-01,  8.0140e-01],
        [-1.1728e+00,  5.1882e-01,  2.1474e+00],
        [ 7.7131e-01, -7.8439e-01,  7.5845e-01],
        [-1.2285e+00,  2.3988e-01,  1.4540e+00],
        [-1.6773e+00, -6.4483e-01,  9.7419e-01],
        [-1.2928e+00, -4.5091e-01,  8.4719e-01],
        [-4.9589e-01,  1.5604e-01,  1.1113e+00],
        [ 4.0582e-01, -7.1288e-02,  6.9316e-01],
        [ 2.5694e-01, -1.4442e+00,  6.1516e-01],
        [ 7.3542e-01, -4.3064e-02,  6.8490e-01],
        [-6.3294e-01,  2.1573e-01,  6.1259e-01],
        [-8.3664e-01, -6.4942e-01,  7.2831e-01],
        [-1.5740e+00, -4.4488e-01, -4.0948e-02],
        [-4.0771e-01, -8.1211e-01,  2.2411e-01],
        [ 4.1251e-01,  1.1054e+00,  2.2268e+00],
        [-1.2751e+00, -8.4081e-01,  2.0843e+00],
        [-7.8643e-01, -6.4254e-01,  7.0609e-01],
        [-9.2442e-01, -2.1351e-01,  6.8320e-01],
        [-8.8771e-01, -7.8210e-01,  1.8096e-01],
        [-1.2909e+00, -1.4816e+00, -2.0811e-01],
        [-6.6547e-01, -4.3537e-02,  5.9628e-01],
        [ 2.7418e-01,  1.2514e+00,  2.6858e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-02-28 15:28:33 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 15:28:33 - INFO - Output shape after view : torch.Size([256, 3])
2025-02-28 15:28:33 - INFO - y batch shape after view : torch.Size([256, 3])
2025-02-28 15:28:36 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.534238, Accuracy = 17.97%, MSE = 0.9961
2025-02-28 15:29:23 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 1.685699, Accuracy = 63.74%, MSE = 0.4446
2025-02-28 15:30:04 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 0.665420, Accuracy = 66.56%, MSE = 0.4003
2025-02-28 15:30:47 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 0.582714, Accuracy = 69.37%, MSE = 0.3600
2025-02-28 15:31:30 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 0.684306, Accuracy = 71.60%, MSE = 0.3246
2025-02-28 15:32:13 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 0.711760, Accuracy = 72.86%, MSE = 0.3045
2025-02-28 15:32:56 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 0.620770, Accuracy = 73.71%, MSE = 0.2919
2025-02-28 15:33:38 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 0.631493, Accuracy = 74.49%, MSE = 0.2801
2025-02-28 15:33:55 - INFO - Epoch 2: Train Loss=0.8258, Train Acc=74.72%, Train MSE=0.2769
2025-02-28 15:34:12 - INFO - Epoch 2: Val Loss=0.8832, Val Acc=80.28%
2025-02-28 15:34:12 - INFO - Classification Report :               precision    recall  f1-score   support

          -1       0.00      0.00      0.00       402
           0       0.80      1.00      0.89      3833
           1       0.00      0.00      0.00       532

    accuracy                           0.80      4767
   macro avg       0.27      0.33      0.30      4767
weighted avg       0.65      0.80      0.72      4767

2025-02-28 15:34:32 - INFO - Epoch : 2 , Batch [ 0 / 75 ] : Loss = 0.662050, Accuracy = 79.69%, MSE = 0.2148
2025-02-28 15:36:47 - INFO - Epoch : 2 , Batch [ 10 / 75 ] : Loss = 0.745771, Accuracy = 78.55%, MSE = 0.2166
2025-02-28 15:38:56 - INFO - Epoch : 2 , Batch [ 20 / 75 ] : Loss = 0.793765, Accuracy = 78.40%, MSE = 0.2171
2025-02-28 15:41:30 - INFO - Epoch : 2 , Batch [ 30 / 75 ] : Loss = 0.644830, Accuracy = 78.79%, MSE = 0.2136
2025-02-28 15:43:40 - INFO - Epoch : 2 , Batch [ 40 / 75 ] : Loss = 0.729257, Accuracy = 78.93%, MSE = 0.2179
2025-02-28 15:45:43 - INFO - Epoch : 2 , Batch [ 50 / 75 ] : Loss = 0.584577, Accuracy = 78.77%, MSE = 0.2197
2025-02-28 15:47:33 - INFO - Epoch : 2 , Batch [ 60 / 75 ] : Loss = 0.645443, Accuracy = 78.95%, MSE = 0.2166
2025-02-28 15:49:17 - INFO - Epoch : 2 , Batch [ 70 / 75 ] : Loss = 0.594289, Accuracy = 79.04%, MSE = 0.2157
2025-02-28 15:49:47 - INFO - Epoch 3: Train Loss=0.6691, Train Acc=78.98%, Train MSE=0.2181
2025-02-28 15:50:03 - INFO - Epoch 3: Val Loss=0.9679, Val Acc=38.70%
2025-02-28 15:50:03 - INFO - Classification Report :               precision    recall  f1-score   support

          -1       0.04      0.24      0.07       402
           0       0.73      0.46      0.56      3833
           1       0.00      0.00      0.00       532

    accuracy                           0.39      4767
   macro avg       0.26      0.23      0.21      4767
weighted avg       0.59      0.39      0.46      4767

2025-02-28 15:50:16 - INFO - Epoch : 3 , Batch [ 0 / 75 ] : Loss = 0.649285, Accuracy = 76.56%, MSE = 0.3047
2025-02-28 15:51:44 - INFO - Epoch : 3 , Batch [ 10 / 75 ] : Loss = 0.567341, Accuracy = 79.23%, MSE = 0.2248
2025-02-28 15:53:17 - INFO - Epoch : 3 , Batch [ 20 / 75 ] : Loss = 0.562336, Accuracy = 78.94%, MSE = 0.2262
2025-02-28 15:54:43 - INFO - Epoch : 3 , Batch [ 30 / 75 ] : Loss = 0.645823, Accuracy = 78.63%, MSE = 0.2394
2025-02-28 15:56:44 - INFO - Epoch : 3 , Batch [ 40 / 75 ] : Loss = 0.768517, Accuracy = 78.85%, MSE = 0.2367
2025-02-28 15:58:56 - INFO - Epoch : 3 , Batch [ 50 / 75 ] : Loss = 0.570095, Accuracy = 79.12%, MSE = 0.2290
2025-02-28 16:00:35 - INFO - Epoch : 3 , Batch [ 60 / 75 ] : Loss = 0.606095, Accuracy = 79.07%, MSE = 0.2266
2025-02-28 16:23:23 - INFO - Full sequence shape : (201, 81)
2025-02-28 16:23:23 - INFO - Decoder sequence length : 188
2025-02-28 16:23:23 - INFO - Slope value shape : (188,)
2025-02-28 16:23:23 - INFO - Slope values : [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  1  1  1  1  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
2025-02-28 16:23:23 - INFO - Target bin max : 1
2025-02-28 16:23:23 - INFO - slope classes : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
2025-02-28 16:23:23 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-02-28 16:23:23 - INFO - Future price : 0
2025-02-28 16:23:23 - INFO - X shape : torch.Size([200, 51])
2025-02-28 16:23:23 - INFO - y shape : torch.Size([3])
2025-02-28 16:23:23 - INFO - y : tensor([0., 1., 0.])
2025-02-28 16:23:23 - INFO - y batch shape : torch.Size([256, 3])
2025-02-28 16:23:23 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-02-28 16:23:23 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-02-28 16:23:23 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-02-28 16:23:23 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-02-28 16:23:32 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-02-28 16:23:32 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-02-28 16:23:32 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-02-28 16:23:45 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-02-28 16:23:45 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-02-28 16:23:45 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 16:23:46 - INFO - Output s : tensor([[ 3.3116e+00, -1.0021e+00, -1.8683e+00],
        [ 3.6658e+00, -7.4882e-01, -8.9763e-01],
        [ 2.0092e+00, -1.7907e+00, -2.2493e+00],
        [ 9.5652e-01, -1.9304e+00, -1.1194e+00],
        [ 4.4603e+00, -1.1988e+00, -1.1333e+00],
        [ 2.6091e+00,  1.0140e+00, -2.3159e+00],
        [ 1.3554e+00, -1.0982e+00, -5.6032e-01],
        [ 2.9257e+00,  3.6702e-01, -1.3644e+00],
        [ 2.0483e+00, -1.0872e+00, -4.0495e+00],
        [ 1.2399e+00, -2.2967e-01, -3.1473e+00],
        [ 2.8774e+00, -9.0650e-01, -1.2251e+00],
        [ 2.8166e+00, -1.5808e+00, -7.1399e-01],
        [ 1.3981e+00, -1.7933e+00, -1.1425e+00],
        [ 2.6811e+00, -1.8702e+00, -2.5251e+00],
        [ 2.7444e+00, -8.3506e-01, -1.7841e+00],
        [ 2.8611e+00, -1.9869e-01, -2.0867e+00],
        [ 2.1827e+00, -9.2098e-01, -2.6750e+00],
        [ 8.0392e-01, -7.0109e-01, -1.7262e+00],
        [ 3.1219e+00, -3.1147e-01, -1.5451e+00],
        [ 2.7219e+00, -4.4746e-01, -1.5727e+00],
        [ 3.2706e+00,  1.4869e+00, -1.6300e+00],
        [ 1.8144e+00, -2.2992e-01, -1.5826e-01],
        [ 9.3254e-01, -2.4599e+00, -6.9963e-01],
        [ 2.7256e+00, -2.2393e-01, -1.7809e+00],
        [ 1.7071e+00, -7.4123e-01, -3.1313e+00],
        [ 2.9100e+00, -6.4134e-01, -2.1986e+00],
        [ 1.9193e+00, -9.3232e-01, -2.5489e-01],
        [ 2.3534e+00, -7.1752e-01, -1.6870e+00],
        [ 1.7857e+00, -1.6762e+00, -1.8305e+00],
        [ 3.3086e+00,  5.2385e-02, -2.1409e+00],
        [ 1.4931e+00, -2.1718e+00, -2.4157e+00],
        [ 3.5656e+00, -2.1315e-01, -1.2499e+00],
        [ 2.8035e+00, -1.7778e+00,  8.7937e-01],
        [ 1.6600e+00, -3.5338e-01, -2.1490e+00],
        [ 3.1338e+00, -6.9679e-01, -4.6096e-01],
        [ 1.5747e+00, -8.2935e-01, -4.4857e-01],
        [ 1.3571e+00, -1.9132e+00, -3.2787e+00],
        [ 5.6011e-01, -1.5741e+00, -1.7458e+00],
        [ 2.8579e+00, -1.6620e+00, -3.7271e+00],
        [ 1.8688e+00, -1.4273e+00, -1.6612e+00],
        [ 2.3428e+00, -1.1616e-01, -2.1496e+00],
        [ 2.6779e+00, -1.4453e+00, -1.5385e+00],
        [ 1.6170e+00, -2.0124e+00, -1.1373e+00],
        [ 2.7839e+00, -9.3334e-01, -5.6098e-01],
        [ 2.4206e+00, -3.9381e-01, -2.2679e+00],
        [ 3.3078e+00,  6.1611e-01, -8.1043e-01],
        [ 3.7788e+00, -4.9712e-01,  5.5327e-01],
        [ 1.2022e+00, -8.3554e-01, -2.2747e+00],
        [-7.1302e-01, -1.9893e+00, -1.3775e+00],
        [ 2.6528e+00, -1.6422e+00, -9.6595e-01],
        [ 4.6865e+00, -1.0942e+00, -1.5063e+00],
        [ 6.0354e-01, -2.1201e+00, -2.9231e+00],
        [ 1.7967e+00, -9.9765e-01, -1.9884e+00],
        [ 1.1271e+00, -1.5318e+00, -2.4645e+00],
        [ 3.0503e+00, -1.0513e-01, -2.4934e+00],
        [ 1.9150e+00, -1.7809e+00, -2.0368e+00],
        [ 2.8065e+00, -1.6695e+00, -2.5188e+00],
        [ 1.3693e+00, -1.7443e+00, -1.1697e+00],
        [ 1.0481e-01, -1.3823e+00, -1.9935e+00],
        [ 2.4130e+00,  5.0284e-01, -1.2959e+00],
        [ 2.3510e+00, -1.3391e+00, -2.5403e+00],
        [ 1.9131e+00, -1.4816e+00, -1.4935e+00],
        [ 2.9150e+00, -1.1027e+00, -6.6477e-01],
        [ 2.3941e+00, -2.6198e+00, -2.8284e+00],
        [ 2.4970e+00, -1.5636e+00, -2.0045e+00],
        [ 9.9423e-01,  1.2428e-02, -2.7260e+00],
        [ 1.4145e+00, -1.6703e+00, -7.4425e-01],
        [ 2.7257e+00, -9.6508e-01, -2.2627e+00],
        [ 3.3235e+00, -6.8114e-01, -2.1713e+00],
        [ 3.2051e+00, -1.5806e+00, -9.5848e-01],
        [ 2.0646e+00, -1.0494e+00, -1.8788e+00],
        [-3.0711e-01, -1.8471e+00, -1.7397e+00],
        [-1.6214e-01,  1.8518e+00, -1.7567e+00],
        [ 4.0852e+00, -9.1806e-01, -1.0590e+00],
        [ 9.5923e-01,  5.2159e-01, -2.9201e+00],
        [ 3.3323e+00, -5.8060e-01, -1.5659e+00],
        [ 1.0069e+00, -7.4933e-01, -2.4682e+00],
        [ 4.3879e+00, -6.0989e-01, -2.0162e+00],
        [ 5.4786e-01,  5.3407e-01, -2.1115e+00],
        [ 3.4189e+00, -1.1442e+00,  5.2739e-01],
        [ 2.0089e+00, -1.0828e+00, -1.7828e+00],
        [ 9.7031e-01, -4.2695e-01, -3.3787e+00],
        [ 1.2965e+00, -1.6476e+00, -2.2445e+00],
        [ 2.6679e+00, -1.1222e+00, -1.8233e+00],
        [ 1.1793e+00, -2.7267e+00, -3.5722e-01],
        [ 2.9919e+00,  3.7775e-01, -1.1575e+00],
        [ 2.0293e+00, -6.8378e-01, -1.7793e+00],
        [ 9.5079e-01,  2.2807e-01, -1.3471e+00],
        [ 2.0246e+00, -3.4906e-01,  4.1480e-01],
        [ 2.2312e+00, -1.0206e+00, -1.5239e+00],
        [ 2.5349e+00, -1.4469e+00, -1.3604e+00],
        [ 1.6232e+00, -4.5437e-01, -1.4869e+00],
        [ 1.8750e+00, -5.2986e-01, -1.0750e+00],
        [ 1.4978e+00, -1.1999e+00, -7.2798e-01],
        [ 2.2879e+00, -8.3012e-01, -8.4146e-01],
        [ 1.9296e+00, -1.2578e+00, -1.9644e+00],
        [ 1.9079e+00,  8.2799e-01, -1.4193e+00],
        [ 2.0575e+00, -2.4882e+00, -1.4079e+00],
        [ 2.2045e+00, -3.5164e-01, -1.5569e+00],
        [ 2.6979e+00, -6.0109e-01, -1.3406e+00],
        [ 1.0046e+00, -1.3788e+00, -2.7291e+00],
        [ 1.1422e+00, -2.5971e+00, -4.2529e-01],
        [ 1.2741e+00, -1.9213e+00, -1.2867e+00],
        [ 2.7916e+00, -3.9682e-01, -1.6149e+00],
        [ 1.5937e+00,  4.9021e-02, -3.5382e-01],
        [ 2.1845e+00, -5.3631e-01, -3.2275e+00],
        [ 1.3668e+00, -1.3204e+00, -2.7957e+00],
        [ 2.2895e+00, -6.1115e-01, -1.2534e+00],
        [ 1.0858e+00, -1.1799e+00,  2.5762e-01],
        [ 1.6798e+00, -1.2430e-01, -1.2609e+00],
        [-2.0827e-01, -4.6623e-01, -1.7490e+00],
        [ 1.6851e+00, -9.0566e-01, -2.2601e+00],
        [-3.1151e-01, -7.8278e-01, -3.0363e+00],
        [ 3.2231e+00,  1.3670e+00, -3.4024e+00],
        [ 1.7798e+00, -1.6609e+00, -1.5156e+00],
        [ 2.7055e+00,  9.3308e-01, -2.2916e+00],
        [ 3.4507e+00, -1.7929e+00, -3.0089e+00],
        [ 2.7002e+00, -8.0498e-01, -2.9798e+00],
        [ 2.7090e+00,  4.2226e-01, -1.3627e+00],
        [ 2.0176e+00, -1.5583e+00, -2.1306e+00],
        [ 2.5744e+00, -1.2101e+00, -6.8365e-01],
        [ 3.6062e+00, -1.0974e+00, -5.8172e-01],
        [ 7.2444e-01, -1.8834e+00, -2.1140e+00],
        [ 2.0129e+00, -1.5990e+00, -2.1993e+00],
        [ 1.7523e+00, -1.3334e+00, -1.2254e+00],
        [ 1.3465e+00, -2.3437e+00, -1.5521e+00],
        [ 1.6214e+00, -1.1037e+00, -1.7245e+00],
        [ 1.9980e+00, -3.8127e-02, -2.1306e+00],
        [ 1.9853e+00, -2.7110e+00, -1.9048e+00],
        [ 2.0569e+00, -1.0582e+00, -1.9354e+00],
        [ 2.2376e-01,  9.5202e-01, -2.2596e+00],
        [ 9.5124e-01,  1.0845e+00, -2.9139e+00],
        [ 4.0622e+00, -1.3598e+00, -1.7461e+00],
        [ 4.1552e+00, -1.1654e-01, -1.9961e+00],
        [ 2.7327e+00, -8.5000e-01, -5.1327e-01],
        [ 1.1375e+00, -2.6437e+00, -2.6236e-01],
        [ 2.7019e+00,  3.5793e-01, -2.9165e+00],
        [ 2.5881e+00, -1.4852e+00,  6.7581e-01],
        [ 3.5543e+00, -3.1353e-01, -1.8025e+00],
        [ 2.9563e+00, -7.4529e-02, -1.5261e+00],
        [ 2.0915e+00, -2.6229e-01, -2.1534e+00],
        [ 3.1388e+00, -1.2675e+00, -1.1597e+00],
        [ 3.1074e+00, -6.5278e-01, -4.0865e-01],
        [ 2.4158e+00, -1.9089e-01, -1.3657e+00],
        [ 5.7922e-01, -1.6848e+00, -3.3561e+00],
        [-3.2049e-01, -1.6612e+00, -2.0524e+00],
        [ 3.0577e+00, -5.0014e-01, -2.6648e+00],
        [ 1.3362e+00, -6.8200e-01, -2.2878e+00],
        [ 2.7890e+00, -1.3104e+00, -3.0033e+00],
        [ 3.3620e+00, -3.0765e-01, -2.6823e+00],
        [ 2.3193e+00, -2.0884e+00, -3.4550e+00],
        [ 3.4567e+00, -1.3434e+00, -1.2093e+00],
        [ 1.7219e+00, -1.7539e+00, -4.4599e-01],
        [ 1.1454e+00, -1.2164e+00, -2.8139e+00],
        [ 1.3567e+00, -1.2393e+00, -2.6268e+00],
        [ 1.2453e+00, -1.2828e+00, -1.8536e+00],
        [ 2.9735e+00,  3.8648e-01, -1.7543e+00],
        [ 8.7548e-01,  5.8376e-01, -7.6848e-01],
        [ 3.3682e-02, -5.2026e-01, -7.9093e-01],
        [ 2.4686e+00, -1.3086e+00, -1.5099e+00],
        [ 3.5545e-01, -1.9339e+00, -2.5779e+00],
        [ 2.4179e+00, -6.6333e-01, -1.1807e+00],
        [ 2.9209e+00, -1.1228e+00, -1.6631e+00],
        [ 9.1688e-01, -7.5374e-01, -1.6446e+00],
        [ 1.9342e+00, -1.4426e+00, -3.1094e+00],
        [ 4.4640e+00, -1.0938e+00, -8.9973e-01],
        [ 2.6306e+00,  1.1510e+00, -1.2964e+00],
        [ 1.8932e+00, -1.2469e+00, -1.6355e+00],
        [ 1.5368e+00, -1.0628e+00, -2.3078e+00],
        [ 2.9880e+00, -1.9635e+00, -3.7639e+00],
        [ 1.4693e+00, -2.2459e+00, -1.6910e+00],
        [ 2.4802e+00, -1.9123e+00, -1.2101e+00],
        [ 4.1059e+00,  6.2628e-01, -1.3539e+00],
        [ 1.7861e+00,  1.3134e-01, -1.7094e+00],
        [ 8.8395e-01,  2.7526e-01, -2.1882e+00],
        [ 1.0925e+00, -2.1669e+00, -1.7505e+00],
        [ 1.1841e+00, -1.0210e+00, -1.6174e+00],
        [ 1.5645e+00,  8.9103e-01, -3.2087e+00],
        [ 1.2233e+00, -1.1374e+00, -1.4056e+00],
        [ 9.4450e-01, -6.7528e-01, -2.3515e+00],
        [ 2.7864e+00, -1.2597e+00, -8.7889e-01],
        [ 1.0094e+00, -1.6744e+00, -2.9541e+00],
        [ 1.1591e+00, -6.7170e-01, -3.2524e+00],
        [ 6.6951e-01, -3.8994e-01, -2.5823e+00],
        [ 3.8330e+00, -1.0698e+00, -1.2142e+00],
        [ 2.8955e+00, -1.8780e+00, -9.6552e-01],
        [ 2.2106e+00, -8.9766e-01,  5.4857e-01],
        [ 1.5820e-02, -1.3532e+00, -3.1970e+00],
        [ 2.5364e+00, -1.5058e+00, -2.5548e+00],
        [ 1.7568e+00, -2.7631e-01, -1.3584e+00],
        [ 2.4486e+00, -1.3976e+00, -1.7192e+00],
        [ 1.5459e+00, -5.3808e-01, -2.1035e+00],
        [ 3.6608e+00,  4.7001e-01, -1.5339e+00],
        [ 1.8453e+00, -2.7912e-01, -1.3892e+00],
        [ 1.9196e+00,  4.2264e-02, -6.5714e-01],
        [ 2.2107e+00, -1.6973e+00, -5.4449e-01],
        [ 1.8436e+00, -1.6446e+00, -2.3668e+00],
        [ 1.8237e+00, -2.5488e-01, -2.1808e+00],
        [ 3.8814e+00, -1.4297e+00, -1.7927e-01],
        [ 1.5674e+00, -1.0285e+00, -6.9552e-01],
        [ 2.5726e+00, -1.6594e+00, -8.0215e-01],
        [ 3.7516e-01, -8.9454e-01, -3.4293e+00],
        [ 4.4087e+00,  4.1292e-03, -8.9498e-01],
        [ 1.6300e+00, -2.4462e-01, -8.6401e-01],
        [ 2.8563e+00, -6.3655e-01, -2.8839e+00],
        [ 3.0758e-01,  6.8238e-01, -2.3365e+00],
        [ 2.4652e+00,  8.6931e-01, -1.6993e+00],
        [ 3.4558e+00, -2.2813e+00, -1.7623e+00],
        [ 1.3917e+00,  4.8879e-02, -2.9845e+00],
        [ 1.5965e+00, -1.0659e-01, -9.6517e-01],
        [ 1.2507e+00,  4.3870e-01, -2.4566e+00],
        [ 9.1044e-01, -1.9278e+00, -1.5962e+00],
        [ 2.7816e+00, -2.1404e+00, -5.1296e-01],
        [ 2.1412e+00, -1.6816e+00, -2.2816e+00],
        [ 3.1283e+00, -5.1441e-01, -1.8128e+00],
        [ 6.7631e-01, -2.0366e+00, -5.1943e-01],
        [ 3.0075e+00, -5.9771e-01, -1.9982e+00],
        [ 1.6940e+00, -1.2808e+00, -1.7910e+00],
        [ 1.9931e+00, -1.4594e+00, -1.9036e+00],
        [ 2.3205e+00, -8.5920e-01, -1.9273e+00],
        [ 1.6447e+00, -1.5141e+00,  1.7922e-01],
        [ 2.4680e+00, -1.4269e+00, -2.3382e+00],
        [ 1.5480e+00, -8.2729e-02, -1.2479e+00],
        [ 2.2729e+00,  5.5515e-01, -1.1375e+00],
        [ 2.1997e+00,  2.6530e-01, -1.4045e+00],
        [ 1.1867e+00, -1.9996e+00, -2.8299e+00],
        [ 1.0541e+00, -2.2095e+00, -1.7397e+00],
        [ 4.1259e+00, -1.2131e+00, -8.4810e-01],
        [ 1.3563e+00, -1.5826e+00, -1.5847e+00],
        [ 2.1634e+00, -1.3916e+00, -2.9534e+00],
        [ 3.8200e+00,  2.7173e-01, -1.9657e+00],
        [ 3.2064e+00, -1.7123e+00, -1.4332e+00],
        [ 3.3442e+00, -8.3043e-02, -1.0176e+00],
        [ 1.5717e+00, -6.4199e-01, -1.2800e+00],
        [ 2.4576e+00, -8.7274e-01, -1.6771e+00],
        [ 4.6303e-01, -2.4767e+00, -2.5032e+00],
        [ 1.4855e+00, -1.1387e+00, -9.4869e-01],
        [ 3.7521e+00,  4.9050e-01, -1.9383e-01],
        [ 1.0364e+00,  4.5548e-02, -2.0286e+00],
        [ 4.2479e+00, -5.5794e-01, -2.1440e+00],
        [ 2.1241e+00, -9.0122e-01, -1.7673e+00],
        [ 1.7169e+00, -1.4276e+00, -3.6507e+00],
        [ 2.7413e+00, -1.0663e+00, -1.3223e+00],
        [ 1.5251e+00, -2.2343e+00, -1.5480e-01],
        [ 1.8102e+00, -5.0695e-01, -2.5715e+00],
        [-5.1913e-01,  6.8267e-01, -1.6397e+00],
        [ 3.0566e+00, -1.7461e+00, -2.4272e+00],
        [ 1.5629e+00, -2.0960e+00, -2.0920e+00],
        [ 1.1597e+00, -1.6757e+00, -1.3594e+00],
        [ 2.0948e+00, -9.6693e-01, -6.2005e-01],
        [ 3.1003e+00, -1.0573e+00, -4.0759e-01],
        [-8.1795e-02,  1.2257e-01, -1.6443e+00],
        [ 1.7106e+00, -1.5017e+00, -5.2216e-01],
        [ 1.8228e+00, -1.9289e+00, -2.2330e+00],
        [ 8.3756e-01, -3.4609e-01, -2.9137e+00],
        [ 2.6284e+00, -8.2738e-01, -5.7307e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-02-28 16:23:46 - INFO - Output shape : torch.Size([256, 3])
2025-02-28 16:23:46 - INFO - Output shape after view : torch.Size([256, 3])
2025-02-28 16:23:46 - INFO - y batch shape after view : torch.Size([256, 3])
2025-02-28 16:24:09 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 8.668739, Accuracy = 19.14%, MSE = 1.8633
2025-02-28 16:36:22 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 6.467408, Accuracy = 33.74%, MSE = 1.2763
2025-02-28 16:49:21 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 3.358051, Accuracy = 34.73%, MSE = 1.2833
2025-02-28 17:23:04 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 3.029591, Accuracy = 35.67%, MSE = 1.2613
2025-02-28 17:44:01 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 3.392240, Accuracy = 36.19%, MSE = 1.2491
2025-02-28 18:36:44 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 3.230472, Accuracy = 37.16%, MSE = 1.2165
2025-02-28 20:31:35 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 3.059792, Accuracy = 37.69%, MSE = 1.2066
2025-02-28 20:42:47 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 3.063882, Accuracy = 38.06%, MSE = 1.1958
2025-02-28 20:46:47 - INFO - Epoch 2: Train Loss=3.9659, Train Acc=38.27%, Train MSE=1.1908
2025-02-28 20:47:04 - INFO - Epoch 2: Val Loss=6.1356, Val Acc=0.00%
2025-03-01 01:29:51 - INFO - Val Loss=6.1356, Val Acc=0.00%
2025-03-01 01:29:51 - INFO - Classification Report :               precision    recall  f1-score   support

          -1       0.00      0.00      0.00       0.0
           0       0.00      0.00      0.00    4767.0
           1       0.00      0.00      0.00       0.0

   micro avg       0.00      0.00      0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 01:32:49 - INFO - Val Loss=6.1356, Val Acc=0.00%
2025-03-01 01:32:49 - INFO - Classification Report :               precision    recall  f1-score   support

           0       0.00      0.00      0.00    4767.0
           1       0.00      0.00      0.00       0.0
           2       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 01:40:18 - INFO - Outputs : tensor([[0.6192, 0.5198, 1.0408],
        [0.6751, 0.4918, 1.1046],
        [0.7381, 0.4094, 1.2027],
        [0.5212, 0.5085, 0.9172],
        [0.4967, 0.5187, 0.8983],
        [0.5753, 0.5215, 1.0070],
        [0.6270, 0.4784, 1.0915],
        [0.6724, 0.4634, 1.1001],
        [0.5427, 0.5176, 0.9651],
        [0.6950, 0.4675, 1.1123],
        [0.6396, 0.5011, 1.0705],
        [0.6890, 0.4632, 1.1058],
        [0.5647, 0.4769, 0.9582],
        [0.7429, 0.4007, 1.2111],
        [0.6143, 0.5192, 1.0312],
        [0.5337, 0.5300, 0.9333],
        [0.6023, 0.5118, 1.0365],
        [0.6851, 0.4547, 1.1404],
        [0.5903, 0.4726, 0.9832],
        [0.5119, 0.5159, 0.9348],
        [0.5353, 0.5030, 0.9338],
        [0.5558, 0.4813, 0.9449],
        [0.5255, 0.5114, 0.9401],
        [0.4986, 0.5065, 0.9006],
        [0.5759, 0.4883, 0.9952],
        [0.6859, 0.4582, 1.1249],
        [0.6404, 0.5045, 1.0981],
        [0.6300, 0.4891, 1.0387],
        [0.4910, 0.5124, 0.8970],
        [0.5003, 0.5180, 0.8985],
        [0.5420, 0.5412, 0.9533],
        [0.7746, 0.3244, 1.2614],
        [0.6307, 0.4903, 1.0387],
        [0.6398, 0.5044, 1.0975],
        [0.6557, 0.4797, 1.0875],
        [0.4987, 0.5066, 0.9008],
        [0.6724, 0.4535, 1.1238],
        [0.7172, 0.3994, 1.1922],
        [0.5216, 0.5006, 0.9551],
        [0.5110, 0.5091, 0.9189],
        [0.6491, 0.4921, 1.0998],
        [0.5753, 0.5215, 1.0070],
        [0.5496, 0.5245, 0.9718],
        [0.6104, 0.5111, 1.0455],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6593, 0.4781, 1.0921],
        [0.5358, 0.5059, 0.9385],
        [0.5130, 0.5035, 0.9084],
        [0.4958, 0.5174, 0.8965],
        [0.8340, 0.3174, 1.2795],
        [0.6214, 0.4690, 1.0405],
        [0.6777, 0.4950, 1.1162],
        [0.5009, 0.5132, 0.9015],
        [0.7748, 0.3475, 1.2341],
        [0.6857, 0.4582, 1.1246],
        [0.8143, 0.3915, 1.2524],
        [0.6562, 0.4790, 1.0883],
        [0.6733, 0.4627, 1.1020],
        [0.5060, 0.5029, 0.9057],
        [0.7058, 0.4417, 1.1507],
        [0.6746, 0.4926, 1.1040],
        [0.5650, 0.4688, 0.9542],
        [0.6492, 0.4729, 1.0799],
        [0.6061, 0.5095, 1.0402],
        [0.5387, 0.5050, 0.9366],
        [0.7912, 0.3203, 1.2739],
        [0.5610, 0.5125, 0.9724],
        [0.6366, 0.4742, 1.0491],
        [0.5272, 0.5110, 0.9345],
        [0.7027, 0.4601, 1.1563],
        [0.7352, 0.4137, 1.1986],
        [0.6099, 0.5182, 1.0356],
        [0.7734, 0.3496, 1.2331],
        [0.4999, 0.5147, 0.9072],
        [0.7734, 0.3496, 1.2331],
        [0.5665, 0.4872, 0.9744],
        [0.6862, 0.4537, 1.1409],
        [0.7745, 0.3467, 1.2341],
        [0.7057, 0.4405, 1.1224],
        [0.6587, 0.4783, 1.0915],
        [0.7172, 0.3994, 1.1922],
        [0.6474, 0.5392, 1.0558],
        [0.5750, 0.4993, 1.0236],
        [0.4988, 0.5183, 0.8986],
        [0.6345, 0.4999, 1.0777],
        [0.5626, 0.4683, 0.9531],
        [0.6476, 0.4697, 1.0592],
        [0.5222, 0.5105, 0.9185],
        [0.7542, 0.3864, 1.2347],
        [0.5315, 0.5033, 0.9342],
        [0.6277, 0.4921, 1.0419],
        [0.8314, 0.3229, 1.2769],
        [0.6677, 0.4244, 1.1465],
        [0.5118, 0.5159, 0.9346],
        [0.5687, 0.5231, 0.9644],
        [0.5221, 0.5073, 0.9164],
        [0.6865, 0.4664, 1.1048],
        [0.4906, 0.5129, 0.8975],
        [0.5319, 0.5263, 0.9330],
        [0.6870, 0.4654, 1.1049],
        [0.5322, 0.5274, 0.9338],
        [0.5212, 0.5085, 0.9172],
        [0.7783, 0.3576, 1.2273],
        [0.7196, 0.3959, 1.1841],
        [0.5444, 0.5186, 0.9663],
        [0.6557, 0.4797, 1.0875],
        [0.5895, 0.4725, 0.9825],
        [0.5620, 0.4988, 0.9988],
        [0.4996, 0.5183, 0.8986],
        [0.5496, 0.5245, 0.9718],
        [0.6478, 0.4835, 1.0715],
        [0.6125, 0.5212, 1.0318],
        [0.5909, 0.4728, 0.9834],
        [0.6024, 0.5062, 1.0354],
        [0.8143, 0.3915, 1.2524],
        [0.6496, 0.4924, 1.1000],
        [0.5665, 0.4872, 0.9744],
        [0.5380, 0.5064, 0.9387],
        [0.5372, 0.5036, 0.9349],
        [0.5586, 0.5207, 0.9788],
        [0.6174, 0.5134, 1.0458],
        [0.5769, 0.5190, 1.0033],
        [0.5310, 0.5030, 0.9333],
        [0.7548, 0.3867, 1.2347],
        [0.7708, 0.3262, 1.2567],
        [0.6751, 0.4918, 1.1046],
        [0.5343, 0.5142, 0.9454],
        [0.6170, 0.5178, 1.0354],
        [0.5750, 0.4993, 1.0236],
        [0.6005, 0.5081, 1.0327],
        [0.5586, 0.5091, 0.9952],
        [0.6564, 0.5133, 1.0577],
        [0.5749, 0.5198, 1.0185],
        [0.7757, 0.3607, 1.2232],
        [0.6497, 0.4802, 1.1098],
        [0.7014, 0.3785, 1.1984],
        [0.6195, 0.5338, 1.0443],
        [0.5754, 0.4879, 0.9945],
        [0.5721, 0.5000, 0.9666],
        [0.6859, 0.4582, 1.1249],
        [0.5301, 0.5108, 0.9440],
        [0.5428, 0.5025, 0.9557],
        [0.7206, 0.4028, 1.1934],
        [0.6776, 0.4984, 1.1252],
        [0.5665, 0.4872, 0.9744],
        [0.7543, 0.3861, 1.2353],
        [0.5315, 0.5033, 0.9342],
        [0.4825, 0.5128, 0.8875],
        [0.5266, 0.5112, 0.9405],
        [0.6851, 0.4547, 1.1404],
        [0.5398, 0.5092, 0.9342],
        [0.5368, 0.5152, 0.9856],
        [0.6188, 0.5169, 1.0376],
        [0.7085, 0.4403, 1.1551],
        [0.5647, 0.4769, 0.9582],
        [0.5972, 0.4953, 1.0292],
        [0.7464, 0.3961, 1.2147],
        [0.6864, 0.4669, 1.1037],
        [0.5650, 0.5185, 1.0117],
        [0.5322, 0.5274, 0.9338],
        [0.7477, 0.3946, 1.2154],
        [0.6735, 0.4516, 1.1262],
        [0.7543, 0.3861, 1.2353],
        [0.5946, 0.4948, 1.0251],
        [0.6792, 0.4950, 1.1175],
        [0.6726, 0.4529, 1.1246],
        [0.5089, 0.5158, 0.9333],
        [0.7562, 0.3540, 1.2381],
        [0.5785, 0.5177, 0.9727],
        [0.6748, 0.4621, 1.1048],
        [0.5995, 0.5083, 1.0321],
        [0.5620, 0.4988, 0.9988],
        [0.5954, 0.4951, 1.0266],
        [0.5349, 0.5016, 0.9331],
        [0.5212, 0.5085, 0.9172],
        [0.6384, 0.4933, 1.0797],
        [0.5387, 0.5050, 0.9366],
        [0.5838, 0.5038, 0.9884],
        [0.6396, 0.5013, 1.0701],
        [0.5293, 0.5087, 0.9312],
        [0.5341, 0.5153, 0.9472],
        [0.8131, 0.3904, 1.2520],
        [0.6859, 0.4582, 1.1249],
        [0.7058, 0.4417, 1.1507],
        [0.7738, 0.3490, 1.2336],
        [0.5373, 0.5384, 0.9498],
        [0.7757, 0.3607, 1.2232],
        [0.6690, 0.4932, 1.1100],
        [0.6491, 0.4921, 1.0998],
        [0.5216, 0.5005, 0.9552],
        [0.5361, 0.5152, 0.9848],
        [0.5838, 0.5038, 0.9884],
        [0.5117, 0.5101, 0.9174],
        [0.5647, 0.4769, 0.9582],
        [0.5903, 0.4726, 0.9832],
        [0.6473, 0.4647, 1.1180],
        [0.6306, 0.4765, 1.0431],
        [0.6408, 0.4749, 1.1030],
        [0.6399, 0.5331, 1.0489],
        [0.6874, 0.4638, 1.1052],
        [0.5387, 0.5050, 0.9366],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6751, 0.5081, 1.1314],
        [0.5383, 0.5049, 0.9577],
        [0.5707, 0.5226, 0.9970],
        [0.6303, 0.5025, 1.0602],
        [0.7545, 0.3860, 1.2353],
        [0.6950, 0.4675, 1.1123],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.6497, 0.4799, 1.0707],
        [0.8081, 0.3574, 1.2713],
        [0.8453, 0.2825, 1.3216],
        [0.5225, 0.5116, 0.9201],
        [0.7523, 0.3903, 1.2298],
        [0.6422, 0.5049, 1.1026],
        [0.5785, 0.5177, 0.9727],
        [0.6022, 0.5069, 1.0348],
        [0.6229, 0.4830, 1.0827],
        [0.6848, 0.4550, 1.1403],
        [0.6865, 0.4664, 1.1048],
        [0.5678, 0.5215, 1.0155],
        [0.6000, 0.4962, 1.0335],
        [0.5657, 0.4865, 0.9743],
        [0.5060, 0.5029, 0.9057],
        [0.5354, 0.5149, 0.9450],
        [0.6659, 0.4234, 1.1465],
        [0.5119, 0.5159, 0.9348],
        [0.5721, 0.4994, 0.9800],
        [0.5097, 0.5154, 0.9333],
        [0.6593, 0.4781, 1.0921],
        [0.7014, 0.3785, 1.1984],
        [0.7206, 0.4028, 1.1934],
        [0.4825, 0.5121, 0.8868],
        [0.5146, 0.5101, 0.9139],
        [0.6402, 0.5323, 1.0487],
        [0.4996, 0.5183, 0.8986],
        [0.6384, 0.4933, 1.0797],
        [0.7780, 0.3589, 1.2259],
        [0.6557, 0.4797, 1.0875],
        [0.5288, 0.5086, 0.9309],
        [0.5766, 0.5006, 1.0299],
        [0.5610, 0.4681, 0.9523],
        [0.6790, 0.4452, 1.1540],
        [0.6270, 0.4784, 1.0915],
        [0.6402, 0.5323, 1.0487],
        [0.6522, 0.4819, 1.1071],
        [0.5275, 0.5112, 0.9335],
        [0.8334, 0.3202, 1.2785],
        [0.7689, 0.3591, 1.2363],
        [0.5895, 0.4725, 0.9825],
        [0.6184, 0.5125, 1.0475],
        [0.8314, 0.3229, 1.2769],
        [0.6476, 0.4697, 1.0592]], device='mps:0')
2025-03-01 01:40:18 - INFO - Outputs shape : torch.Size([256, 3])
2025-03-01 01:40:18 - INFO - Y batch : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:18 - INFO - Y batch shape : torch.Size([256, 3])
2025-03-01 01:40:18 - INFO - Outputs argmax : tensor([[0.6192, 0.5198, 1.0408],
        [0.6751, 0.4918, 1.1046],
        [0.7381, 0.4094, 1.2027],
        [0.5212, 0.5085, 0.9172],
        [0.4967, 0.5187, 0.8983],
        [0.5753, 0.5215, 1.0070],
        [0.6270, 0.4784, 1.0915],
        [0.6724, 0.4634, 1.1001],
        [0.5427, 0.5176, 0.9651],
        [0.6950, 0.4675, 1.1123],
        [0.6396, 0.5011, 1.0705],
        [0.6890, 0.4632, 1.1058],
        [0.5647, 0.4769, 0.9582],
        [0.7429, 0.4007, 1.2111],
        [0.6143, 0.5192, 1.0312],
        [0.5337, 0.5300, 0.9333],
        [0.6023, 0.5118, 1.0365],
        [0.6851, 0.4547, 1.1404],
        [0.5903, 0.4726, 0.9832],
        [0.5119, 0.5159, 0.9348],
        [0.5353, 0.5030, 0.9338],
        [0.5558, 0.4813, 0.9449],
        [0.5255, 0.5114, 0.9401],
        [0.4986, 0.5065, 0.9006],
        [0.5759, 0.4883, 0.9952],
        [0.6859, 0.4582, 1.1249],
        [0.6404, 0.5045, 1.0981],
        [0.6300, 0.4891, 1.0387],
        [0.4910, 0.5124, 0.8970],
        [0.5003, 0.5180, 0.8985],
        [0.5420, 0.5412, 0.9533],
        [0.7746, 0.3244, 1.2614],
        [0.6307, 0.4903, 1.0387],
        [0.6398, 0.5044, 1.0975],
        [0.6557, 0.4797, 1.0875],
        [0.4987, 0.5066, 0.9008],
        [0.6724, 0.4535, 1.1238],
        [0.7172, 0.3994, 1.1922],
        [0.5216, 0.5006, 0.9551],
        [0.5110, 0.5091, 0.9189],
        [0.6491, 0.4921, 1.0998],
        [0.5753, 0.5215, 1.0070],
        [0.5496, 0.5245, 0.9718],
        [0.6104, 0.5111, 1.0455],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6593, 0.4781, 1.0921],
        [0.5358, 0.5059, 0.9385],
        [0.5130, 0.5035, 0.9084],
        [0.4958, 0.5174, 0.8965],
        [0.8340, 0.3174, 1.2795],
        [0.6214, 0.4690, 1.0405],
        [0.6777, 0.4950, 1.1162],
        [0.5009, 0.5132, 0.9015],
        [0.7748, 0.3475, 1.2341],
        [0.6857, 0.4582, 1.1246],
        [0.8143, 0.3915, 1.2524],
        [0.6562, 0.4790, 1.0883],
        [0.6733, 0.4627, 1.1020],
        [0.5060, 0.5029, 0.9057],
        [0.7058, 0.4417, 1.1507],
        [0.6746, 0.4926, 1.1040],
        [0.5650, 0.4688, 0.9542],
        [0.6492, 0.4729, 1.0799],
        [0.6061, 0.5095, 1.0402],
        [0.5387, 0.5050, 0.9366],
        [0.7912, 0.3203, 1.2739],
        [0.5610, 0.5125, 0.9724],
        [0.6366, 0.4742, 1.0491],
        [0.5272, 0.5110, 0.9345],
        [0.7027, 0.4601, 1.1563],
        [0.7352, 0.4137, 1.1986],
        [0.6099, 0.5182, 1.0356],
        [0.7734, 0.3496, 1.2331],
        [0.4999, 0.5147, 0.9072],
        [0.7734, 0.3496, 1.2331],
        [0.5665, 0.4872, 0.9744],
        [0.6862, 0.4537, 1.1409],
        [0.7745, 0.3467, 1.2341],
        [0.7057, 0.4405, 1.1224],
        [0.6587, 0.4783, 1.0915],
        [0.7172, 0.3994, 1.1922],
        [0.6474, 0.5392, 1.0558],
        [0.5750, 0.4993, 1.0236],
        [0.4988, 0.5183, 0.8986],
        [0.6345, 0.4999, 1.0777],
        [0.5626, 0.4683, 0.9531],
        [0.6476, 0.4697, 1.0592],
        [0.5222, 0.5105, 0.9185],
        [0.7542, 0.3864, 1.2347],
        [0.5315, 0.5033, 0.9342],
        [0.6277, 0.4921, 1.0419],
        [0.8314, 0.3229, 1.2769],
        [0.6677, 0.4244, 1.1465],
        [0.5118, 0.5159, 0.9346],
        [0.5687, 0.5231, 0.9644],
        [0.5221, 0.5073, 0.9164],
        [0.6865, 0.4664, 1.1048],
        [0.4906, 0.5129, 0.8975],
        [0.5319, 0.5263, 0.9330],
        [0.6870, 0.4654, 1.1049],
        [0.5322, 0.5274, 0.9338],
        [0.5212, 0.5085, 0.9172],
        [0.7783, 0.3576, 1.2273],
        [0.7196, 0.3959, 1.1841],
        [0.5444, 0.5186, 0.9663],
        [0.6557, 0.4797, 1.0875],
        [0.5895, 0.4725, 0.9825],
        [0.5620, 0.4988, 0.9988],
        [0.4996, 0.5183, 0.8986],
        [0.5496, 0.5245, 0.9718],
        [0.6478, 0.4835, 1.0715],
        [0.6125, 0.5212, 1.0318],
        [0.5909, 0.4728, 0.9834],
        [0.6024, 0.5062, 1.0354],
        [0.8143, 0.3915, 1.2524],
        [0.6496, 0.4924, 1.1000],
        [0.5665, 0.4872, 0.9744],
        [0.5380, 0.5064, 0.9387],
        [0.5372, 0.5036, 0.9349],
        [0.5586, 0.5207, 0.9788],
        [0.6174, 0.5134, 1.0458],
        [0.5769, 0.5190, 1.0033],
        [0.5310, 0.5030, 0.9333],
        [0.7548, 0.3867, 1.2347],
        [0.7708, 0.3262, 1.2567],
        [0.6751, 0.4918, 1.1046],
        [0.5343, 0.5142, 0.9454],
        [0.6170, 0.5178, 1.0354],
        [0.5750, 0.4993, 1.0236],
        [0.6005, 0.5081, 1.0327],
        [0.5586, 0.5091, 0.9952],
        [0.6564, 0.5133, 1.0577],
        [0.5749, 0.5198, 1.0185],
        [0.7757, 0.3607, 1.2232],
        [0.6497, 0.4802, 1.1098],
        [0.7014, 0.3785, 1.1984],
        [0.6195, 0.5338, 1.0443],
        [0.5754, 0.4879, 0.9945],
        [0.5721, 0.5000, 0.9666],
        [0.6859, 0.4582, 1.1249],
        [0.5301, 0.5108, 0.9440],
        [0.5428, 0.5025, 0.9557],
        [0.7206, 0.4028, 1.1934],
        [0.6776, 0.4984, 1.1252],
        [0.5665, 0.4872, 0.9744],
        [0.7543, 0.3861, 1.2353],
        [0.5315, 0.5033, 0.9342],
        [0.4825, 0.5128, 0.8875],
        [0.5266, 0.5112, 0.9405],
        [0.6851, 0.4547, 1.1404],
        [0.5398, 0.5092, 0.9342],
        [0.5368, 0.5152, 0.9856],
        [0.6188, 0.5169, 1.0376],
        [0.7085, 0.4403, 1.1551],
        [0.5647, 0.4769, 0.9582],
        [0.5972, 0.4953, 1.0292],
        [0.7464, 0.3961, 1.2147],
        [0.6864, 0.4669, 1.1037],
        [0.5650, 0.5185, 1.0117],
        [0.5322, 0.5274, 0.9338],
        [0.7477, 0.3946, 1.2154],
        [0.6735, 0.4516, 1.1262],
        [0.7543, 0.3861, 1.2353],
        [0.5946, 0.4948, 1.0251],
        [0.6792, 0.4950, 1.1175],
        [0.6726, 0.4529, 1.1246],
        [0.5089, 0.5158, 0.9333],
        [0.7562, 0.3540, 1.2381],
        [0.5785, 0.5177, 0.9727],
        [0.6748, 0.4621, 1.1048],
        [0.5995, 0.5083, 1.0321],
        [0.5620, 0.4988, 0.9988],
        [0.5954, 0.4951, 1.0266],
        [0.5349, 0.5016, 0.9331],
        [0.5212, 0.5085, 0.9172],
        [0.6384, 0.4933, 1.0797],
        [0.5387, 0.5050, 0.9366],
        [0.5838, 0.5038, 0.9884],
        [0.6396, 0.5013, 1.0701],
        [0.5293, 0.5087, 0.9312],
        [0.5341, 0.5153, 0.9472],
        [0.8131, 0.3904, 1.2520],
        [0.6859, 0.4582, 1.1249],
        [0.7058, 0.4417, 1.1507],
        [0.7738, 0.3490, 1.2336],
        [0.5373, 0.5384, 0.9498],
        [0.7757, 0.3607, 1.2232],
        [0.6690, 0.4932, 1.1100],
        [0.6491, 0.4921, 1.0998],
        [0.5216, 0.5005, 0.9552],
        [0.5361, 0.5152, 0.9848],
        [0.5838, 0.5038, 0.9884],
        [0.5117, 0.5101, 0.9174],
        [0.5647, 0.4769, 0.9582],
        [0.5903, 0.4726, 0.9832],
        [0.6473, 0.4647, 1.1180],
        [0.6306, 0.4765, 1.0431],
        [0.6408, 0.4749, 1.1030],
        [0.6399, 0.5331, 1.0489],
        [0.6874, 0.4638, 1.1052],
        [0.5387, 0.5050, 0.9366],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6751, 0.5081, 1.1314],
        [0.5383, 0.5049, 0.9577],
        [0.5707, 0.5226, 0.9970],
        [0.6303, 0.5025, 1.0602],
        [0.7545, 0.3860, 1.2353],
        [0.6950, 0.4675, 1.1123],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.6497, 0.4799, 1.0707],
        [0.8081, 0.3574, 1.2713],
        [0.8453, 0.2825, 1.3216],
        [0.5225, 0.5116, 0.9201],
        [0.7523, 0.3903, 1.2298],
        [0.6422, 0.5049, 1.1026],
        [0.5785, 0.5177, 0.9727],
        [0.6022, 0.5069, 1.0348],
        [0.6229, 0.4830, 1.0827],
        [0.6848, 0.4550, 1.1403],
        [0.6865, 0.4664, 1.1048],
        [0.5678, 0.5215, 1.0155],
        [0.6000, 0.4962, 1.0335],
        [0.5657, 0.4865, 0.9743],
        [0.5060, 0.5029, 0.9057],
        [0.5354, 0.5149, 0.9450],
        [0.6659, 0.4234, 1.1465],
        [0.5119, 0.5159, 0.9348],
        [0.5721, 0.4994, 0.9800],
        [0.5097, 0.5154, 0.9333],
        [0.6593, 0.4781, 1.0921],
        [0.7014, 0.3785, 1.1984],
        [0.7206, 0.4028, 1.1934],
        [0.4825, 0.5121, 0.8868],
        [0.5146, 0.5101, 0.9139],
        [0.6402, 0.5323, 1.0487],
        [0.4996, 0.5183, 0.8986],
        [0.6384, 0.4933, 1.0797],
        [0.7780, 0.3589, 1.2259],
        [0.6557, 0.4797, 1.0875],
        [0.5288, 0.5086, 0.9309],
        [0.5766, 0.5006, 1.0299],
        [0.5610, 0.4681, 0.9523],
        [0.6790, 0.4452, 1.1540],
        [0.6270, 0.4784, 1.0915],
        [0.6402, 0.5323, 1.0487],
        [0.6522, 0.4819, 1.1071],
        [0.5275, 0.5112, 0.9335],
        [0.8334, 0.3202, 1.2785],
        [0.7689, 0.3591, 1.2363],
        [0.5895, 0.4725, 0.9825],
        [0.6184, 0.5125, 1.0475],
        [0.8314, 0.3229, 1.2769],
        [0.6476, 0.4697, 1.0592]], device='mps:0')
2025-03-01 01:40:18 - INFO - Y batch argmax : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:19 - INFO - Outputs : tensor([[0.5217, 0.5098, 0.9181],
        [0.7765, 0.3618, 1.2210],
        [0.5598, 0.5121, 0.9708],
        [0.7731, 0.3248, 1.2598],
        [0.5701, 0.4763, 0.9590],
        [0.5946, 0.4948, 1.0251],
        [0.6024, 0.5062, 1.0354],
        [0.7406, 0.4038, 1.2077],
        [0.6471, 0.4738, 1.0791],
        [0.6580, 0.4784, 1.0902],
        [0.5979, 0.5085, 1.0310],
        [0.4984, 0.5065, 0.9005],
        [0.7514, 0.3564, 1.2320],
        [0.5315, 0.5262, 0.9334],
        [0.7172, 0.3994, 1.1922],
        [0.6504, 0.4807, 1.1091],
        [0.5741, 0.5214, 1.0051],
        [0.5642, 0.4766, 0.9579],
        [0.6061, 0.5095, 1.0402],
        [0.5636, 0.4986, 1.0012],
        [0.6865, 0.4668, 1.1039],
        [0.5212, 0.5092, 0.9175],
        [0.6417, 0.4751, 1.1041],
        [0.6515, 0.4809, 1.1092],
        [0.7027, 0.4601, 1.1563],
        [0.5316, 0.5265, 0.9335],
        [0.5388, 0.4909, 0.9606],
        [0.6455, 0.4929, 1.0900],
        [0.5447, 0.5196, 0.9672],
        [0.5565, 0.4812, 0.9450],
        [0.7014, 0.3785, 1.1984],
        [0.6747, 0.4922, 1.1047],
        [0.6394, 0.5333, 1.0483],
        [0.5933, 0.4949, 1.0232],
        [0.5703, 0.4757, 0.9589],
        [0.6265, 0.4796, 1.0902],
        [0.6195, 0.5338, 1.0443],
        [0.6864, 0.4669, 1.1037],
        [0.5275, 0.5112, 0.9335],
        [0.6622, 0.4161, 1.1484],
        [0.5758, 0.4881, 0.9952],
        [0.7014, 0.4624, 1.1538],
        [0.5004, 0.5147, 0.9072],
        [0.5319, 0.5106, 0.9458],
        [0.5846, 0.5129, 0.9968],
        [0.6317, 0.4891, 1.0599],
        [0.6471, 0.4734, 1.0783],
        [0.5750, 0.4993, 1.0236],
        [0.6325, 0.5023, 1.0624],
        [0.6522, 0.4819, 1.1071],
        [0.7530, 0.3894, 1.2305],
        [0.6352, 0.5022, 1.0647],
        [0.7167, 0.3957, 1.1927],
        [0.5225, 0.5116, 0.9201],
        [0.4966, 0.5186, 0.8985],
        [0.6757, 0.4945, 1.1151],
        [0.5650, 0.4688, 0.9542],
        [0.5991, 0.5406, 1.0148],
        [0.5716, 0.4992, 0.9793],
        [0.5744, 0.4884, 0.9933],
        [0.5393, 0.5093, 0.9347],
        [0.6656, 0.4240, 1.1462],
        [0.5838, 0.5038, 0.9884],
        [0.6652, 0.4279, 1.1457],
        [0.5586, 0.5091, 0.9952],
        [0.5266, 0.5112, 0.9405],
        [0.6880, 0.4634, 1.1053],
        [0.6747, 0.4922, 1.1047],
        [0.6011, 0.5143, 1.0162],
        [0.4966, 0.5188, 0.8984],
        [0.6270, 0.4976, 1.0738],
        [0.7767, 0.3610, 1.2222],
        [0.5633, 0.4685, 0.9535],
        [0.5705, 0.4990, 0.9782],
        [0.6988, 0.4610, 1.1260],
        [0.5201, 0.5006, 0.9513],
        [0.5400, 0.5248, 0.9444],
        [0.7206, 0.3950, 1.1853],
        [0.7562, 0.3540, 1.2381],
        [0.4909, 0.5134, 0.8978],
        [0.5118, 0.5159, 0.9346],
        [0.7738, 0.3490, 1.2336],
        [0.6400, 0.4727, 1.0520],
        [0.6270, 0.4784, 1.0915],
        [0.6419, 0.4720, 1.0536],
        [0.7429, 0.4007, 1.2111],
        [0.6734, 0.5072, 1.1310],
        [0.6754, 0.5082, 1.1314],
        [0.6260, 0.4998, 1.0553],
        [0.6880, 0.4634, 1.1053],
        [0.5377, 0.5385, 0.9515],
        [0.4974, 0.5185, 0.8985],
        [0.6011, 0.4963, 1.0350],
        [0.5690, 0.4769, 0.9590],
        [0.5400, 0.5092, 0.9343],
        [0.6478, 0.4835, 1.0715],
        [0.6161, 0.5141, 1.0433],
        [0.5413, 0.5232, 0.9436],
        [0.5439, 0.5019, 0.9558],
        [0.6857, 0.4582, 1.1246],
        [0.6880, 0.4634, 1.1053],
        [0.6188, 0.5119, 1.0491],
        [0.5400, 0.5092, 0.9343],
        [0.6312, 0.4892, 1.0587],
        [0.7206, 0.4028, 1.1934],
        [0.6870, 0.4654, 1.1049],
        [0.8126, 0.3892, 1.2520],
        [0.8334, 0.3189, 1.2791],
        [0.5369, 0.5048, 0.9558],
        [0.4920, 0.5108, 0.8958],
        [0.5318, 0.5132, 0.9446],
        [0.4967, 0.5187, 0.8983],
        [0.7049, 0.4486, 1.1463],
        [0.5661, 0.5198, 1.0132],
        [0.6656, 0.4240, 1.1462],
        [0.6731, 0.4940, 1.1134],
        [0.5645, 0.4687, 0.9541],
        [0.5898, 0.4723, 0.9826],
        [0.5337, 0.5137, 0.9454],
        [0.7776, 0.3593, 1.2251],
        [0.6858, 0.4581, 1.1245],
        [0.4964, 0.5184, 0.8979],
        [0.5116, 0.5097, 0.9181],
        [0.5707, 0.5226, 0.9970],
        [0.5275, 0.5112, 0.9335],
        [0.5380, 0.5036, 0.9372],
        [0.7783, 0.3576, 1.2273],
        [0.6733, 0.4627, 1.1020],
        [0.5753, 0.4877, 0.9949],
        [0.5225, 0.5111, 0.9193],
        [0.6366, 0.4742, 1.0491],
        [0.6631, 0.4129, 1.1498],
        [0.6111, 0.5173, 1.0371],
        [0.6023, 0.5118, 1.0365],
        [0.6455, 0.4705, 1.0569],
        [0.7085, 0.4403, 1.1551],
        [0.5116, 0.5097, 0.9181],
        [0.7206, 0.3950, 1.1853],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.5225, 0.5111, 0.9193],
        [0.5337, 0.5300, 0.9333],
        [0.4824, 0.5125, 0.8871],
        [0.7748, 0.3475, 1.2341],
        [0.5667, 0.5208, 1.0142],
        [0.6761, 0.4485, 1.1493],
        [0.8426, 0.2834, 1.3188],
        [0.7542, 0.3864, 1.2347],
        [0.6817, 0.4220, 1.1541],
        [0.5116, 0.5100, 0.9175],
        [0.6652, 0.4279, 1.1457],
        [0.5225, 0.5116, 0.9201],
        [0.6338, 0.5003, 1.0771],
        [0.4966, 0.5186, 0.8985],
        [0.5207, 0.5004, 0.9534],
        [0.6037, 0.5115, 1.0377],
        [0.8128, 0.3895, 1.2519],
        [0.5381, 0.5045, 0.9373],
        [0.5024, 0.5125, 0.9020],
        [0.7177, 0.4006, 1.1922],
        [0.6188, 0.5114, 1.0519],
        [0.6037, 0.5115, 1.0377],
        [0.6024, 0.5062, 1.0354],
        [0.4986, 0.5065, 0.9006],
        [0.6270, 0.4976, 1.0738],
        [0.5345, 0.5316, 0.9330],
        [0.5663, 0.4869, 0.9745],
        [0.5954, 0.4951, 1.0266],
        [0.5765, 0.5005, 1.0295],
        [0.6628, 0.4143, 1.1493],
        [0.5343, 0.5153, 0.9468],
        [0.6751, 0.5081, 1.1314],
        [0.6656, 0.4240, 1.1462],
        [0.5950, 0.5082, 0.9950],
        [0.5745, 0.5202, 1.0178],
        [0.7464, 0.3961, 1.2147],
        [0.7298, 0.4085, 1.1756],
        [0.5368, 0.5152, 0.9856],
        [0.7543, 0.3861, 1.2353],
        [0.7562, 0.3540, 1.2381],
        [0.7528, 0.3893, 1.2312],
        [0.6465, 0.4648, 1.1166],
        [0.5353, 0.5030, 0.9338],
        [0.5054, 0.5024, 0.9044],
        [0.5130, 0.5035, 0.9084],
        [0.5721, 0.4994, 0.9800],
        [0.6748, 0.4499, 1.1475],
        [0.5717, 0.4999, 0.9664],
        [0.8453, 0.2825, 1.3216],
        [0.6280, 0.5029, 1.0579],
        [0.5959, 0.4952, 1.0275],
        [0.5565, 0.4812, 0.9450],
        [0.6300, 0.4891, 1.0387],
        [0.6167, 0.5202, 1.0365],
        [0.5388, 0.4909, 0.9606],
        [0.5258, 0.5363, 0.9441],
        [0.7035, 0.4091, 1.1636],
        [0.5859, 0.5130, 0.9980],
        [0.5280, 0.5106, 0.9416],
        [0.7077, 0.4424, 1.1521],
        [0.7003, 0.4610, 1.1276],
        [0.6742, 0.4929, 1.1033],
        [0.5221, 0.5073, 0.9164],
        [0.6959, 0.4678, 1.1119],
        [0.5221, 0.5073, 0.9164],
        [0.5756, 0.5191, 1.0198],
        [0.5146, 0.5101, 0.9139],
        [0.5661, 0.5198, 1.0132],
        [0.6405, 0.5045, 1.0986],
        [0.5280, 0.5106, 0.9416],
        [0.7913, 0.3256, 1.2717],
        [0.6023, 0.5147, 1.0170],
        [0.6434, 0.4712, 1.0546],
        [0.5946, 0.4948, 1.0251],
        [0.5009, 0.5132, 0.9015],
        [0.6859, 0.4582, 1.1249],
        [0.6751, 0.5081, 1.1314],
        [0.6808, 0.4949, 1.1192],
        [0.6496, 0.4804, 1.0708],
        [0.6855, 0.4673, 1.1033],
        [0.7543, 0.3861, 1.2353],
        [0.6306, 0.4765, 1.0431],
        [0.6656, 0.4266, 1.1451],
        [0.6639, 0.4114, 1.1507],
        [0.4984, 0.5065, 0.9005],
        [0.6872, 0.4646, 1.1054],
        [0.6726, 0.4529, 1.1246],
        [0.5383, 0.5049, 0.9577],
        [0.7298, 0.4085, 1.1756],
        [0.6398, 0.5321, 1.0484],
        [0.6180, 0.4838, 1.0762],
        [0.7017, 0.3792, 1.1981],
        [0.5347, 0.5145, 0.9453],
        [0.7689, 0.3591, 1.2363],
        [0.5341, 0.5153, 0.9472],
        [0.6170, 0.5178, 1.0354],
        [0.5964, 0.4987, 1.0235],
        [0.6052, 0.5099, 1.0389],
        [0.7213, 0.3944, 1.1854],
        [0.5114, 0.5102, 0.9173],
        [0.5222, 0.5105, 0.9185],
        [0.5741, 0.5203, 1.0168],
        [0.7396, 0.4079, 1.2051],
        [0.6884, 0.4634, 1.1152],
        [0.5995, 0.5083, 1.0321],
        [0.6400, 0.5325, 1.0485],
        [0.7046, 0.4488, 1.1469],
        [0.5586, 0.5091, 0.9954],
        [0.7043, 0.4488, 1.1460],
        [0.6280, 0.5029, 1.0579],
        [0.5784, 0.5180, 0.9728],
        [0.4966, 0.5186, 0.8985],
        [0.5223, 0.5072, 0.9160],
        [0.5243, 0.5117, 0.9395],
        [0.6346, 0.4751, 1.0473],
        [0.6011, 0.5143, 1.0162]], device='mps:0')
2025-03-01 01:40:19 - INFO - Outputs shape : torch.Size([256, 3])
2025-03-01 01:40:19 - INFO - Y batch : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:19 - INFO - Y batch shape : torch.Size([256, 3])
2025-03-01 01:40:20 - INFO - Outputs argmax : tensor([[0.5217, 0.5098, 0.9181],
        [0.7765, 0.3618, 1.2210],
        [0.5598, 0.5121, 0.9708],
        [0.7731, 0.3248, 1.2598],
        [0.5701, 0.4763, 0.9590],
        [0.5946, 0.4948, 1.0251],
        [0.6024, 0.5062, 1.0354],
        [0.7406, 0.4038, 1.2077],
        [0.6471, 0.4738, 1.0791],
        [0.6580, 0.4784, 1.0902],
        [0.5979, 0.5085, 1.0310],
        [0.4984, 0.5065, 0.9005],
        [0.7514, 0.3564, 1.2320],
        [0.5315, 0.5262, 0.9334],
        [0.7172, 0.3994, 1.1922],
        [0.6504, 0.4807, 1.1091],
        [0.5741, 0.5214, 1.0051],
        [0.5642, 0.4766, 0.9579],
        [0.6061, 0.5095, 1.0402],
        [0.5636, 0.4986, 1.0012],
        [0.6865, 0.4668, 1.1039],
        [0.5212, 0.5092, 0.9175],
        [0.6417, 0.4751, 1.1041],
        [0.6515, 0.4809, 1.1092],
        [0.7027, 0.4601, 1.1563],
        [0.5316, 0.5265, 0.9335],
        [0.5388, 0.4909, 0.9606],
        [0.6455, 0.4929, 1.0900],
        [0.5447, 0.5196, 0.9672],
        [0.5565, 0.4812, 0.9450],
        [0.7014, 0.3785, 1.1984],
        [0.6747, 0.4922, 1.1047],
        [0.6394, 0.5333, 1.0483],
        [0.5933, 0.4949, 1.0232],
        [0.5703, 0.4757, 0.9589],
        [0.6265, 0.4796, 1.0902],
        [0.6195, 0.5338, 1.0443],
        [0.6864, 0.4669, 1.1037],
        [0.5275, 0.5112, 0.9335],
        [0.6622, 0.4161, 1.1484],
        [0.5758, 0.4881, 0.9952],
        [0.7014, 0.4624, 1.1538],
        [0.5004, 0.5147, 0.9072],
        [0.5319, 0.5106, 0.9458],
        [0.5846, 0.5129, 0.9968],
        [0.6317, 0.4891, 1.0599],
        [0.6471, 0.4734, 1.0783],
        [0.5750, 0.4993, 1.0236],
        [0.6325, 0.5023, 1.0624],
        [0.6522, 0.4819, 1.1071],
        [0.7530, 0.3894, 1.2305],
        [0.6352, 0.5022, 1.0647],
        [0.7167, 0.3957, 1.1927],
        [0.5225, 0.5116, 0.9201],
        [0.4966, 0.5186, 0.8985],
        [0.6757, 0.4945, 1.1151],
        [0.5650, 0.4688, 0.9542],
        [0.5991, 0.5406, 1.0148],
        [0.5716, 0.4992, 0.9793],
        [0.5744, 0.4884, 0.9933],
        [0.5393, 0.5093, 0.9347],
        [0.6656, 0.4240, 1.1462],
        [0.5838, 0.5038, 0.9884],
        [0.6652, 0.4279, 1.1457],
        [0.5586, 0.5091, 0.9952],
        [0.5266, 0.5112, 0.9405],
        [0.6880, 0.4634, 1.1053],
        [0.6747, 0.4922, 1.1047],
        [0.6011, 0.5143, 1.0162],
        [0.4966, 0.5188, 0.8984],
        [0.6270, 0.4976, 1.0738],
        [0.7767, 0.3610, 1.2222],
        [0.5633, 0.4685, 0.9535],
        [0.5705, 0.4990, 0.9782],
        [0.6988, 0.4610, 1.1260],
        [0.5201, 0.5006, 0.9513],
        [0.5400, 0.5248, 0.9444],
        [0.7206, 0.3950, 1.1853],
        [0.7562, 0.3540, 1.2381],
        [0.4909, 0.5134, 0.8978],
        [0.5118, 0.5159, 0.9346],
        [0.7738, 0.3490, 1.2336],
        [0.6400, 0.4727, 1.0520],
        [0.6270, 0.4784, 1.0915],
        [0.6419, 0.4720, 1.0536],
        [0.7429, 0.4007, 1.2111],
        [0.6734, 0.5072, 1.1310],
        [0.6754, 0.5082, 1.1314],
        [0.6260, 0.4998, 1.0553],
        [0.6880, 0.4634, 1.1053],
        [0.5377, 0.5385, 0.9515],
        [0.4974, 0.5185, 0.8985],
        [0.6011, 0.4963, 1.0350],
        [0.5690, 0.4769, 0.9590],
        [0.5400, 0.5092, 0.9343],
        [0.6478, 0.4835, 1.0715],
        [0.6161, 0.5141, 1.0433],
        [0.5413, 0.5232, 0.9436],
        [0.5439, 0.5019, 0.9558],
        [0.6857, 0.4582, 1.1246],
        [0.6880, 0.4634, 1.1053],
        [0.6188, 0.5119, 1.0491],
        [0.5400, 0.5092, 0.9343],
        [0.6312, 0.4892, 1.0587],
        [0.7206, 0.4028, 1.1934],
        [0.6870, 0.4654, 1.1049],
        [0.8126, 0.3892, 1.2520],
        [0.8334, 0.3189, 1.2791],
        [0.5369, 0.5048, 0.9558],
        [0.4920, 0.5108, 0.8958],
        [0.5318, 0.5132, 0.9446],
        [0.4967, 0.5187, 0.8983],
        [0.7049, 0.4486, 1.1463],
        [0.5661, 0.5198, 1.0132],
        [0.6656, 0.4240, 1.1462],
        [0.6731, 0.4940, 1.1134],
        [0.5645, 0.4687, 0.9541],
        [0.5898, 0.4723, 0.9826],
        [0.5337, 0.5137, 0.9454],
        [0.7776, 0.3593, 1.2251],
        [0.6858, 0.4581, 1.1245],
        [0.4964, 0.5184, 0.8979],
        [0.5116, 0.5097, 0.9181],
        [0.5707, 0.5226, 0.9970],
        [0.5275, 0.5112, 0.9335],
        [0.5380, 0.5036, 0.9372],
        [0.7783, 0.3576, 1.2273],
        [0.6733, 0.4627, 1.1020],
        [0.5753, 0.4877, 0.9949],
        [0.5225, 0.5111, 0.9193],
        [0.6366, 0.4742, 1.0491],
        [0.6631, 0.4129, 1.1498],
        [0.6111, 0.5173, 1.0371],
        [0.6023, 0.5118, 1.0365],
        [0.6455, 0.4705, 1.0569],
        [0.7085, 0.4403, 1.1551],
        [0.5116, 0.5097, 0.9181],
        [0.7206, 0.3950, 1.1853],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.5225, 0.5111, 0.9193],
        [0.5337, 0.5300, 0.9333],
        [0.4824, 0.5125, 0.8871],
        [0.7748, 0.3475, 1.2341],
        [0.5667, 0.5208, 1.0142],
        [0.6761, 0.4485, 1.1493],
        [0.8426, 0.2834, 1.3188],
        [0.7542, 0.3864, 1.2347],
        [0.6817, 0.4220, 1.1541],
        [0.5116, 0.5100, 0.9175],
        [0.6652, 0.4279, 1.1457],
        [0.5225, 0.5116, 0.9201],
        [0.6338, 0.5003, 1.0771],
        [0.4966, 0.5186, 0.8985],
        [0.5207, 0.5004, 0.9534],
        [0.6037, 0.5115, 1.0377],
        [0.8128, 0.3895, 1.2519],
        [0.5381, 0.5045, 0.9373],
        [0.5024, 0.5125, 0.9020],
        [0.7177, 0.4006, 1.1922],
        [0.6188, 0.5114, 1.0519],
        [0.6037, 0.5115, 1.0377],
        [0.6024, 0.5062, 1.0354],
        [0.4986, 0.5065, 0.9006],
        [0.6270, 0.4976, 1.0738],
        [0.5345, 0.5316, 0.9330],
        [0.5663, 0.4869, 0.9745],
        [0.5954, 0.4951, 1.0266],
        [0.5765, 0.5005, 1.0295],
        [0.6628, 0.4143, 1.1493],
        [0.5343, 0.5153, 0.9468],
        [0.6751, 0.5081, 1.1314],
        [0.6656, 0.4240, 1.1462],
        [0.5950, 0.5082, 0.9950],
        [0.5745, 0.5202, 1.0178],
        [0.7464, 0.3961, 1.2147],
        [0.7298, 0.4085, 1.1756],
        [0.5368, 0.5152, 0.9856],
        [0.7543, 0.3861, 1.2353],
        [0.7562, 0.3540, 1.2381],
        [0.7528, 0.3893, 1.2312],
        [0.6465, 0.4648, 1.1166],
        [0.5353, 0.5030, 0.9338],
        [0.5054, 0.5024, 0.9044],
        [0.5130, 0.5035, 0.9084],
        [0.5721, 0.4994, 0.9800],
        [0.6748, 0.4499, 1.1475],
        [0.5717, 0.4999, 0.9664],
        [0.8453, 0.2825, 1.3216],
        [0.6280, 0.5029, 1.0579],
        [0.5959, 0.4952, 1.0275],
        [0.5565, 0.4812, 0.9450],
        [0.6300, 0.4891, 1.0387],
        [0.6167, 0.5202, 1.0365],
        [0.5388, 0.4909, 0.9606],
        [0.5258, 0.5363, 0.9441],
        [0.7035, 0.4091, 1.1636],
        [0.5859, 0.5130, 0.9980],
        [0.5280, 0.5106, 0.9416],
        [0.7077, 0.4424, 1.1521],
        [0.7003, 0.4610, 1.1276],
        [0.6742, 0.4929, 1.1033],
        [0.5221, 0.5073, 0.9164],
        [0.6959, 0.4678, 1.1119],
        [0.5221, 0.5073, 0.9164],
        [0.5756, 0.5191, 1.0198],
        [0.5146, 0.5101, 0.9139],
        [0.5661, 0.5198, 1.0132],
        [0.6405, 0.5045, 1.0986],
        [0.5280, 0.5106, 0.9416],
        [0.7913, 0.3256, 1.2717],
        [0.6023, 0.5147, 1.0170],
        [0.6434, 0.4712, 1.0546],
        [0.5946, 0.4948, 1.0251],
        [0.5009, 0.5132, 0.9015],
        [0.6859, 0.4582, 1.1249],
        [0.6751, 0.5081, 1.1314],
        [0.6808, 0.4949, 1.1192],
        [0.6496, 0.4804, 1.0708],
        [0.6855, 0.4673, 1.1033],
        [0.7543, 0.3861, 1.2353],
        [0.6306, 0.4765, 1.0431],
        [0.6656, 0.4266, 1.1451],
        [0.6639, 0.4114, 1.1507],
        [0.4984, 0.5065, 0.9005],
        [0.6872, 0.4646, 1.1054],
        [0.6726, 0.4529, 1.1246],
        [0.5383, 0.5049, 0.9577],
        [0.7298, 0.4085, 1.1756],
        [0.6398, 0.5321, 1.0484],
        [0.6180, 0.4838, 1.0762],
        [0.7017, 0.3792, 1.1981],
        [0.5347, 0.5145, 0.9453],
        [0.7689, 0.3591, 1.2363],
        [0.5341, 0.5153, 0.9472],
        [0.6170, 0.5178, 1.0354],
        [0.5964, 0.4987, 1.0235],
        [0.6052, 0.5099, 1.0389],
        [0.7213, 0.3944, 1.1854],
        [0.5114, 0.5102, 0.9173],
        [0.5222, 0.5105, 0.9185],
        [0.5741, 0.5203, 1.0168],
        [0.7396, 0.4079, 1.2051],
        [0.6884, 0.4634, 1.1152],
        [0.5995, 0.5083, 1.0321],
        [0.6400, 0.5325, 1.0485],
        [0.7046, 0.4488, 1.1469],
        [0.5586, 0.5091, 0.9954],
        [0.7043, 0.4488, 1.1460],
        [0.6280, 0.5029, 1.0579],
        [0.5784, 0.5180, 0.9728],
        [0.4966, 0.5186, 0.8985],
        [0.5223, 0.5072, 0.9160],
        [0.5243, 0.5117, 0.9395],
        [0.6346, 0.4751, 1.0473],
        [0.6011, 0.5143, 1.0162]], device='mps:0')
2025-03-01 01:40:20 - INFO - Y batch argmax : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:21 - INFO - Outputs : tensor([[0.6422, 0.5049, 1.1026],
        [0.7381, 0.4094, 1.2027],
        [0.5400, 0.5092, 0.9343],
        [0.7010, 0.3807, 1.1960],
        [0.6654, 0.4272, 1.1452],
        [0.5986, 0.4955, 1.0308],
        [0.5647, 0.4769, 0.9582],
        [0.6757, 0.4945, 1.1151],
        [0.6455, 0.4929, 1.0900],
        [0.5705, 0.4990, 0.9782],
        [0.6859, 0.4582, 1.1249],
        [0.6884, 0.4634, 1.1152],
        [0.6671, 0.4228, 1.1479],
        [0.7780, 0.3589, 1.2259],
        [0.6496, 0.4804, 1.0708],
        [0.5721, 0.4994, 0.9800],
        [0.5377, 0.5385, 0.9515],
        [0.7223, 0.4040, 1.1940],
        [0.7278, 0.4105, 1.1734],
        [0.8143, 0.3915, 1.2524],
        [0.5212, 0.5007, 0.9547],
        [0.6418, 0.5047, 1.1018],
        [0.6628, 0.4143, 1.1493],
        [0.6167, 0.5202, 1.0365],
        [0.6998, 0.4404, 1.1197],
        [0.7356, 0.4031, 1.1816],
        [0.6743, 0.4941, 1.1139],
        [0.5733, 0.5212, 1.0033],
        [0.5995, 0.5083, 1.0321],
        [0.5369, 0.5048, 0.9558],
        [0.6422, 0.5049, 1.1026],
        [0.5909, 0.4728, 0.9834],
        [0.5330, 0.5290, 0.9332],
        [0.6853, 0.4548, 1.1403],
        [0.7013, 0.3779, 1.1989],
        [0.7767, 0.3610, 1.2222],
        [0.5319, 0.5106, 0.9458],
        [0.7085, 0.4403, 1.1551],
        [0.7169, 0.3970, 1.1926],
        [0.5518, 0.5153, 0.9713],
        [0.5769, 0.5190, 1.0033],
        [0.6023, 0.5118, 1.0365],
        [0.5362, 0.5135, 0.9315],
        [0.5354, 0.5014, 0.9333],
        [0.6751, 0.4918, 1.1046],
        [0.7718, 0.3583, 1.2381],
        [0.7403, 0.4058, 1.2064],
        [0.5354, 0.5014, 0.9333],
        [0.5701, 0.4763, 0.9590],
        [0.5846, 0.5131, 0.9967],
        [0.6422, 0.5049, 1.1026],
        [0.6259, 0.5036, 1.0557],
        [0.5315, 0.5265, 0.9329],
        [0.6540, 0.4679, 1.0895],
        [0.5212, 0.5007, 0.9547],
        [0.6946, 0.4669, 1.1129],
        [0.5428, 0.5025, 0.9557],
        [0.5258, 0.5363, 0.9441],
        [0.6988, 0.4407, 1.1189],
        [0.6710, 0.4643, 1.0979],
        [0.5723, 0.5204, 0.9918],
        [0.6777, 0.4950, 1.1162],
        [0.7767, 0.3610, 1.2222],
        [0.6400, 0.5322, 1.0485],
        [0.6628, 0.4143, 1.1493],
        [0.7768, 0.3598, 1.2242],
        [0.4825, 0.5121, 0.8868],
        [0.6300, 0.4891, 1.0387],
        [0.5110, 0.5091, 0.9189],
        [0.5362, 0.4928, 0.9579],
        [0.5060, 0.5029, 0.9057],
        [0.6747, 0.4922, 1.1047],
        [0.5400, 0.5092, 0.9343],
        [0.5254, 0.5363, 0.9436],
        [0.6325, 0.5023, 1.0624],
        [0.6337, 0.4902, 1.0710],
        [0.6410, 0.5046, 1.0999],
        [0.6047, 0.5163, 1.0192],
        [0.6396, 0.5011, 1.0705],
        [0.6742, 0.4929, 1.1033],
        [0.6014, 0.4961, 1.0357],
        [0.5337, 0.5300, 0.9333],
        [0.6422, 0.5049, 1.1026],
        [0.5586, 0.5091, 0.9954],
        [0.6777, 0.4950, 1.1162],
        [0.6746, 0.4924, 1.1040],
        [0.5003, 0.5180, 0.8985],
        [0.6557, 0.4797, 1.0875],
        [0.7077, 0.4424, 1.1521],
        [0.6504, 0.4807, 1.1091],
        [0.5207, 0.5004, 0.9534],
        [0.6655, 0.4085, 1.1526],
        [0.5358, 0.5059, 0.9385],
        [0.6609, 0.4179, 1.1470],
        [0.6609, 0.4179, 1.1470],
        [0.5446, 0.5191, 0.9668],
        [0.6418, 0.5047, 1.1018],
        [0.6259, 0.4999, 1.0556],
        [0.7352, 0.4137, 1.1986],
        [0.5746, 0.4756, 0.9600],
        [0.7057, 0.4405, 1.1224],
        [0.6400, 0.4727, 1.0520],
        [0.5723, 0.5204, 0.9918],
        [0.5060, 0.5029, 0.9057],
        [0.6609, 0.4179, 1.1470],
        [0.6325, 0.5023, 1.0624],
        [0.7008, 0.3818, 1.1943],
        [0.6790, 0.4452, 1.1540],
        [0.5747, 0.4996, 1.0246],
        [0.7077, 0.4424, 1.1521],
        [0.5447, 0.5196, 0.9672],
        [0.6418, 0.5047, 1.1018],
        [0.6654, 0.4272, 1.1452],
        [0.6143, 0.5151, 1.0410],
        [0.5723, 0.5175, 0.9935],
        [0.6005, 0.5081, 1.0327],
        [0.6410, 0.5046, 1.0999],
        [0.6872, 0.4646, 1.1054],
        [0.6496, 0.4804, 1.0708],
        [0.5717, 0.4999, 0.9664],
        [0.7027, 0.4601, 1.1563],
        [0.5504, 0.5248, 0.9732],
        [0.5903, 0.4726, 0.9832],
        [0.4825, 0.5128, 0.8875],
        [0.6959, 0.4678, 1.1119],
        [0.5518, 0.5153, 0.9713],
        [0.6188, 0.5169, 1.0376],
        [0.6306, 0.4765, 1.0431],
        [0.6654, 0.4272, 1.1452],
        [0.4825, 0.5123, 0.8869],
        [0.5112, 0.5092, 0.9183],
        [0.5620, 0.4681, 0.9528],
        [0.5491, 0.4980, 0.9406],
        [0.5774, 0.5012, 1.0304],
        [0.7464, 0.3961, 1.2147],
        [0.5994, 0.4957, 1.0320],
        [0.6814, 0.4233, 1.1535],
        [0.6759, 0.5089, 1.1313],
        [0.6734, 0.5072, 1.1310],
        [0.7014, 0.4588, 1.1437],
        [0.7003, 0.4610, 1.1276],
        [0.6048, 0.5110, 1.0388],
        [0.6167, 0.5202, 1.0365],
        [0.5730, 0.4998, 0.9807],
        [0.5756, 0.5191, 1.0198],
        [0.5097, 0.5154, 0.9333],
        [0.5518, 0.5153, 0.9713],
        [0.6259, 0.4999, 1.0556],
        [0.5895, 0.4725, 0.9825],
        [0.6757, 0.4945, 1.1151],
        [0.5318, 0.5132, 0.9446],
        [0.6346, 0.4751, 1.0473],
        [0.5116, 0.5103, 0.9175],
        [0.6515, 0.4809, 1.1092],
        [0.5598, 0.5121, 0.9708],
        [0.5420, 0.5222, 0.9433],
        [0.5442, 0.5180, 0.9659],
        [0.5214, 0.5079, 0.9170],
        [0.6295, 0.4886, 1.0389],
        [0.5954, 0.4982, 1.0224],
        [0.6673, 0.4246, 1.1467],
        [0.5785, 0.5183, 0.9730],
        [0.5451, 0.5184, 0.9461],
        [0.4958, 0.5174, 0.8965],
        [0.5964, 0.5086, 1.0298],
        [0.5354, 0.5149, 0.9450],
        [0.5509, 0.5248, 0.9742],
        [0.4909, 0.5134, 0.8978],
        [0.5057, 0.5027, 0.9053],
        [0.8143, 0.3915, 1.2524],
        [0.6337, 0.4849, 1.0655],
        [0.6286, 0.4768, 1.0410],
        [0.5118, 0.5159, 0.9346],
        [0.7767, 0.3610, 1.2222],
        [0.5410, 0.5412, 0.9525],
        [0.6710, 0.4643, 1.0979],
        [0.5393, 0.5093, 0.9347],
        [0.6260, 0.4998, 1.0553],
        [0.5347, 0.5034, 0.9334],
        [0.5254, 0.5363, 0.9436],
        [0.5263, 0.5360, 0.9448],
        [0.4982, 0.5185, 0.8986],
        [0.6286, 0.4805, 1.0810],
        [0.7017, 0.4592, 1.1431],
        [0.7913, 0.3256, 1.2717],
        [0.5509, 0.5248, 0.9742],
        [0.6396, 0.5011, 1.0705],
        [0.7182, 0.4018, 1.1924],
        [0.6655, 0.4248, 1.1458],
        [0.5657, 0.4865, 0.9743],
        [0.5741, 0.5214, 1.0051],
        [0.6939, 0.4670, 1.1143],
        [0.7017, 0.3792, 1.1981],
        [0.5380, 0.5064, 0.9387],
        [0.5909, 0.4728, 0.9834],
        [0.5447, 0.5196, 0.9672],
        [0.4825, 0.5128, 0.8875],
        [0.6337, 0.4902, 1.0710],
        [0.7278, 0.4105, 1.1734],
        [0.7078, 0.4403, 1.1539],
        [0.6004, 0.5142, 1.0153],
        [0.6865, 0.4668, 1.1039],
        [0.5310, 0.5030, 0.9333],
        [0.5774, 0.5015, 1.0308],
        [0.6733, 0.4737, 1.1182],
        [0.5954, 0.4951, 1.0266],
        [0.5352, 0.5138, 0.9309],
        [0.7543, 0.3861, 1.2353],
        [0.5707, 0.5226, 0.9970],
        [0.5420, 0.5222, 0.9433],
        [0.6855, 0.4673, 1.1033],
        [0.4910, 0.5124, 0.8970],
        [0.6156, 0.5184, 1.0333],
        [0.6557, 0.4797, 1.0875],
        [0.7339, 0.4048, 1.1797],
        [0.6383, 0.4734, 1.0506],
        [0.6747, 0.4922, 1.1047],
        [0.7085, 0.4403, 1.1551],
        [0.8131, 0.3904, 1.2520],
        [0.7077, 0.4424, 1.1521],
        [0.7014, 0.4588, 1.1437],
        [0.5756, 0.5191, 1.0198],
        [0.5751, 0.5001, 1.0263],
        [0.7169, 0.3970, 1.1926],
        [0.6790, 0.4452, 1.1540],
        [0.5118, 0.5159, 0.9346],
        [0.5598, 0.5121, 0.9708],
        [0.6515, 0.4809, 1.1092],
        [0.5319, 0.5263, 0.9330],
        [0.7533, 0.3892, 1.2320],
        [0.6761, 0.4485, 1.1493],
        [0.5347, 0.5138, 0.9305],
        [0.6877, 0.4634, 1.1000],
        [0.5705, 0.4990, 0.9782],
        [0.6343, 0.4902, 1.0724],
        [0.6580, 0.4784, 1.0902],
        [0.7748, 0.3475, 1.2341],
        [0.5362, 0.5135, 0.9315],
        [0.5979, 0.5085, 1.0310],
        [0.5554, 0.4971, 0.9933],
        [0.8140, 0.3913, 1.2523],
        [0.5743, 0.4751, 0.9597],
        [0.6400, 0.5325, 1.0485],
        [0.6000, 0.5143, 1.0148],
        [0.5122, 0.5158, 0.9350],
        [0.5592, 0.5016, 0.9941],
        [0.6408, 0.4749, 1.1030],
        [0.4968, 0.5188, 0.8984],
        [0.5322, 0.5274, 0.9338],
        [0.5224, 0.5120, 0.9208],
        [0.5793, 0.5051, 1.0346],
        [0.5380, 0.5214, 0.9425],
        [0.7010, 0.3807, 1.1960],
        [0.5774, 0.5012, 1.0304],
        [0.5380, 0.5214, 0.9425],
        [0.5444, 0.5186, 0.9663]], device='mps:0')
2025-03-01 01:40:21 - INFO - Outputs shape : torch.Size([256, 3])
2025-03-01 01:40:21 - INFO - Y batch : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:21 - INFO - Y batch shape : torch.Size([256, 3])
2025-03-01 01:40:21 - INFO - Outputs argmax : tensor([[0.6422, 0.5049, 1.1026],
        [0.7381, 0.4094, 1.2027],
        [0.5400, 0.5092, 0.9343],
        [0.7010, 0.3807, 1.1960],
        [0.6654, 0.4272, 1.1452],
        [0.5986, 0.4955, 1.0308],
        [0.5647, 0.4769, 0.9582],
        [0.6757, 0.4945, 1.1151],
        [0.6455, 0.4929, 1.0900],
        [0.5705, 0.4990, 0.9782],
        [0.6859, 0.4582, 1.1249],
        [0.6884, 0.4634, 1.1152],
        [0.6671, 0.4228, 1.1479],
        [0.7780, 0.3589, 1.2259],
        [0.6496, 0.4804, 1.0708],
        [0.5721, 0.4994, 0.9800],
        [0.5377, 0.5385, 0.9515],
        [0.7223, 0.4040, 1.1940],
        [0.7278, 0.4105, 1.1734],
        [0.8143, 0.3915, 1.2524],
        [0.5212, 0.5007, 0.9547],
        [0.6418, 0.5047, 1.1018],
        [0.6628, 0.4143, 1.1493],
        [0.6167, 0.5202, 1.0365],
        [0.6998, 0.4404, 1.1197],
        [0.7356, 0.4031, 1.1816],
        [0.6743, 0.4941, 1.1139],
        [0.5733, 0.5212, 1.0033],
        [0.5995, 0.5083, 1.0321],
        [0.5369, 0.5048, 0.9558],
        [0.6422, 0.5049, 1.1026],
        [0.5909, 0.4728, 0.9834],
        [0.5330, 0.5290, 0.9332],
        [0.6853, 0.4548, 1.1403],
        [0.7013, 0.3779, 1.1989],
        [0.7767, 0.3610, 1.2222],
        [0.5319, 0.5106, 0.9458],
        [0.7085, 0.4403, 1.1551],
        [0.7169, 0.3970, 1.1926],
        [0.5518, 0.5153, 0.9713],
        [0.5769, 0.5190, 1.0033],
        [0.6023, 0.5118, 1.0365],
        [0.5362, 0.5135, 0.9315],
        [0.5354, 0.5014, 0.9333],
        [0.6751, 0.4918, 1.1046],
        [0.7718, 0.3583, 1.2381],
        [0.7403, 0.4058, 1.2064],
        [0.5354, 0.5014, 0.9333],
        [0.5701, 0.4763, 0.9590],
        [0.5846, 0.5131, 0.9967],
        [0.6422, 0.5049, 1.1026],
        [0.6259, 0.5036, 1.0557],
        [0.5315, 0.5265, 0.9329],
        [0.6540, 0.4679, 1.0895],
        [0.5212, 0.5007, 0.9547],
        [0.6946, 0.4669, 1.1129],
        [0.5428, 0.5025, 0.9557],
        [0.5258, 0.5363, 0.9441],
        [0.6988, 0.4407, 1.1189],
        [0.6710, 0.4643, 1.0979],
        [0.5723, 0.5204, 0.9918],
        [0.6777, 0.4950, 1.1162],
        [0.7767, 0.3610, 1.2222],
        [0.6400, 0.5322, 1.0485],
        [0.6628, 0.4143, 1.1493],
        [0.7768, 0.3598, 1.2242],
        [0.4825, 0.5121, 0.8868],
        [0.6300, 0.4891, 1.0387],
        [0.5110, 0.5091, 0.9189],
        [0.5362, 0.4928, 0.9579],
        [0.5060, 0.5029, 0.9057],
        [0.6747, 0.4922, 1.1047],
        [0.5400, 0.5092, 0.9343],
        [0.5254, 0.5363, 0.9436],
        [0.6325, 0.5023, 1.0624],
        [0.6337, 0.4902, 1.0710],
        [0.6410, 0.5046, 1.0999],
        [0.6047, 0.5163, 1.0192],
        [0.6396, 0.5011, 1.0705],
        [0.6742, 0.4929, 1.1033],
        [0.6014, 0.4961, 1.0357],
        [0.5337, 0.5300, 0.9333],
        [0.6422, 0.5049, 1.1026],
        [0.5586, 0.5091, 0.9954],
        [0.6777, 0.4950, 1.1162],
        [0.6746, 0.4924, 1.1040],
        [0.5003, 0.5180, 0.8985],
        [0.6557, 0.4797, 1.0875],
        [0.7077, 0.4424, 1.1521],
        [0.6504, 0.4807, 1.1091],
        [0.5207, 0.5004, 0.9534],
        [0.6655, 0.4085, 1.1526],
        [0.5358, 0.5059, 0.9385],
        [0.6609, 0.4179, 1.1470],
        [0.6609, 0.4179, 1.1470],
        [0.5446, 0.5191, 0.9668],
        [0.6418, 0.5047, 1.1018],
        [0.6259, 0.4999, 1.0556],
        [0.7352, 0.4137, 1.1986],
        [0.5746, 0.4756, 0.9600],
        [0.7057, 0.4405, 1.1224],
        [0.6400, 0.4727, 1.0520],
        [0.5723, 0.5204, 0.9918],
        [0.5060, 0.5029, 0.9057],
        [0.6609, 0.4179, 1.1470],
        [0.6325, 0.5023, 1.0624],
        [0.7008, 0.3818, 1.1943],
        [0.6790, 0.4452, 1.1540],
        [0.5747, 0.4996, 1.0246],
        [0.7077, 0.4424, 1.1521],
        [0.5447, 0.5196, 0.9672],
        [0.6418, 0.5047, 1.1018],
        [0.6654, 0.4272, 1.1452],
        [0.6143, 0.5151, 1.0410],
        [0.5723, 0.5175, 0.9935],
        [0.6005, 0.5081, 1.0327],
        [0.6410, 0.5046, 1.0999],
        [0.6872, 0.4646, 1.1054],
        [0.6496, 0.4804, 1.0708],
        [0.5717, 0.4999, 0.9664],
        [0.7027, 0.4601, 1.1563],
        [0.5504, 0.5248, 0.9732],
        [0.5903, 0.4726, 0.9832],
        [0.4825, 0.5128, 0.8875],
        [0.6959, 0.4678, 1.1119],
        [0.5518, 0.5153, 0.9713],
        [0.6188, 0.5169, 1.0376],
        [0.6306, 0.4765, 1.0431],
        [0.6654, 0.4272, 1.1452],
        [0.4825, 0.5123, 0.8869],
        [0.5112, 0.5092, 0.9183],
        [0.5620, 0.4681, 0.9528],
        [0.5491, 0.4980, 0.9406],
        [0.5774, 0.5012, 1.0304],
        [0.7464, 0.3961, 1.2147],
        [0.5994, 0.4957, 1.0320],
        [0.6814, 0.4233, 1.1535],
        [0.6759, 0.5089, 1.1313],
        [0.6734, 0.5072, 1.1310],
        [0.7014, 0.4588, 1.1437],
        [0.7003, 0.4610, 1.1276],
        [0.6048, 0.5110, 1.0388],
        [0.6167, 0.5202, 1.0365],
        [0.5730, 0.4998, 0.9807],
        [0.5756, 0.5191, 1.0198],
        [0.5097, 0.5154, 0.9333],
        [0.5518, 0.5153, 0.9713],
        [0.6259, 0.4999, 1.0556],
        [0.5895, 0.4725, 0.9825],
        [0.6757, 0.4945, 1.1151],
        [0.5318, 0.5132, 0.9446],
        [0.6346, 0.4751, 1.0473],
        [0.5116, 0.5103, 0.9175],
        [0.6515, 0.4809, 1.1092],
        [0.5598, 0.5121, 0.9708],
        [0.5420, 0.5222, 0.9433],
        [0.5442, 0.5180, 0.9659],
        [0.5214, 0.5079, 0.9170],
        [0.6295, 0.4886, 1.0389],
        [0.5954, 0.4982, 1.0224],
        [0.6673, 0.4246, 1.1467],
        [0.5785, 0.5183, 0.9730],
        [0.5451, 0.5184, 0.9461],
        [0.4958, 0.5174, 0.8965],
        [0.5964, 0.5086, 1.0298],
        [0.5354, 0.5149, 0.9450],
        [0.5509, 0.5248, 0.9742],
        [0.4909, 0.5134, 0.8978],
        [0.5057, 0.5027, 0.9053],
        [0.8143, 0.3915, 1.2524],
        [0.6337, 0.4849, 1.0655],
        [0.6286, 0.4768, 1.0410],
        [0.5118, 0.5159, 0.9346],
        [0.7767, 0.3610, 1.2222],
        [0.5410, 0.5412, 0.9525],
        [0.6710, 0.4643, 1.0979],
        [0.5393, 0.5093, 0.9347],
        [0.6260, 0.4998, 1.0553],
        [0.5347, 0.5034, 0.9334],
        [0.5254, 0.5363, 0.9436],
        [0.5263, 0.5360, 0.9448],
        [0.4982, 0.5185, 0.8986],
        [0.6286, 0.4805, 1.0810],
        [0.7017, 0.4592, 1.1431],
        [0.7913, 0.3256, 1.2717],
        [0.5509, 0.5248, 0.9742],
        [0.6396, 0.5011, 1.0705],
        [0.7182, 0.4018, 1.1924],
        [0.6655, 0.4248, 1.1458],
        [0.5657, 0.4865, 0.9743],
        [0.5741, 0.5214, 1.0051],
        [0.6939, 0.4670, 1.1143],
        [0.7017, 0.3792, 1.1981],
        [0.5380, 0.5064, 0.9387],
        [0.5909, 0.4728, 0.9834],
        [0.5447, 0.5196, 0.9672],
        [0.4825, 0.5128, 0.8875],
        [0.6337, 0.4902, 1.0710],
        [0.7278, 0.4105, 1.1734],
        [0.7078, 0.4403, 1.1539],
        [0.6004, 0.5142, 1.0153],
        [0.6865, 0.4668, 1.1039],
        [0.5310, 0.5030, 0.9333],
        [0.5774, 0.5015, 1.0308],
        [0.6733, 0.4737, 1.1182],
        [0.5954, 0.4951, 1.0266],
        [0.5352, 0.5138, 0.9309],
        [0.7543, 0.3861, 1.2353],
        [0.5707, 0.5226, 0.9970],
        [0.5420, 0.5222, 0.9433],
        [0.6855, 0.4673, 1.1033],
        [0.4910, 0.5124, 0.8970],
        [0.6156, 0.5184, 1.0333],
        [0.6557, 0.4797, 1.0875],
        [0.7339, 0.4048, 1.1797],
        [0.6383, 0.4734, 1.0506],
        [0.6747, 0.4922, 1.1047],
        [0.7085, 0.4403, 1.1551],
        [0.8131, 0.3904, 1.2520],
        [0.7077, 0.4424, 1.1521],
        [0.7014, 0.4588, 1.1437],
        [0.5756, 0.5191, 1.0198],
        [0.5751, 0.5001, 1.0263],
        [0.7169, 0.3970, 1.1926],
        [0.6790, 0.4452, 1.1540],
        [0.5118, 0.5159, 0.9346],
        [0.5598, 0.5121, 0.9708],
        [0.6515, 0.4809, 1.1092],
        [0.5319, 0.5263, 0.9330],
        [0.7533, 0.3892, 1.2320],
        [0.6761, 0.4485, 1.1493],
        [0.5347, 0.5138, 0.9305],
        [0.6877, 0.4634, 1.1000],
        [0.5705, 0.4990, 0.9782],
        [0.6343, 0.4902, 1.0724],
        [0.6580, 0.4784, 1.0902],
        [0.7748, 0.3475, 1.2341],
        [0.5362, 0.5135, 0.9315],
        [0.5979, 0.5085, 1.0310],
        [0.5554, 0.4971, 0.9933],
        [0.8140, 0.3913, 1.2523],
        [0.5743, 0.4751, 0.9597],
        [0.6400, 0.5325, 1.0485],
        [0.6000, 0.5143, 1.0148],
        [0.5122, 0.5158, 0.9350],
        [0.5592, 0.5016, 0.9941],
        [0.6408, 0.4749, 1.1030],
        [0.4968, 0.5188, 0.8984],
        [0.5322, 0.5274, 0.9338],
        [0.5224, 0.5120, 0.9208],
        [0.5793, 0.5051, 1.0346],
        [0.5380, 0.5214, 0.9425],
        [0.7010, 0.3807, 1.1960],
        [0.5774, 0.5012, 1.0304],
        [0.5380, 0.5214, 0.9425],
        [0.5444, 0.5186, 0.9663]], device='mps:0')
2025-03-01 01:40:21 - INFO - Y batch argmax : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:22 - INFO - Outputs : tensor([[0.6180, 0.5200, 1.0386],
        [0.7758, 0.3608, 1.2232],
        [0.7017, 0.3784, 1.1987],
        [0.5216, 0.5006, 0.9551],
        [0.5207, 0.5048, 0.9193],
        [0.5703, 0.4753, 0.9589],
        [0.5439, 0.5019, 0.9558],
        [0.5721, 0.5000, 0.9666],
        [0.7477, 0.3946, 1.2154],
        [0.7762, 0.3605, 1.2231],
        [0.4988, 0.5183, 0.8986],
        [0.6337, 0.4849, 1.0655],
        [0.5120, 0.5158, 0.9349],
        [0.7765, 0.3618, 1.2210],
        [0.5118, 0.5159, 0.9346],
        [0.6700, 0.4653, 1.0959],
        [0.6317, 0.4891, 1.0580],
        [0.5490, 0.4977, 0.9403],
        [0.6496, 0.4924, 1.1000],
        [0.6658, 0.4076, 1.1533],
        [0.8340, 0.3174, 1.2795],
        [0.5368, 0.5152, 0.9856],
        [0.5376, 0.5384, 0.9507],
        [0.6170, 0.5178, 1.0354],
        [0.6540, 0.4679, 1.0895],
        [0.7028, 0.4398, 1.1212],
        [0.6352, 0.4847, 1.0674],
        [0.6455, 0.4705, 1.0569],
        [0.5373, 0.5384, 0.9498],
        [0.6404, 0.5045, 1.0981],
        [0.6472, 0.4838, 1.0586],
        [0.6473, 0.4647, 1.1180],
        [0.7530, 0.3894, 1.2305],
        [0.5120, 0.5158, 0.9349],
        [0.6504, 0.4807, 1.1091],
        [0.5770, 0.5005, 1.0298],
        [0.7085, 0.4403, 1.1551],
        [0.6478, 0.4835, 1.0715],
        [0.5244, 0.5128, 0.9681],
        [0.6673, 0.4246, 1.1467],
        [0.5009, 0.5132, 0.9015],
        [0.6491, 0.4921, 1.0998],
        [0.6014, 0.4961, 1.0357],
        [0.6884, 0.4634, 1.1152],
        [0.5491, 0.4976, 0.9401],
        [0.7734, 0.3496, 1.2331],
        [0.5397, 0.5092, 0.9344],
        [0.5004, 0.5147, 0.9072],
        [0.6200, 0.5195, 1.0421],
        [0.6404, 0.5045, 1.0981],
        [0.6264, 0.4771, 1.0389],
        [0.6325, 0.5023, 1.0624],
        [0.6328, 0.4760, 1.0454],
        [0.6352, 0.4847, 1.0674],
        [0.8453, 0.2825, 1.3216],
        [0.5212, 0.5085, 0.9172],
        [0.6870, 0.4654, 1.1049],
        [0.5314, 0.5261, 0.9332],
        [0.5225, 0.5111, 0.9193],
        [0.5491, 0.4980, 0.9406],
        [0.6229, 0.5188, 1.0460],
        [0.5800, 0.5052, 1.0352],
        [0.6473, 0.4647, 1.1180],
        [0.6047, 0.5163, 1.0192],
        [0.6024, 0.5062, 1.0354],
        [0.6306, 0.4765, 1.0431],
        [0.5021, 0.5123, 0.9027],
        [0.4982, 0.5185, 0.8986],
        [0.6156, 0.5184, 1.0333],
        [0.5343, 0.5142, 0.9454],
        [0.5586, 0.5091, 0.9952],
        [0.6022, 0.5069, 1.0348],
        [0.7528, 0.3893, 1.2312],
        [0.6761, 0.4485, 1.1493],
        [0.4932, 0.5104, 0.8949],
        [0.6270, 0.4784, 1.0915],
        [0.6286, 0.4805, 1.0810],
        [0.5707, 0.5226, 0.9970],
        [0.5657, 0.4865, 0.9743],
        [0.6400, 0.5325, 1.0485],
        [0.6884, 0.4634, 1.1152],
        [0.7530, 0.3894, 1.2305],
        [0.5272, 0.5110, 0.9345],
        [0.5741, 0.5214, 1.0051],
        [0.6393, 0.5012, 1.0692],
        [0.5773, 0.5188, 1.0041],
        [0.6286, 0.4805, 1.0810],
        [0.5227, 0.5009, 0.9565],
        [0.6335, 0.5006, 1.0767],
        [0.6195, 0.5338, 1.0443],
        [0.6761, 0.4485, 1.1493],
        [0.6200, 0.5195, 1.0421],
        [0.6419, 0.4720, 1.0536],
        [0.5490, 0.4977, 0.9403],
        [0.6130, 0.5200, 1.0291],
        [0.5959, 0.4952, 1.0275],
        [0.5755, 0.4879, 0.9949],
        [0.8143, 0.3915, 1.2524],
        [0.6733, 0.4627, 1.1020],
        [0.5586, 0.5207, 0.9788],
        [0.7913, 0.3209, 1.2736],
        [0.6249, 0.4815, 1.0871],
        [0.6877, 0.4634, 1.1000],
        [0.6376, 0.5015, 1.0670],
        [0.6393, 0.5012, 1.0692],
        [0.7074, 0.4401, 1.1539],
        [0.6745, 0.5077, 1.1313],
        [0.6496, 0.4820, 1.0716],
        [0.5398, 0.5092, 0.9342],
        [0.6214, 0.4690, 1.0405],
        [0.5298, 0.5086, 0.9312],
        [0.8131, 0.3885, 1.2527],
        [0.5307, 0.5026, 0.9326],
        [0.5864, 0.5137, 0.9987],
        [0.5647, 0.4769, 0.9582],
        [0.5770, 0.4940, 1.0238],
        [0.5272, 0.5110, 0.9345],
        [0.5451, 0.5184, 0.9461],
        [0.6188, 0.5169, 1.0376],
        [0.5490, 0.4977, 0.9403],
        [0.6622, 0.4161, 1.1484],
        [0.5293, 0.5087, 0.9312],
        [0.7689, 0.3591, 1.2363],
        [0.6400, 0.4727, 1.0520],
        [0.6306, 0.4765, 1.0431],
        [0.5377, 0.5385, 0.9515],
        [0.5439, 0.5019, 0.9558],
        [0.5846, 0.5131, 0.9967],
        [0.7262, 0.4125, 1.1717],
        [0.5097, 0.5154, 0.9333],
        [0.5741, 0.5214, 1.0051],
        [0.5619, 0.5048, 0.9628],
        [0.5724, 0.5002, 0.9667],
        [0.5777, 0.5023, 1.0314],
        [0.6201, 0.5339, 1.0452],
        [0.5439, 0.5019, 0.9558],
        [0.6188, 0.5119, 1.0491],
        [0.6345, 0.4999, 1.0777],
        [0.6004, 0.5142, 1.0153],
        [0.5330, 0.5134, 0.9450],
        [0.7039, 0.4413, 1.1495],
        [0.5349, 0.5016, 0.9331],
        [0.4912, 0.5119, 0.8966],
        [0.6214, 0.5191, 1.0441],
        [0.7214, 0.4327, 1.1791],
        [0.5216, 0.5005, 0.9552],
        [0.6639, 0.4114, 1.1507],
        [0.7018, 0.4413, 1.1473],
        [0.5354, 0.5149, 0.9450],
        [0.6647, 0.4098, 1.1516],
        [0.7514, 0.3564, 1.2320],
        [0.6105, 0.5110, 1.0455],
        [0.5504, 0.5248, 0.9732],
        [0.5386, 0.5152, 0.9868],
        [0.7527, 0.3556, 1.2343],
        [0.5106, 0.5155, 0.9337],
        [0.6862, 0.4644, 1.0981],
        [0.7544, 0.3880, 1.2331],
        [0.6480, 0.5391, 1.0570],
        [0.6639, 0.4114, 1.1507],
        [0.4987, 0.5066, 0.9008],
        [0.5207, 0.5048, 0.9193],
        [0.5509, 0.5248, 0.9742],
        [0.5610, 0.5125, 0.9724],
        [0.6418, 0.5047, 1.1018],
        [0.6790, 0.4452, 1.1540],
        [0.8128, 0.3895, 1.2519],
        [0.6005, 0.5081, 1.0327],
        [0.5723, 0.5204, 0.9918],
        [0.5895, 0.4725, 0.9825],
        [0.4967, 0.5187, 0.8983],
        [0.5703, 0.4753, 0.9589],
        [0.8081, 0.3574, 1.2713],
        [0.5518, 0.5153, 0.9713],
        [0.5345, 0.5316, 0.9330],
        [0.6306, 0.4896, 1.0390],
        [0.5021, 0.5123, 0.9027],
        [0.5387, 0.5247, 0.9432],
        [0.5097, 0.5154, 0.9333],
        [0.5330, 0.5134, 0.9450],
        [0.6192, 0.5116, 1.0511],
        [0.5354, 0.5014, 0.9333],
        [0.5751, 0.5001, 1.0263],
        [0.7018, 0.4413, 1.1473],
        [0.6229, 0.4830, 1.0827],
        [0.4824, 0.5125, 0.8871],
        [0.5678, 0.5215, 1.0155],
        [0.7003, 0.4610, 1.1276],
        [0.6188, 0.5114, 1.0519],
        [0.6720, 0.4537, 1.1240],
        [0.6471, 0.4734, 1.0783],
        [0.6878, 0.4638, 1.1151],
        [0.5390, 0.5051, 0.9368],
        [0.7042, 0.4066, 1.1656],
        [0.5212, 0.5007, 0.9547],
        [0.5347, 0.5138, 0.9305],
        [0.5656, 0.4770, 0.9584],
        [0.6434, 0.4712, 1.0546],
        [0.5895, 0.4725, 0.9825],
        [0.6383, 0.4734, 1.0506],
        [0.5724, 0.5002, 0.9667],
        [0.6307, 0.4936, 1.0584],
        [0.5864, 0.5137, 0.9987],
        [0.6184, 0.5125, 1.0475],
        [0.8327, 0.3217, 1.2776],
        [0.7669, 0.3596, 1.2353],
        [0.6249, 0.4815, 1.0871],
        [0.6724, 0.4535, 1.1238],
        [0.6703, 0.4934, 1.1111],
        [0.5755, 0.4879, 0.9949],
        [0.7542, 0.3864, 1.2347],
        [0.7514, 0.3564, 1.2320],
        [0.6404, 0.4875, 1.0514],
        [0.6000, 0.4962, 1.0335],
        [0.6473, 0.4647, 1.1180],
        [0.7214, 0.4033, 1.1937],
        [0.5393, 0.5093, 0.9347],
        [0.5341, 0.5309, 0.9333],
        [0.5636, 0.4986, 1.0012],
        [0.6404, 0.5045, 1.0981],
        [0.6814, 0.4233, 1.1535],
        [0.7010, 0.3812, 1.1951],
        [0.6422, 0.5049, 1.1026],
        [0.6998, 0.4404, 1.1197],
        [0.7745, 0.3467, 1.2341],
        [0.6497, 0.4799, 1.0707],
        [0.8426, 0.2834, 1.3188],
        [0.6562, 0.4790, 1.0883],
        [0.6582, 0.4782, 1.0910],
        [0.6656, 0.4266, 1.1451],
        [0.5225, 0.5111, 0.9193],
        [0.6418, 0.4872, 1.0527],
        [0.7776, 0.3593, 1.2251],
        [0.6865, 0.4664, 1.1048],
        [0.7035, 0.4091, 1.1636],
        [0.5207, 0.5004, 0.9534],
        [0.5221, 0.5073, 0.9164],
        [0.6673, 0.4246, 1.1467],
        [0.6865, 0.4668, 1.1039],
        [0.6562, 0.4790, 1.0883],
        [0.5626, 0.4683, 0.9531],
        [0.5490, 0.4977, 0.9403],
        [0.7498, 0.3570, 1.2297],
        [0.6214, 0.5191, 1.0441],
        [0.6270, 0.4784, 1.0915],
        [0.7214, 0.4327, 1.1791],
        [0.6589, 0.4210, 1.1449],
        [0.7381, 0.4094, 1.2027],
        [0.7012, 0.3800, 1.1971],
        [0.5586, 0.5038, 0.9929],
        [0.6286, 0.4768, 1.0410],
        [0.6312, 0.4892, 1.0587],
        [0.5301, 0.5086, 0.9313],
        [0.7262, 0.4125, 1.1717],
        [0.5726, 0.5215, 1.0015],
        [0.5743, 0.4751, 0.9597]], device='mps:0')
2025-03-01 01:40:22 - INFO - Outputs shape : torch.Size([256, 3])
2025-03-01 01:40:23 - INFO - Y batch : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:40:23 - INFO - Y batch shape : torch.Size([256, 3])
2025-03-01 01:40:23 - INFO - Outputs argmax : tensor([[0.6180, 0.5200, 1.0386],
        [0.7758, 0.3608, 1.2232],
        [0.7017, 0.3784, 1.1987],
        [0.5216, 0.5006, 0.9551],
        [0.5207, 0.5048, 0.9193],
        [0.5703, 0.4753, 0.9589],
        [0.5439, 0.5019, 0.9558],
        [0.5721, 0.5000, 0.9666],
        [0.7477, 0.3946, 1.2154],
        [0.7762, 0.3605, 1.2231],
        [0.4988, 0.5183, 0.8986],
        [0.6337, 0.4849, 1.0655],
        [0.5120, 0.5158, 0.9349],
        [0.7765, 0.3618, 1.2210],
        [0.5118, 0.5159, 0.9346],
        [0.6700, 0.4653, 1.0959],
        [0.6317, 0.4891, 1.0580],
        [0.5490, 0.4977, 0.9403],
        [0.6496, 0.4924, 1.1000],
        [0.6658, 0.4076, 1.1533],
        [0.8340, 0.3174, 1.2795],
        [0.5368, 0.5152, 0.9856],
        [0.5376, 0.5384, 0.9507],
        [0.6170, 0.5178, 1.0354],
        [0.6540, 0.4679, 1.0895],
        [0.7028, 0.4398, 1.1212],
        [0.6352, 0.4847, 1.0674],
        [0.6455, 0.4705, 1.0569],
        [0.5373, 0.5384, 0.9498],
        [0.6404, 0.5045, 1.0981],
        [0.6472, 0.4838, 1.0586],
        [0.6473, 0.4647, 1.1180],
        [0.7530, 0.3894, 1.2305],
        [0.5120, 0.5158, 0.9349],
        [0.6504, 0.4807, 1.1091],
        [0.5770, 0.5005, 1.0298],
        [0.7085, 0.4403, 1.1551],
        [0.6478, 0.4835, 1.0715],
        [0.5244, 0.5128, 0.9681],
        [0.6673, 0.4246, 1.1467],
        [0.5009, 0.5132, 0.9015],
        [0.6491, 0.4921, 1.0998],
        [0.6014, 0.4961, 1.0357],
        [0.6884, 0.4634, 1.1152],
        [0.5491, 0.4976, 0.9401],
        [0.7734, 0.3496, 1.2331],
        [0.5397, 0.5092, 0.9344],
        [0.5004, 0.5147, 0.9072],
        [0.6200, 0.5195, 1.0421],
        [0.6404, 0.5045, 1.0981],
        [0.6264, 0.4771, 1.0389],
        [0.6325, 0.5023, 1.0624],
        [0.6328, 0.4760, 1.0454],
        [0.6352, 0.4847, 1.0674],
        [0.8453, 0.2825, 1.3216],
        [0.5212, 0.5085, 0.9172],
        [0.6870, 0.4654, 1.1049],
        [0.5314, 0.5261, 0.9332],
        [0.5225, 0.5111, 0.9193],
        [0.5491, 0.4980, 0.9406],
        [0.6229, 0.5188, 1.0460],
        [0.5800, 0.5052, 1.0352],
        [0.6473, 0.4647, 1.1180],
        [0.6047, 0.5163, 1.0192],
        [0.6024, 0.5062, 1.0354],
        [0.6306, 0.4765, 1.0431],
        [0.5021, 0.5123, 0.9027],
        [0.4982, 0.5185, 0.8986],
        [0.6156, 0.5184, 1.0333],
        [0.5343, 0.5142, 0.9454],
        [0.5586, 0.5091, 0.9952],
        [0.6022, 0.5069, 1.0348],
        [0.7528, 0.3893, 1.2312],
        [0.6761, 0.4485, 1.1493],
        [0.4932, 0.5104, 0.8949],
        [0.6270, 0.4784, 1.0915],
        [0.6286, 0.4805, 1.0810],
        [0.5707, 0.5226, 0.9970],
        [0.5657, 0.4865, 0.9743],
        [0.6400, 0.5325, 1.0485],
        [0.6884, 0.4634, 1.1152],
        [0.7530, 0.3894, 1.2305],
        [0.5272, 0.5110, 0.9345],
        [0.5741, 0.5214, 1.0051],
        [0.6393, 0.5012, 1.0692],
        [0.5773, 0.5188, 1.0041],
        [0.6286, 0.4805, 1.0810],
        [0.5227, 0.5009, 0.9565],
        [0.6335, 0.5006, 1.0767],
        [0.6195, 0.5338, 1.0443],
        [0.6761, 0.4485, 1.1493],
        [0.6200, 0.5195, 1.0421],
        [0.6419, 0.4720, 1.0536],
        [0.5490, 0.4977, 0.9403],
        [0.6130, 0.5200, 1.0291],
        [0.5959, 0.4952, 1.0275],
        [0.5755, 0.4879, 0.9949],
        [0.8143, 0.3915, 1.2524],
        [0.6733, 0.4627, 1.1020],
        [0.5586, 0.5207, 0.9788],
        [0.7913, 0.3209, 1.2736],
        [0.6249, 0.4815, 1.0871],
        [0.6877, 0.4634, 1.1000],
        [0.6376, 0.5015, 1.0670],
        [0.6393, 0.5012, 1.0692],
        [0.7074, 0.4401, 1.1539],
        [0.6745, 0.5077, 1.1313],
        [0.6496, 0.4820, 1.0716],
        [0.5398, 0.5092, 0.9342],
        [0.6214, 0.4690, 1.0405],
        [0.5298, 0.5086, 0.9312],
        [0.8131, 0.3885, 1.2527],
        [0.5307, 0.5026, 0.9326],
        [0.5864, 0.5137, 0.9987],
        [0.5647, 0.4769, 0.9582],
        [0.5770, 0.4940, 1.0238],
        [0.5272, 0.5110, 0.9345],
        [0.5451, 0.5184, 0.9461],
        [0.6188, 0.5169, 1.0376],
        [0.5490, 0.4977, 0.9403],
        [0.6622, 0.4161, 1.1484],
        [0.5293, 0.5087, 0.9312],
        [0.7689, 0.3591, 1.2363],
        [0.6400, 0.4727, 1.0520],
        [0.6306, 0.4765, 1.0431],
        [0.5377, 0.5385, 0.9515],
        [0.5439, 0.5019, 0.9558],
        [0.5846, 0.5131, 0.9967],
        [0.7262, 0.4125, 1.1717],
        [0.5097, 0.5154, 0.9333],
        [0.5741, 0.5214, 1.0051],
        [0.5619, 0.5048, 0.9628],
        [0.5724, 0.5002, 0.9667],
        [0.5777, 0.5023, 1.0314],
        [0.6201, 0.5339, 1.0452],
        [0.5439, 0.5019, 0.9558],
        [0.6188, 0.5119, 1.0491],
        [0.6345, 0.4999, 1.0777],
        [0.6004, 0.5142, 1.0153],
        [0.5330, 0.5134, 0.9450],
        [0.7039, 0.4413, 1.1495],
        [0.5349, 0.5016, 0.9331],
        [0.4912, 0.5119, 0.8966],
        [0.6214, 0.5191, 1.0441],
        [0.7214, 0.4327, 1.1791],
        [0.5216, 0.5005, 0.9552],
        [0.6639, 0.4114, 1.1507],
        [0.7018, 0.4413, 1.1473],
        [0.5354, 0.5149, 0.9450],
        [0.6647, 0.4098, 1.1516],
        [0.7514, 0.3564, 1.2320],
        [0.6105, 0.5110, 1.0455],
        [0.5504, 0.5248, 0.9732],
        [0.5386, 0.5152, 0.9868],
        [0.7527, 0.3556, 1.2343],
        [0.5106, 0.5155, 0.9337],
        [0.6862, 0.4644, 1.0981],
        [0.7544, 0.3880, 1.2331],
        [0.6480, 0.5391, 1.0570],
        [0.6639, 0.4114, 1.1507],
        [0.4987, 0.5066, 0.9008],
        [0.5207, 0.5048, 0.9193],
        [0.5509, 0.5248, 0.9742],
        [0.5610, 0.5125, 0.9724],
        [0.6418, 0.5047, 1.1018],
        [0.6790, 0.4452, 1.1540],
        [0.8128, 0.3895, 1.2519],
        [0.6005, 0.5081, 1.0327],
        [0.5723, 0.5204, 0.9918],
        [0.5895, 0.4725, 0.9825],
        [0.4967, 0.5187, 0.8983],
        [0.5703, 0.4753, 0.9589],
        [0.8081, 0.3574, 1.2713],
        [0.5518, 0.5153, 0.9713],
        [0.5345, 0.5316, 0.9330],
        [0.6306, 0.4896, 1.0390],
        [0.5021, 0.5123, 0.9027],
        [0.5387, 0.5247, 0.9432],
        [0.5097, 0.5154, 0.9333],
        [0.5330, 0.5134, 0.9450],
        [0.6192, 0.5116, 1.0511],
        [0.5354, 0.5014, 0.9333],
        [0.5751, 0.5001, 1.0263],
        [0.7018, 0.4413, 1.1473],
        [0.6229, 0.4830, 1.0827],
        [0.4824, 0.5125, 0.8871],
        [0.5678, 0.5215, 1.0155],
        [0.7003, 0.4610, 1.1276],
        [0.6188, 0.5114, 1.0519],
        [0.6720, 0.4537, 1.1240],
        [0.6471, 0.4734, 1.0783],
        [0.6878, 0.4638, 1.1151],
        [0.5390, 0.5051, 0.9368],
        [0.7042, 0.4066, 1.1656],
        [0.5212, 0.5007, 0.9547],
        [0.5347, 0.5138, 0.9305],
        [0.5656, 0.4770, 0.9584],
        [0.6434, 0.4712, 1.0546],
        [0.5895, 0.4725, 0.9825],
        [0.6383, 0.4734, 1.0506],
        [0.5724, 0.5002, 0.9667],
        [0.6307, 0.4936, 1.0584],
        [0.5864, 0.5137, 0.9987],
        [0.6184, 0.5125, 1.0475],
        [0.8327, 0.3217, 1.2776],
        [0.7669, 0.3596, 1.2353],
        [0.6249, 0.4815, 1.0871],
        [0.6724, 0.4535, 1.1238],
        [0.6703, 0.4934, 1.1111],
        [0.5755, 0.4879, 0.9949],
        [0.7542, 0.3864, 1.2347],
        [0.7514, 0.3564, 1.2320],
        [0.6404, 0.4875, 1.0514],
        [0.6000, 0.4962, 1.0335],
        [0.6473, 0.4647, 1.1180],
        [0.7214, 0.4033, 1.1937],
        [0.5393, 0.5093, 0.9347],
        [0.5341, 0.5309, 0.9333],
        [0.5636, 0.4986, 1.0012],
        [0.6404, 0.5045, 1.0981],
        [0.6814, 0.4233, 1.1535],
        [0.7010, 0.3812, 1.1951],
        [0.6422, 0.5049, 1.1026],
        [0.6998, 0.4404, 1.1197],
        [0.7745, 0.3467, 1.2341],
        [0.6497, 0.4799, 1.0707],
        [0.8426, 0.2834, 1.3188],
        [0.6562, 0.4790, 1.0883],
        [0.6582, 0.4782, 1.0910],
        [0.6656, 0.4266, 1.1451],
        [0.5225, 0.5111, 0.9193],
        [0.6418, 0.4872, 1.0527],
        [0.7776, 0.3593, 1.2251],
        [0.6865, 0.4664, 1.1048],
        [0.7035, 0.4091, 1.1636],
        [0.5207, 0.5004, 0.9534],
        [0.5221, 0.5073, 0.9164],
        [0.6673, 0.4246, 1.1467],
        [0.6865, 0.4668, 1.1039],
        [0.6562, 0.4790, 1.0883],
        [0.5626, 0.4683, 0.9531],
        [0.5490, 0.4977, 0.9403],
        [0.7498, 0.3570, 1.2297],
        [0.6214, 0.5191, 1.0441],
        [0.6270, 0.4784, 1.0915],
        [0.7214, 0.4327, 1.1791],
        [0.6589, 0.4210, 1.1449],
        [0.7381, 0.4094, 1.2027],
        [0.7012, 0.3800, 1.1971],
        [0.5586, 0.5038, 0.9929],
        [0.6286, 0.4768, 1.0410],
        [0.6312, 0.4892, 1.0587],
        [0.5301, 0.5086, 0.9313],
        [0.7262, 0.4125, 1.1717],
        [0.5726, 0.5215, 1.0015],
        [0.5743, 0.4751, 0.9597]], device='mps:0')
2025-03-01 01:40:23 - INFO - Y batch argmax : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:41:25 - INFO - Size of validation loader : 19
2025-03-01 01:41:26 - INFO - Outputs : tensor([[0.6192, 0.5198, 1.0408],
        [0.6751, 0.4918, 1.1046],
        [0.7381, 0.4094, 1.2027],
        [0.5212, 0.5085, 0.9172],
        [0.4967, 0.5187, 0.8983],
        [0.5753, 0.5215, 1.0070],
        [0.6270, 0.4784, 1.0915],
        [0.6724, 0.4634, 1.1001],
        [0.5427, 0.5176, 0.9651],
        [0.6950, 0.4675, 1.1123],
        [0.6396, 0.5011, 1.0705],
        [0.6890, 0.4632, 1.1058],
        [0.5647, 0.4769, 0.9582],
        [0.7429, 0.4007, 1.2111],
        [0.6143, 0.5192, 1.0312],
        [0.5337, 0.5300, 0.9333],
        [0.6023, 0.5118, 1.0365],
        [0.6851, 0.4547, 1.1404],
        [0.5903, 0.4726, 0.9832],
        [0.5119, 0.5159, 0.9348],
        [0.5353, 0.5030, 0.9338],
        [0.5558, 0.4813, 0.9449],
        [0.5255, 0.5114, 0.9401],
        [0.4986, 0.5065, 0.9006],
        [0.5759, 0.4883, 0.9952],
        [0.6859, 0.4582, 1.1249],
        [0.6404, 0.5045, 1.0981],
        [0.6300, 0.4891, 1.0387],
        [0.4910, 0.5124, 0.8970],
        [0.5003, 0.5180, 0.8985],
        [0.5420, 0.5412, 0.9533],
        [0.7746, 0.3244, 1.2614],
        [0.6307, 0.4903, 1.0387],
        [0.6398, 0.5044, 1.0975],
        [0.6557, 0.4797, 1.0875],
        [0.4987, 0.5066, 0.9008],
        [0.6724, 0.4535, 1.1238],
        [0.7172, 0.3994, 1.1922],
        [0.5216, 0.5006, 0.9551],
        [0.5110, 0.5091, 0.9189],
        [0.6491, 0.4921, 1.0998],
        [0.5753, 0.5215, 1.0070],
        [0.5496, 0.5245, 0.9718],
        [0.6104, 0.5111, 1.0455],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6593, 0.4781, 1.0921],
        [0.5358, 0.5059, 0.9385],
        [0.5130, 0.5035, 0.9084],
        [0.4958, 0.5174, 0.8965],
        [0.8340, 0.3174, 1.2795],
        [0.6214, 0.4690, 1.0405],
        [0.6777, 0.4950, 1.1162],
        [0.5009, 0.5132, 0.9015],
        [0.7748, 0.3475, 1.2341],
        [0.6857, 0.4582, 1.1246],
        [0.8143, 0.3915, 1.2524],
        [0.6562, 0.4790, 1.0883],
        [0.6733, 0.4627, 1.1020],
        [0.5060, 0.5029, 0.9057],
        [0.7058, 0.4417, 1.1507],
        [0.6746, 0.4926, 1.1040],
        [0.5650, 0.4688, 0.9542],
        [0.6492, 0.4729, 1.0799],
        [0.6061, 0.5095, 1.0402],
        [0.5387, 0.5050, 0.9366],
        [0.7912, 0.3203, 1.2739],
        [0.5610, 0.5125, 0.9724],
        [0.6366, 0.4742, 1.0491],
        [0.5272, 0.5110, 0.9345],
        [0.7027, 0.4601, 1.1563],
        [0.7352, 0.4137, 1.1986],
        [0.6099, 0.5182, 1.0356],
        [0.7734, 0.3496, 1.2331],
        [0.4999, 0.5147, 0.9072],
        [0.7734, 0.3496, 1.2331],
        [0.5665, 0.4872, 0.9744],
        [0.6862, 0.4537, 1.1409],
        [0.7745, 0.3467, 1.2341],
        [0.7057, 0.4405, 1.1224],
        [0.6587, 0.4783, 1.0915],
        [0.7172, 0.3994, 1.1922],
        [0.6474, 0.5392, 1.0558],
        [0.5750, 0.4993, 1.0236],
        [0.4988, 0.5183, 0.8986],
        [0.6345, 0.4999, 1.0777],
        [0.5626, 0.4683, 0.9531],
        [0.6476, 0.4697, 1.0592],
        [0.5222, 0.5105, 0.9185],
        [0.7542, 0.3864, 1.2347],
        [0.5315, 0.5033, 0.9342],
        [0.6277, 0.4921, 1.0419],
        [0.8314, 0.3229, 1.2769],
        [0.6677, 0.4244, 1.1465],
        [0.5118, 0.5159, 0.9346],
        [0.5687, 0.5231, 0.9644],
        [0.5221, 0.5073, 0.9164],
        [0.6865, 0.4664, 1.1048],
        [0.4906, 0.5129, 0.8975],
        [0.5319, 0.5263, 0.9330],
        [0.6870, 0.4654, 1.1049],
        [0.5322, 0.5274, 0.9338],
        [0.5212, 0.5085, 0.9172],
        [0.7783, 0.3576, 1.2273],
        [0.7196, 0.3959, 1.1841],
        [0.5444, 0.5186, 0.9663],
        [0.6557, 0.4797, 1.0875],
        [0.5895, 0.4725, 0.9825],
        [0.5620, 0.4988, 0.9988],
        [0.4996, 0.5183, 0.8986],
        [0.5496, 0.5245, 0.9718],
        [0.6478, 0.4835, 1.0715],
        [0.6125, 0.5212, 1.0318],
        [0.5909, 0.4728, 0.9834],
        [0.6024, 0.5062, 1.0354],
        [0.8143, 0.3915, 1.2524],
        [0.6496, 0.4924, 1.1000],
        [0.5665, 0.4872, 0.9744],
        [0.5380, 0.5064, 0.9387],
        [0.5372, 0.5036, 0.9349],
        [0.5586, 0.5207, 0.9788],
        [0.6174, 0.5134, 1.0458],
        [0.5769, 0.5190, 1.0033],
        [0.5310, 0.5030, 0.9333],
        [0.7548, 0.3867, 1.2347],
        [0.7708, 0.3262, 1.2567],
        [0.6751, 0.4918, 1.1046],
        [0.5343, 0.5142, 0.9454],
        [0.6170, 0.5178, 1.0354],
        [0.5750, 0.4993, 1.0236],
        [0.6005, 0.5081, 1.0327],
        [0.5586, 0.5091, 0.9952],
        [0.6564, 0.5133, 1.0577],
        [0.5749, 0.5198, 1.0185],
        [0.7757, 0.3607, 1.2232],
        [0.6497, 0.4802, 1.1098],
        [0.7014, 0.3785, 1.1984],
        [0.6195, 0.5338, 1.0443],
        [0.5754, 0.4879, 0.9945],
        [0.5721, 0.5000, 0.9666],
        [0.6859, 0.4582, 1.1249],
        [0.5301, 0.5108, 0.9440],
        [0.5428, 0.5025, 0.9557],
        [0.7206, 0.4028, 1.1934],
        [0.6776, 0.4984, 1.1252],
        [0.5665, 0.4872, 0.9744],
        [0.7543, 0.3861, 1.2353],
        [0.5315, 0.5033, 0.9342],
        [0.4825, 0.5128, 0.8875],
        [0.5266, 0.5112, 0.9405],
        [0.6851, 0.4547, 1.1404],
        [0.5398, 0.5092, 0.9342],
        [0.5368, 0.5152, 0.9856],
        [0.6188, 0.5169, 1.0376],
        [0.7085, 0.4403, 1.1551],
        [0.5647, 0.4769, 0.9582],
        [0.5972, 0.4953, 1.0292],
        [0.7464, 0.3961, 1.2147],
        [0.6864, 0.4669, 1.1037],
        [0.5650, 0.5185, 1.0117],
        [0.5322, 0.5274, 0.9338],
        [0.7477, 0.3946, 1.2154],
        [0.6735, 0.4516, 1.1262],
        [0.7543, 0.3861, 1.2353],
        [0.5946, 0.4948, 1.0251],
        [0.6792, 0.4950, 1.1175],
        [0.6726, 0.4529, 1.1246],
        [0.5089, 0.5158, 0.9333],
        [0.7562, 0.3540, 1.2381],
        [0.5785, 0.5177, 0.9727],
        [0.6748, 0.4621, 1.1048],
        [0.5995, 0.5083, 1.0321],
        [0.5620, 0.4988, 0.9988],
        [0.5954, 0.4951, 1.0266],
        [0.5349, 0.5016, 0.9331],
        [0.5212, 0.5085, 0.9172],
        [0.6384, 0.4933, 1.0797],
        [0.5387, 0.5050, 0.9366],
        [0.5838, 0.5038, 0.9884],
        [0.6396, 0.5013, 1.0701],
        [0.5293, 0.5087, 0.9312],
        [0.5341, 0.5153, 0.9472],
        [0.8131, 0.3904, 1.2520],
        [0.6859, 0.4582, 1.1249],
        [0.7058, 0.4417, 1.1507],
        [0.7738, 0.3490, 1.2336],
        [0.5373, 0.5384, 0.9498],
        [0.7757, 0.3607, 1.2232],
        [0.6690, 0.4932, 1.1100],
        [0.6491, 0.4921, 1.0998],
        [0.5216, 0.5005, 0.9552],
        [0.5361, 0.5152, 0.9848],
        [0.5838, 0.5038, 0.9884],
        [0.5117, 0.5101, 0.9174],
        [0.5647, 0.4769, 0.9582],
        [0.5903, 0.4726, 0.9832],
        [0.6473, 0.4647, 1.1180],
        [0.6306, 0.4765, 1.0431],
        [0.6408, 0.4749, 1.1030],
        [0.6399, 0.5331, 1.0489],
        [0.6874, 0.4638, 1.1052],
        [0.5387, 0.5050, 0.9366],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6751, 0.5081, 1.1314],
        [0.5383, 0.5049, 0.9577],
        [0.5707, 0.5226, 0.9970],
        [0.6303, 0.5025, 1.0602],
        [0.7545, 0.3860, 1.2353],
        [0.6950, 0.4675, 1.1123],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.6497, 0.4799, 1.0707],
        [0.8081, 0.3574, 1.2713],
        [0.8453, 0.2825, 1.3216],
        [0.5225, 0.5116, 0.9201],
        [0.7523, 0.3903, 1.2298],
        [0.6422, 0.5049, 1.1026],
        [0.5785, 0.5177, 0.9727],
        [0.6022, 0.5069, 1.0348],
        [0.6229, 0.4830, 1.0827],
        [0.6848, 0.4550, 1.1403],
        [0.6865, 0.4664, 1.1048],
        [0.5678, 0.5215, 1.0155],
        [0.6000, 0.4962, 1.0335],
        [0.5657, 0.4865, 0.9743],
        [0.5060, 0.5029, 0.9057],
        [0.5354, 0.5149, 0.9450],
        [0.6659, 0.4234, 1.1465],
        [0.5119, 0.5159, 0.9348],
        [0.5721, 0.4994, 0.9800],
        [0.5097, 0.5154, 0.9333],
        [0.6593, 0.4781, 1.0921],
        [0.7014, 0.3785, 1.1984],
        [0.7206, 0.4028, 1.1934],
        [0.4825, 0.5121, 0.8868],
        [0.5146, 0.5101, 0.9139],
        [0.6402, 0.5323, 1.0487],
        [0.4996, 0.5183, 0.8986],
        [0.6384, 0.4933, 1.0797],
        [0.7780, 0.3589, 1.2259],
        [0.6557, 0.4797, 1.0875],
        [0.5288, 0.5086, 0.9309],
        [0.5766, 0.5006, 1.0299],
        [0.5610, 0.4681, 0.9523],
        [0.6790, 0.4452, 1.1540],
        [0.6270, 0.4784, 1.0915],
        [0.6402, 0.5323, 1.0487],
        [0.6522, 0.4819, 1.1071],
        [0.5275, 0.5112, 0.9335],
        [0.8334, 0.3202, 1.2785],
        [0.7689, 0.3591, 1.2363],
        [0.5895, 0.4725, 0.9825],
        [0.6184, 0.5125, 1.0475],
        [0.8314, 0.3229, 1.2769],
        [0.6476, 0.4697, 1.0592]], device='mps:0')
2025-03-01 01:41:26 - INFO - Outputs shape : torch.Size([256, 3])
2025-03-01 01:41:26 - INFO - Y batch : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:41:26 - INFO - Y batch shape : torch.Size([256, 3])
2025-03-01 01:41:26 - INFO - Outputs argmax : tensor([[0.6192, 0.5198, 1.0408],
        [0.6751, 0.4918, 1.1046],
        [0.7381, 0.4094, 1.2027],
        [0.5212, 0.5085, 0.9172],
        [0.4967, 0.5187, 0.8983],
        [0.5753, 0.5215, 1.0070],
        [0.6270, 0.4784, 1.0915],
        [0.6724, 0.4634, 1.1001],
        [0.5427, 0.5176, 0.9651],
        [0.6950, 0.4675, 1.1123],
        [0.6396, 0.5011, 1.0705],
        [0.6890, 0.4632, 1.1058],
        [0.5647, 0.4769, 0.9582],
        [0.7429, 0.4007, 1.2111],
        [0.6143, 0.5192, 1.0312],
        [0.5337, 0.5300, 0.9333],
        [0.6023, 0.5118, 1.0365],
        [0.6851, 0.4547, 1.1404],
        [0.5903, 0.4726, 0.9832],
        [0.5119, 0.5159, 0.9348],
        [0.5353, 0.5030, 0.9338],
        [0.5558, 0.4813, 0.9449],
        [0.5255, 0.5114, 0.9401],
        [0.4986, 0.5065, 0.9006],
        [0.5759, 0.4883, 0.9952],
        [0.6859, 0.4582, 1.1249],
        [0.6404, 0.5045, 1.0981],
        [0.6300, 0.4891, 1.0387],
        [0.4910, 0.5124, 0.8970],
        [0.5003, 0.5180, 0.8985],
        [0.5420, 0.5412, 0.9533],
        [0.7746, 0.3244, 1.2614],
        [0.6307, 0.4903, 1.0387],
        [0.6398, 0.5044, 1.0975],
        [0.6557, 0.4797, 1.0875],
        [0.4987, 0.5066, 0.9008],
        [0.6724, 0.4535, 1.1238],
        [0.7172, 0.3994, 1.1922],
        [0.5216, 0.5006, 0.9551],
        [0.5110, 0.5091, 0.9189],
        [0.6491, 0.4921, 1.0998],
        [0.5753, 0.5215, 1.0070],
        [0.5496, 0.5245, 0.9718],
        [0.6104, 0.5111, 1.0455],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6593, 0.4781, 1.0921],
        [0.5358, 0.5059, 0.9385],
        [0.5130, 0.5035, 0.9084],
        [0.4958, 0.5174, 0.8965],
        [0.8340, 0.3174, 1.2795],
        [0.6214, 0.4690, 1.0405],
        [0.6777, 0.4950, 1.1162],
        [0.5009, 0.5132, 0.9015],
        [0.7748, 0.3475, 1.2341],
        [0.6857, 0.4582, 1.1246],
        [0.8143, 0.3915, 1.2524],
        [0.6562, 0.4790, 1.0883],
        [0.6733, 0.4627, 1.1020],
        [0.5060, 0.5029, 0.9057],
        [0.7058, 0.4417, 1.1507],
        [0.6746, 0.4926, 1.1040],
        [0.5650, 0.4688, 0.9542],
        [0.6492, 0.4729, 1.0799],
        [0.6061, 0.5095, 1.0402],
        [0.5387, 0.5050, 0.9366],
        [0.7912, 0.3203, 1.2739],
        [0.5610, 0.5125, 0.9724],
        [0.6366, 0.4742, 1.0491],
        [0.5272, 0.5110, 0.9345],
        [0.7027, 0.4601, 1.1563],
        [0.7352, 0.4137, 1.1986],
        [0.6099, 0.5182, 1.0356],
        [0.7734, 0.3496, 1.2331],
        [0.4999, 0.5147, 0.9072],
        [0.7734, 0.3496, 1.2331],
        [0.5665, 0.4872, 0.9744],
        [0.6862, 0.4537, 1.1409],
        [0.7745, 0.3467, 1.2341],
        [0.7057, 0.4405, 1.1224],
        [0.6587, 0.4783, 1.0915],
        [0.7172, 0.3994, 1.1922],
        [0.6474, 0.5392, 1.0558],
        [0.5750, 0.4993, 1.0236],
        [0.4988, 0.5183, 0.8986],
        [0.6345, 0.4999, 1.0777],
        [0.5626, 0.4683, 0.9531],
        [0.6476, 0.4697, 1.0592],
        [0.5222, 0.5105, 0.9185],
        [0.7542, 0.3864, 1.2347],
        [0.5315, 0.5033, 0.9342],
        [0.6277, 0.4921, 1.0419],
        [0.8314, 0.3229, 1.2769],
        [0.6677, 0.4244, 1.1465],
        [0.5118, 0.5159, 0.9346],
        [0.5687, 0.5231, 0.9644],
        [0.5221, 0.5073, 0.9164],
        [0.6865, 0.4664, 1.1048],
        [0.4906, 0.5129, 0.8975],
        [0.5319, 0.5263, 0.9330],
        [0.6870, 0.4654, 1.1049],
        [0.5322, 0.5274, 0.9338],
        [0.5212, 0.5085, 0.9172],
        [0.7783, 0.3576, 1.2273],
        [0.7196, 0.3959, 1.1841],
        [0.5444, 0.5186, 0.9663],
        [0.6557, 0.4797, 1.0875],
        [0.5895, 0.4725, 0.9825],
        [0.5620, 0.4988, 0.9988],
        [0.4996, 0.5183, 0.8986],
        [0.5496, 0.5245, 0.9718],
        [0.6478, 0.4835, 1.0715],
        [0.6125, 0.5212, 1.0318],
        [0.5909, 0.4728, 0.9834],
        [0.6024, 0.5062, 1.0354],
        [0.8143, 0.3915, 1.2524],
        [0.6496, 0.4924, 1.1000],
        [0.5665, 0.4872, 0.9744],
        [0.5380, 0.5064, 0.9387],
        [0.5372, 0.5036, 0.9349],
        [0.5586, 0.5207, 0.9788],
        [0.6174, 0.5134, 1.0458],
        [0.5769, 0.5190, 1.0033],
        [0.5310, 0.5030, 0.9333],
        [0.7548, 0.3867, 1.2347],
        [0.7708, 0.3262, 1.2567],
        [0.6751, 0.4918, 1.1046],
        [0.5343, 0.5142, 0.9454],
        [0.6170, 0.5178, 1.0354],
        [0.5750, 0.4993, 1.0236],
        [0.6005, 0.5081, 1.0327],
        [0.5586, 0.5091, 0.9952],
        [0.6564, 0.5133, 1.0577],
        [0.5749, 0.5198, 1.0185],
        [0.7757, 0.3607, 1.2232],
        [0.6497, 0.4802, 1.1098],
        [0.7014, 0.3785, 1.1984],
        [0.6195, 0.5338, 1.0443],
        [0.5754, 0.4879, 0.9945],
        [0.5721, 0.5000, 0.9666],
        [0.6859, 0.4582, 1.1249],
        [0.5301, 0.5108, 0.9440],
        [0.5428, 0.5025, 0.9557],
        [0.7206, 0.4028, 1.1934],
        [0.6776, 0.4984, 1.1252],
        [0.5665, 0.4872, 0.9744],
        [0.7543, 0.3861, 1.2353],
        [0.5315, 0.5033, 0.9342],
        [0.4825, 0.5128, 0.8875],
        [0.5266, 0.5112, 0.9405],
        [0.6851, 0.4547, 1.1404],
        [0.5398, 0.5092, 0.9342],
        [0.5368, 0.5152, 0.9856],
        [0.6188, 0.5169, 1.0376],
        [0.7085, 0.4403, 1.1551],
        [0.5647, 0.4769, 0.9582],
        [0.5972, 0.4953, 1.0292],
        [0.7464, 0.3961, 1.2147],
        [0.6864, 0.4669, 1.1037],
        [0.5650, 0.5185, 1.0117],
        [0.5322, 0.5274, 0.9338],
        [0.7477, 0.3946, 1.2154],
        [0.6735, 0.4516, 1.1262],
        [0.7543, 0.3861, 1.2353],
        [0.5946, 0.4948, 1.0251],
        [0.6792, 0.4950, 1.1175],
        [0.6726, 0.4529, 1.1246],
        [0.5089, 0.5158, 0.9333],
        [0.7562, 0.3540, 1.2381],
        [0.5785, 0.5177, 0.9727],
        [0.6748, 0.4621, 1.1048],
        [0.5995, 0.5083, 1.0321],
        [0.5620, 0.4988, 0.9988],
        [0.5954, 0.4951, 1.0266],
        [0.5349, 0.5016, 0.9331],
        [0.5212, 0.5085, 0.9172],
        [0.6384, 0.4933, 1.0797],
        [0.5387, 0.5050, 0.9366],
        [0.5838, 0.5038, 0.9884],
        [0.6396, 0.5013, 1.0701],
        [0.5293, 0.5087, 0.9312],
        [0.5341, 0.5153, 0.9472],
        [0.8131, 0.3904, 1.2520],
        [0.6859, 0.4582, 1.1249],
        [0.7058, 0.4417, 1.1507],
        [0.7738, 0.3490, 1.2336],
        [0.5373, 0.5384, 0.9498],
        [0.7757, 0.3607, 1.2232],
        [0.6690, 0.4932, 1.1100],
        [0.6491, 0.4921, 1.0998],
        [0.5216, 0.5005, 0.9552],
        [0.5361, 0.5152, 0.9848],
        [0.5838, 0.5038, 0.9884],
        [0.5117, 0.5101, 0.9174],
        [0.5647, 0.4769, 0.9582],
        [0.5903, 0.4726, 0.9832],
        [0.6473, 0.4647, 1.1180],
        [0.6306, 0.4765, 1.0431],
        [0.6408, 0.4749, 1.1030],
        [0.6399, 0.5331, 1.0489],
        [0.6874, 0.4638, 1.1052],
        [0.5387, 0.5050, 0.9366],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6751, 0.5081, 1.1314],
        [0.5383, 0.5049, 0.9577],
        [0.5707, 0.5226, 0.9970],
        [0.6303, 0.5025, 1.0602],
        [0.7545, 0.3860, 1.2353],
        [0.6950, 0.4675, 1.1123],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.6497, 0.4799, 1.0707],
        [0.8081, 0.3574, 1.2713],
        [0.8453, 0.2825, 1.3216],
        [0.5225, 0.5116, 0.9201],
        [0.7523, 0.3903, 1.2298],
        [0.6422, 0.5049, 1.1026],
        [0.5785, 0.5177, 0.9727],
        [0.6022, 0.5069, 1.0348],
        [0.6229, 0.4830, 1.0827],
        [0.6848, 0.4550, 1.1403],
        [0.6865, 0.4664, 1.1048],
        [0.5678, 0.5215, 1.0155],
        [0.6000, 0.4962, 1.0335],
        [0.5657, 0.4865, 0.9743],
        [0.5060, 0.5029, 0.9057],
        [0.5354, 0.5149, 0.9450],
        [0.6659, 0.4234, 1.1465],
        [0.5119, 0.5159, 0.9348],
        [0.5721, 0.4994, 0.9800],
        [0.5097, 0.5154, 0.9333],
        [0.6593, 0.4781, 1.0921],
        [0.7014, 0.3785, 1.1984],
        [0.7206, 0.4028, 1.1934],
        [0.4825, 0.5121, 0.8868],
        [0.5146, 0.5101, 0.9139],
        [0.6402, 0.5323, 1.0487],
        [0.4996, 0.5183, 0.8986],
        [0.6384, 0.4933, 1.0797],
        [0.7780, 0.3589, 1.2259],
        [0.6557, 0.4797, 1.0875],
        [0.5288, 0.5086, 0.9309],
        [0.5766, 0.5006, 1.0299],
        [0.5610, 0.4681, 0.9523],
        [0.6790, 0.4452, 1.1540],
        [0.6270, 0.4784, 1.0915],
        [0.6402, 0.5323, 1.0487],
        [0.6522, 0.4819, 1.1071],
        [0.5275, 0.5112, 0.9335],
        [0.8334, 0.3202, 1.2785],
        [0.7689, 0.3591, 1.2363],
        [0.5895, 0.4725, 0.9825],
        [0.6184, 0.5125, 1.0475],
        [0.8314, 0.3229, 1.2769],
        [0.6476, 0.4697, 1.0592]], device='mps:0')
2025-03-01 01:41:27 - INFO - Y batch argmax : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:41:42 - INFO - Val Loss=6.1356, Val Acc=0.00%
2025-03-01 01:41:42 - INFO - Classification Report :               precision    recall  f1-score   support

           0       0.00      0.00      0.00    4767.0
           1       0.00      0.00      0.00       0.0
           2       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 01:46:08 - INFO - Size of validation loader : 19
2025-03-01 01:46:18 - INFO - Outputs : tensor([[0.6192, 0.5198, 1.0408],
        [0.6751, 0.4918, 1.1046],
        [0.7381, 0.4094, 1.2027],
        [0.5212, 0.5085, 0.9172],
        [0.4967, 0.5187, 0.8983],
        [0.5753, 0.5215, 1.0070],
        [0.6270, 0.4784, 1.0915],
        [0.6724, 0.4634, 1.1001],
        [0.5427, 0.5176, 0.9651],
        [0.6950, 0.4675, 1.1123],
        [0.6396, 0.5011, 1.0705],
        [0.6890, 0.4632, 1.1058],
        [0.5647, 0.4769, 0.9582],
        [0.7429, 0.4007, 1.2111],
        [0.6143, 0.5192, 1.0312],
        [0.5337, 0.5300, 0.9333],
        [0.6023, 0.5118, 1.0365],
        [0.6851, 0.4547, 1.1404],
        [0.5903, 0.4726, 0.9832],
        [0.5119, 0.5159, 0.9348],
        [0.5353, 0.5030, 0.9338],
        [0.5558, 0.4813, 0.9449],
        [0.5255, 0.5114, 0.9401],
        [0.4986, 0.5065, 0.9006],
        [0.5759, 0.4883, 0.9952],
        [0.6859, 0.4582, 1.1249],
        [0.6404, 0.5045, 1.0981],
        [0.6300, 0.4891, 1.0387],
        [0.4910, 0.5124, 0.8970],
        [0.5003, 0.5180, 0.8985],
        [0.5420, 0.5412, 0.9533],
        [0.7746, 0.3244, 1.2614],
        [0.6307, 0.4903, 1.0387],
        [0.6398, 0.5044, 1.0975],
        [0.6557, 0.4797, 1.0875],
        [0.4987, 0.5066, 0.9008],
        [0.6724, 0.4535, 1.1238],
        [0.7172, 0.3994, 1.1922],
        [0.5216, 0.5006, 0.9551],
        [0.5110, 0.5091, 0.9189],
        [0.6491, 0.4921, 1.0998],
        [0.5753, 0.5215, 1.0070],
        [0.5496, 0.5245, 0.9718],
        [0.6104, 0.5111, 1.0455],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6593, 0.4781, 1.0921],
        [0.5358, 0.5059, 0.9385],
        [0.5130, 0.5035, 0.9084],
        [0.4958, 0.5174, 0.8965],
        [0.8340, 0.3174, 1.2795],
        [0.6214, 0.4690, 1.0405],
        [0.6777, 0.4950, 1.1162],
        [0.5009, 0.5132, 0.9015],
        [0.7748, 0.3475, 1.2341],
        [0.6857, 0.4582, 1.1246],
        [0.8143, 0.3915, 1.2524],
        [0.6562, 0.4790, 1.0883],
        [0.6733, 0.4627, 1.1020],
        [0.5060, 0.5029, 0.9057],
        [0.7058, 0.4417, 1.1507],
        [0.6746, 0.4926, 1.1040],
        [0.5650, 0.4688, 0.9542],
        [0.6492, 0.4729, 1.0799],
        [0.6061, 0.5095, 1.0402],
        [0.5387, 0.5050, 0.9366],
        [0.7912, 0.3203, 1.2739],
        [0.5610, 0.5125, 0.9724],
        [0.6366, 0.4742, 1.0491],
        [0.5272, 0.5110, 0.9345],
        [0.7027, 0.4601, 1.1563],
        [0.7352, 0.4137, 1.1986],
        [0.6099, 0.5182, 1.0356],
        [0.7734, 0.3496, 1.2331],
        [0.4999, 0.5147, 0.9072],
        [0.7734, 0.3496, 1.2331],
        [0.5665, 0.4872, 0.9744],
        [0.6862, 0.4537, 1.1409],
        [0.7745, 0.3467, 1.2341],
        [0.7057, 0.4405, 1.1224],
        [0.6587, 0.4783, 1.0915],
        [0.7172, 0.3994, 1.1922],
        [0.6474, 0.5392, 1.0558],
        [0.5750, 0.4993, 1.0236],
        [0.4988, 0.5183, 0.8986],
        [0.6345, 0.4999, 1.0777],
        [0.5626, 0.4683, 0.9531],
        [0.6476, 0.4697, 1.0592],
        [0.5222, 0.5105, 0.9185],
        [0.7542, 0.3864, 1.2347],
        [0.5315, 0.5033, 0.9342],
        [0.6277, 0.4921, 1.0419],
        [0.8314, 0.3229, 1.2769],
        [0.6677, 0.4244, 1.1465],
        [0.5118, 0.5159, 0.9346],
        [0.5687, 0.5231, 0.9644],
        [0.5221, 0.5073, 0.9164],
        [0.6865, 0.4664, 1.1048],
        [0.4906, 0.5129, 0.8975],
        [0.5319, 0.5263, 0.9330],
        [0.6870, 0.4654, 1.1049],
        [0.5322, 0.5274, 0.9338],
        [0.5212, 0.5085, 0.9172],
        [0.7783, 0.3576, 1.2273],
        [0.7196, 0.3959, 1.1841],
        [0.5444, 0.5186, 0.9663],
        [0.6557, 0.4797, 1.0875],
        [0.5895, 0.4725, 0.9825],
        [0.5620, 0.4988, 0.9988],
        [0.4996, 0.5183, 0.8986],
        [0.5496, 0.5245, 0.9718],
        [0.6478, 0.4835, 1.0715],
        [0.6125, 0.5212, 1.0318],
        [0.5909, 0.4728, 0.9834],
        [0.6024, 0.5062, 1.0354],
        [0.8143, 0.3915, 1.2524],
        [0.6496, 0.4924, 1.1000],
        [0.5665, 0.4872, 0.9744],
        [0.5380, 0.5064, 0.9387],
        [0.5372, 0.5036, 0.9349],
        [0.5586, 0.5207, 0.9788],
        [0.6174, 0.5134, 1.0458],
        [0.5769, 0.5190, 1.0033],
        [0.5310, 0.5030, 0.9333],
        [0.7548, 0.3867, 1.2347],
        [0.7708, 0.3262, 1.2567],
        [0.6751, 0.4918, 1.1046],
        [0.5343, 0.5142, 0.9454],
        [0.6170, 0.5178, 1.0354],
        [0.5750, 0.4993, 1.0236],
        [0.6005, 0.5081, 1.0327],
        [0.5586, 0.5091, 0.9952],
        [0.6564, 0.5133, 1.0577],
        [0.5749, 0.5198, 1.0185],
        [0.7757, 0.3607, 1.2232],
        [0.6497, 0.4802, 1.1098],
        [0.7014, 0.3785, 1.1984],
        [0.6195, 0.5338, 1.0443],
        [0.5754, 0.4879, 0.9945],
        [0.5721, 0.5000, 0.9666],
        [0.6859, 0.4582, 1.1249],
        [0.5301, 0.5108, 0.9440],
        [0.5428, 0.5025, 0.9557],
        [0.7206, 0.4028, 1.1934],
        [0.6776, 0.4984, 1.1252],
        [0.5665, 0.4872, 0.9744],
        [0.7543, 0.3861, 1.2353],
        [0.5315, 0.5033, 0.9342],
        [0.4825, 0.5128, 0.8875],
        [0.5266, 0.5112, 0.9405],
        [0.6851, 0.4547, 1.1404],
        [0.5398, 0.5092, 0.9342],
        [0.5368, 0.5152, 0.9856],
        [0.6188, 0.5169, 1.0376],
        [0.7085, 0.4403, 1.1551],
        [0.5647, 0.4769, 0.9582],
        [0.5972, 0.4953, 1.0292],
        [0.7464, 0.3961, 1.2147],
        [0.6864, 0.4669, 1.1037],
        [0.5650, 0.5185, 1.0117],
        [0.5322, 0.5274, 0.9338],
        [0.7477, 0.3946, 1.2154],
        [0.6735, 0.4516, 1.1262],
        [0.7543, 0.3861, 1.2353],
        [0.5946, 0.4948, 1.0251],
        [0.6792, 0.4950, 1.1175],
        [0.6726, 0.4529, 1.1246],
        [0.5089, 0.5158, 0.9333],
        [0.7562, 0.3540, 1.2381],
        [0.5785, 0.5177, 0.9727],
        [0.6748, 0.4621, 1.1048],
        [0.5995, 0.5083, 1.0321],
        [0.5620, 0.4988, 0.9988],
        [0.5954, 0.4951, 1.0266],
        [0.5349, 0.5016, 0.9331],
        [0.5212, 0.5085, 0.9172],
        [0.6384, 0.4933, 1.0797],
        [0.5387, 0.5050, 0.9366],
        [0.5838, 0.5038, 0.9884],
        [0.6396, 0.5013, 1.0701],
        [0.5293, 0.5087, 0.9312],
        [0.5341, 0.5153, 0.9472],
        [0.8131, 0.3904, 1.2520],
        [0.6859, 0.4582, 1.1249],
        [0.7058, 0.4417, 1.1507],
        [0.7738, 0.3490, 1.2336],
        [0.5373, 0.5384, 0.9498],
        [0.7757, 0.3607, 1.2232],
        [0.6690, 0.4932, 1.1100],
        [0.6491, 0.4921, 1.0998],
        [0.5216, 0.5005, 0.9552],
        [0.5361, 0.5152, 0.9848],
        [0.5838, 0.5038, 0.9884],
        [0.5117, 0.5101, 0.9174],
        [0.5647, 0.4769, 0.9582],
        [0.5903, 0.4726, 0.9832],
        [0.6473, 0.4647, 1.1180],
        [0.6306, 0.4765, 1.0431],
        [0.6408, 0.4749, 1.1030],
        [0.6399, 0.5331, 1.0489],
        [0.6874, 0.4638, 1.1052],
        [0.5387, 0.5050, 0.9366],
        [0.7074, 0.4401, 1.1539],
        [0.5265, 0.5084, 0.9299],
        [0.6751, 0.5081, 1.1314],
        [0.5383, 0.5049, 0.9577],
        [0.5707, 0.5226, 0.9970],
        [0.6303, 0.5025, 1.0602],
        [0.7545, 0.3860, 1.2353],
        [0.6950, 0.4675, 1.1123],
        [0.5387, 0.5049, 0.9372],
        [0.6325, 0.5023, 1.0624],
        [0.6497, 0.4799, 1.0707],
        [0.8081, 0.3574, 1.2713],
        [0.8453, 0.2825, 1.3216],
        [0.5225, 0.5116, 0.9201],
        [0.7523, 0.3903, 1.2298],
        [0.6422, 0.5049, 1.1026],
        [0.5785, 0.5177, 0.9727],
        [0.6022, 0.5069, 1.0348],
        [0.6229, 0.4830, 1.0827],
        [0.6848, 0.4550, 1.1403],
        [0.6865, 0.4664, 1.1048],
        [0.5678, 0.5215, 1.0155],
        [0.6000, 0.4962, 1.0335],
        [0.5657, 0.4865, 0.9743],
        [0.5060, 0.5029, 0.9057],
        [0.5354, 0.5149, 0.9450],
        [0.6659, 0.4234, 1.1465],
        [0.5119, 0.5159, 0.9348],
        [0.5721, 0.4994, 0.9800],
        [0.5097, 0.5154, 0.9333],
        [0.6593, 0.4781, 1.0921],
        [0.7014, 0.3785, 1.1984],
        [0.7206, 0.4028, 1.1934],
        [0.4825, 0.5121, 0.8868],
        [0.5146, 0.5101, 0.9139],
        [0.6402, 0.5323, 1.0487],
        [0.4996, 0.5183, 0.8986],
        [0.6384, 0.4933, 1.0797],
        [0.7780, 0.3589, 1.2259],
        [0.6557, 0.4797, 1.0875],
        [0.5288, 0.5086, 0.9309],
        [0.5766, 0.5006, 1.0299],
        [0.5610, 0.4681, 0.9523],
        [0.6790, 0.4452, 1.1540],
        [0.6270, 0.4784, 1.0915],
        [0.6402, 0.5323, 1.0487],
        [0.6522, 0.4819, 1.1071],
        [0.5275, 0.5112, 0.9335],
        [0.8334, 0.3202, 1.2785],
        [0.7689, 0.3591, 1.2363],
        [0.5895, 0.4725, 0.9825],
        [0.6184, 0.5125, 1.0475],
        [0.8314, 0.3229, 1.2769],
        [0.6476, 0.4697, 1.0592]], device='mps:0')
2025-03-01 01:46:18 - INFO - Outputs shape : torch.Size([256, 3])
2025-03-01 01:46:19 - INFO - Y batch : tensor([[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]], device='mps:0')
2025-03-01 01:46:19 - INFO - Y batch shape : torch.Size([256, 3])
2025-03-01 01:46:19 - INFO - Outputs argmax : [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
2025-03-01 01:46:19 - INFO - Y batch argmax : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
2025-03-01 01:46:40 - INFO - Val Loss=6.1356, Val Acc=0.00%
2025-03-01 01:46:40 - INFO - Classification Report :               precision    recall  f1-score   support

           0       0.00      0.00      0.00    4767.0
           1       0.00      0.00      0.00       0.0
           2       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 02:07:15 - INFO - Full sequence shape : (201, 81)
2025-03-01 02:07:15 - INFO - Decoder sequence length : 188
2025-03-01 02:07:15 - INFO - Slope value shape : (188,)
2025-03-01 02:07:15 - INFO - Slope values : [2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1
 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 1 1 1 2 2 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1]
2025-03-01 02:07:15 - INFO - Target bin max : 1
2025-03-01 02:07:15 - INFO - slope classes : tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
2025-03-01 02:07:15 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-03-01 02:07:15 - INFO - Future price : 1
2025-03-01 02:07:15 - INFO - X shape : torch.Size([200, 51])
2025-03-01 02:07:15 - INFO - y shape : torch.Size([3])
2025-03-01 02:07:15 - INFO - y : tensor([0., 1., 0.])
2025-03-01 02:07:15 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 02:07:15 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 02:07:15 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 02:07:15 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 02:07:15 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 02:07:24 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 02:07:24 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 02:07:24 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 02:07:35 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 02:07:35 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 02:07:35 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 02:07:36 - INFO - Output s : tensor([[ 8.3460e-01,  1.0687e+00,  9.0930e-01],
        [ 2.8804e-02,  1.9463e+00, -1.9718e-01],
        [ 1.0962e-01,  1.1038e+00,  1.6302e+00],
        [ 9.5603e-02, -1.1830e+00, -1.2518e+00],
        [ 4.5376e-01,  1.1466e-01,  9.9345e-01],
        [-7.2445e-01,  9.5252e-01, -6.0528e-01],
        [ 1.6743e+00,  1.9612e-01,  7.4959e-01],
        [ 7.8500e-01,  1.7049e+00,  4.5872e-01],
        [ 8.7027e-01,  7.5270e-01,  8.0533e-01],
        [-1.0545e+00,  1.0427e+00, -1.2100e+00],
        [ 1.1791e-01, -9.1056e-01,  7.0452e-01],
        [ 9.0275e-01,  2.5068e+00,  4.1006e-01],
        [-2.1864e+00,  2.4772e+00, -1.0617e+00],
        [-1.4839e-01, -9.7199e-01, -7.4658e-03],
        [ 9.9545e-01,  6.9583e-01,  1.0638e+00],
        [ 1.6296e+00,  1.9255e-01,  2.3277e+00],
        [-1.4672e+00,  1.1783e+00, -7.5640e-01],
        [ 5.4301e-01,  1.6126e+00,  1.7150e+00],
        [ 1.2590e+00,  6.8976e-01,  7.5731e-01],
        [-7.5948e-01, -6.2104e-01,  9.0983e-01],
        [ 1.4934e+00,  3.9700e+00, -3.2466e-01],
        [-1.5957e+00, -1.5313e+00, -9.5014e-01],
        [-9.0915e-01, -4.4293e-01, -4.3643e-02],
        [-1.7730e-01, -1.7302e-01, -9.3426e-01],
        [ 1.3088e+00,  2.9465e-01,  1.2030e-01],
        [-4.2080e-01, -2.3372e+00,  6.3797e-01],
        [ 4.9212e-01, -1.0554e-01,  6.3777e-01],
        [ 2.2363e-01,  1.1077e+00,  9.3857e-01],
        [-1.2321e+00,  1.5062e+00, -1.8703e+00],
        [ 8.7365e-01,  1.5634e+00,  4.7396e-01],
        [ 1.6963e+00,  1.3512e+00,  9.4552e-01],
        [-7.9564e-01, -6.4561e-01, -2.7264e-01],
        [-6.3991e-01, -1.4023e+00, -1.5109e+00],
        [ 2.8266e-01,  6.8519e-01, -5.5313e-02],
        [-5.4673e-01, -3.4763e-01, -4.3577e-01],
        [ 1.3418e+00,  1.5478e+00,  7.8478e-01],
        [-1.1432e+00, -2.7529e-01, -2.9259e-01],
        [-1.3866e-02,  6.3273e-02,  8.3462e-01],
        [-9.5119e-01, -1.4821e-01, -1.6873e+00],
        [-8.0090e-01,  2.7744e+00, -1.5736e+00],
        [ 6.6278e-01,  1.0063e+00,  4.2610e-01],
        [ 1.2825e-01, -1.5981e-01, -6.2228e-01],
        [ 7.7593e-01,  1.0602e+00,  1.8149e+00],
        [-2.4346e-01,  1.1728e+00,  1.6290e+00],
        [ 6.0061e-01,  6.8360e-01,  1.3239e+00],
        [ 1.5283e+00, -3.5516e-01,  2.9320e-01],
        [-2.8504e+00,  8.5969e-01, -4.1467e-01],
        [-2.1618e-01,  4.1695e-01, -2.0120e+00],
        [ 3.3912e-01,  1.9914e-01, -1.0837e+00],
        [-8.5956e-01,  2.2627e-01,  6.3292e-01],
        [ 8.2705e-01,  6.4894e-01,  9.7603e-01],
        [ 6.1783e-02,  3.9523e-02,  8.0812e-01],
        [ 1.1043e+00,  2.6458e-01,  5.2476e-01],
        [ 5.6577e-01, -1.3318e+00,  1.1801e+00],
        [ 3.2477e-02,  8.2984e-01, -2.1829e+00],
        [-2.0669e+00,  2.5560e+00, -1.9906e+00],
        [ 1.0396e+00,  7.7768e-01,  3.6578e-01],
        [ 1.6732e+00,  2.5031e+00,  2.0254e-01],
        [-2.0108e+00, -4.1828e-01,  5.6371e-01],
        [-1.5423e-01, -1.0246e+00, -3.8516e-01],
        [-7.0872e-01, -1.8381e+00, -9.0422e-01],
        [-2.1380e+00,  5.5014e-02, -1.7805e+00],
        [ 7.0335e-02,  2.1705e+00,  1.3882e+00],
        [-1.0712e+00,  4.3642e-01, -2.2457e-01],
        [-2.5530e+00,  1.1085e+00, -2.1262e+00],
        [ 7.9618e-02,  6.4388e-01,  8.8563e-01],
        [ 1.4508e+00,  9.6040e-01, -1.1139e-01],
        [-2.1686e-01, -4.3175e-01,  1.3288e-01],
        [ 1.9775e+00,  2.3063e+00, -6.0589e-01],
        [ 3.4509e-01, -1.6574e+00, -6.0657e-01],
        [ 2.5566e-02,  6.8343e-02, -5.2589e-01],
        [-8.2340e-02,  1.9417e+00,  3.9497e-01],
        [ 1.3297e+00,  1.8307e+00,  6.3890e-01],
        [-7.1127e-01, -8.9416e-01, -2.0861e-02],
        [-3.1156e-01, -9.1186e-01, -6.6468e-01],
        [-2.9379e-01,  2.4098e+00,  7.1158e-02],
        [ 2.9391e-01,  5.5170e-01,  6.3758e-01],
        [ 7.1473e-01,  5.1967e-01,  1.8878e+00],
        [ 5.9631e-01, -1.8845e-01,  1.2381e-01],
        [-6.9257e-01,  1.1937e-01, -6.0658e-01],
        [ 2.8005e-01, -1.9216e-01, -5.6125e-01],
        [ 8.2250e-01, -6.8337e-02,  5.3268e-01],
        [ 1.1965e-01,  1.4562e+00,  2.3122e-01],
        [ 1.2269e+00, -9.7566e-01,  4.1131e-01],
        [ 1.9385e+00, -6.4587e-01,  1.2690e+00],
        [-6.3610e-01, -7.9736e-02, -6.7664e-01],
        [-2.1473e+00,  1.2347e+00, -2.3031e+00],
        [ 1.3390e+00,  1.9752e+00,  3.7652e-01],
        [-7.5194e-01, -2.4677e+00, -4.6836e-01],
        [ 1.3476e+00, -1.6778e-01,  3.4155e-01],
        [ 7.4426e-01,  6.1050e-01,  9.2176e-01],
        [ 6.3906e-01,  1.3247e+00,  1.7381e+00],
        [ 6.2226e-01,  2.9288e+00, -1.3594e-01],
        [ 1.0374e-01, -3.8296e-01,  3.8118e-01],
        [ 6.0446e-01,  1.1525e-01, -1.8407e-01],
        [ 6.3308e-01,  1.7652e+00,  1.3517e+00],
        [-1.7247e+00,  1.4178e+00, -5.4654e-01],
        [-4.8258e-01,  1.3333e+00, -1.3232e-01],
        [-9.5229e-01,  1.0966e-01, -9.8468e-01],
        [ 5.6576e-01, -5.0603e-01,  4.8546e-01],
        [ 1.0820e+00,  1.2103e+00,  2.3025e-01],
        [-1.3890e+00,  3.2976e-01, -1.4234e+00],
        [ 5.7215e-02, -3.0204e-02,  3.1339e-01],
        [ 3.4851e-01, -2.3645e-01,  1.2954e+00],
        [-7.2461e-01,  2.0417e+00,  1.3386e-01],
        [-1.1603e+00, -9.0450e-01, -1.4496e+00],
        [-3.3215e-01, -6.8323e-01,  6.6459e-01],
        [-1.3820e+00, -6.7382e-02, -1.2733e-02],
        [ 3.5092e-01, -2.2026e+00,  3.3997e-01],
        [ 3.5092e-01, -8.2371e-01, -4.3381e-01],
        [ 8.0772e-01,  1.6646e+00,  6.9731e-01],
        [ 4.6615e-01,  8.5266e-01, -3.1380e-01],
        [ 3.7549e-01,  1.3425e+00, -4.6975e-01],
        [ 1.1517e+00, -1.2695e-01,  5.4532e-01],
        [ 9.3155e-01, -1.3699e+00,  1.7692e+00],
        [ 1.3880e+00,  7.7485e-01,  3.1522e-01],
        [-3.4991e-01,  5.8247e-01, -3.0266e+00],
        [ 3.5726e-01,  2.3121e+00, -7.6757e-01],
        [-6.1438e-02,  1.7149e+00,  6.0264e-01],
        [ 1.2641e-01,  8.9602e-01, -6.8238e-01],
        [ 1.7516e+00,  1.0656e+00,  1.8363e+00],
        [-1.8733e-01, -7.8202e-01,  2.4160e-01],
        [ 2.3996e-01,  9.6557e-01,  1.1428e+00],
        [-1.5641e+00,  1.0887e-01,  7.9083e-01],
        [ 1.0929e+00,  1.4392e+00,  5.0824e-01],
        [ 3.7418e-01, -5.7506e-01, -1.4212e-01],
        [ 7.1931e-01,  1.3399e-02,  1.5400e+00],
        [-1.3001e+00, -8.2324e-01,  6.0376e-01],
        [ 9.0587e-02,  2.5659e-01,  3.5671e-02],
        [ 1.9152e+00, -6.7892e-02, -1.8698e-02],
        [ 2.8263e-01,  1.4418e+00,  2.2305e-01],
        [ 4.3415e-01, -1.0070e+00,  7.3227e-02],
        [ 2.6635e-01,  9.0830e-01,  1.7449e+00],
        [-2.5392e-01, -3.0977e-01,  7.7642e-01],
        [-2.0897e+00,  7.5342e-02,  9.0287e-01],
        [ 1.7472e+00,  1.7094e+00,  7.0241e-01],
        [ 1.1481e+00, -1.2715e+00,  4.8426e-01],
        [-1.0493e+00,  2.6942e+00, -7.0342e-01],
        [ 1.3186e+00,  1.3779e+00,  2.0394e-01],
        [-4.1721e-01, -1.6739e+00, -4.7985e-02],
        [ 2.8381e-01, -1.2585e+00,  8.1327e-01],
        [ 7.8901e-01, -3.0640e-01, -2.0109e-01],
        [ 5.7426e-02,  1.5325e+00,  2.5617e-01],
        [ 1.0670e+00,  6.6078e-01,  1.2523e+00],
        [ 2.5611e+00,  1.0796e+00,  1.9649e+00],
        [ 4.5790e-01,  3.9426e-02, -8.7217e-02],
        [ 4.1627e-01,  1.1248e+00, -6.6968e-01],
        [-1.6886e-01, -1.0466e+00,  3.1491e-01],
        [-2.1112e+00,  9.1880e-01, -2.4309e+00],
        [ 8.6760e-01,  3.0503e+00, -6.9898e-01],
        [ 1.9956e+00,  1.3280e+00, -3.8049e-01],
        [-1.7923e+00, -7.4505e-01,  8.8731e-02],
        [-1.2272e+00,  1.4876e+00, -1.6405e-01],
        [ 2.0847e+00, -3.5516e-02, -5.3954e-01],
        [-5.6065e-02,  4.8432e-01,  1.7334e-01],
        [ 7.0468e-01,  3.4799e-01,  4.5934e-01],
        [ 1.2266e+00,  1.8500e+00,  1.1969e+00],
        [-8.9969e-01,  1.4794e+00, -6.0723e-01],
        [ 7.5576e-01,  3.1745e-01, -7.2805e-01],
        [ 1.2739e+00,  1.6358e+00, -9.0073e-01],
        [-3.4321e-01, -1.8236e+00,  8.5957e-01],
        [ 7.5248e-01, -1.7660e-01, -2.9970e-01],
        [ 1.5755e+00,  2.2776e+00,  6.7414e-02],
        [ 8.2091e-01, -1.3171e+00, -2.1292e-02],
        [-1.7493e+00,  1.0667e+00, -2.5631e-01],
        [-7.5230e-01,  2.3103e+00, -2.3508e+00],
        [-5.1670e-01, -2.6097e-01,  1.9456e-01],
        [-1.5375e+00, -1.6980e-01, -3.2301e-01],
        [ 9.2884e-01,  1.0794e+00,  1.2869e+00],
        [-3.3008e-01,  2.0817e+00, -1.3834e+00],
        [ 2.2698e-01, -1.1469e+00, -1.7910e+00],
        [ 2.9843e+00,  1.4828e+00,  6.5562e-01],
        [ 9.1129e-01,  1.1876e+00,  4.1732e-01],
        [ 1.0242e+00, -5.5529e-01, -1.1913e-01],
        [-3.9003e-01,  1.9661e+00, -1.5653e+00],
        [ 6.4320e-02,  6.0791e-01,  5.0048e-01],
        [-7.0417e-01,  2.8175e+00,  8.7523e-02],
        [ 5.6449e-01,  9.1422e-01,  2.4181e-01],
        [-1.1867e+00, -2.0702e+00,  3.7323e-01],
        [ 1.2104e+00,  1.5549e+00, -5.1126e-01],
        [ 7.7309e-01, -6.9581e-01,  1.0149e+00],
        [ 5.0499e-01, -3.6083e+00, -9.3007e-01],
        [ 3.4858e-01,  8.4259e-01, -2.6996e-01],
        [ 1.0910e+00,  1.4484e+00, -1.8606e-01],
        [ 8.6121e-01, -1.4708e+00,  5.6973e-01],
        [ 5.9283e-01,  9.5324e-01,  1.2437e+00],
        [ 1.6976e+00, -7.8426e-02,  5.0379e-01],
        [ 1.0268e+00,  3.7226e-01,  2.7481e-01],
        [ 1.1922e+00,  9.9404e-01,  4.6764e-01],
        [-2.0729e-01,  4.3069e-01,  2.4000e-01],
        [ 2.9935e-01,  2.6199e+00, -1.5727e+00],
        [ 1.7327e+00, -4.3337e-01,  3.7778e-01],
        [ 9.1477e-01,  1.8317e+00,  4.3740e-01],
        [ 3.1859e-01, -1.2571e-01,  1.6538e+00],
        [ 6.4193e-01,  2.0431e+00,  1.7038e-03],
        [ 1.2553e+00, -8.8878e-01,  1.8250e+00],
        [-2.6855e+00,  9.7629e-01, -8.5404e-01],
        [-6.8396e-01,  1.7155e+00, -6.1572e-02],
        [ 1.1357e+00,  1.7716e-01, -6.6803e-02],
        [ 6.9736e-01, -2.0005e+00,  1.1253e+00],
        [ 9.0884e-01,  5.7418e-01,  6.9927e-01],
        [ 1.0603e+00,  1.3022e+00, -4.8938e-02],
        [ 9.1168e-01, -3.9892e-01, -1.1106e-01],
        [ 3.3808e-01,  1.5186e+00, -3.2014e-01],
        [ 6.8853e-01,  9.9589e-01,  1.3103e+00],
        [ 1.2190e+00,  1.3625e+00,  2.5390e-01],
        [-4.4007e-01,  1.4022e+00,  1.6173e+00],
        [ 2.3766e+00,  1.2689e+00,  2.4026e-01],
        [-5.5362e-02, -5.3875e-01, -9.3028e-01],
        [-1.7905e+00, -4.1585e-02, -6.3427e-01],
        [-1.0354e+00,  9.9816e-01, -3.2325e-01],
        [ 5.7875e-01, -8.2834e-01,  1.6288e+00],
        [ 1.7928e+00,  2.0827e-01,  8.6276e-01],
        [ 1.2480e-02,  1.4005e+00, -1.1803e+00],
        [-1.9071e+00, -3.9399e-01, -7.2185e-01],
        [ 1.3329e+00,  4.3541e-01,  5.9397e-01],
        [-9.5412e-02, -6.5232e-01, -2.9193e-02],
        [-6.1146e-01, -1.9240e+00, -2.3911e-01],
        [-8.5175e-01,  1.2059e+00, -2.3774e+00],
        [-8.4066e-01,  6.9890e-01, -2.0425e+00],
        [ 1.0106e-01,  2.7329e-01,  1.2474e+00],
        [ 1.2928e-01,  1.2482e+00, -5.5083e-01],
        [ 7.0402e-01,  8.1897e-02,  7.0470e-01],
        [-1.7482e-01,  9.9333e-01,  3.3068e-01],
        [ 2.9729e-01,  3.0837e+00, -4.8195e-01],
        [ 1.5777e+00, -8.6077e-01,  8.6674e-01],
        [ 1.5872e+00, -3.3113e-01,  7.3776e-01],
        [ 1.6619e-01,  5.5712e-01,  6.5595e-01],
        [-9.4148e-01, -2.0646e-01, -8.8807e-01],
        [-2.0828e+00, -2.0750e+00,  4.7741e-01],
        [ 7.5742e-02,  2.4649e+00, -2.2519e-01],
        [ 3.2023e-01, -6.4400e-01,  2.9255e-02],
        [ 7.9804e-02,  5.6493e-02,  1.8027e-01],
        [ 9.4660e-01,  1.1099e+00,  3.5874e-01],
        [ 1.1608e+00,  6.6898e-01,  1.0571e+00],
        [-1.4412e-01, -1.7527e-01, -4.2399e-01],
        [ 1.1696e-01,  9.8033e-01, -1.0169e+00],
        [-6.0319e-01,  2.3564e+00, -4.0551e-01],
        [-9.8410e-01,  6.6991e-01, -1.5382e+00],
        [ 2.0034e+00,  6.7624e-01,  5.8481e-02],
        [ 1.1994e+00,  1.2779e+00,  1.3871e+00],
        [ 4.8046e-01,  5.4649e-01,  1.5250e+00],
        [ 1.6413e-01,  2.0704e+00, -7.3780e-01],
        [-7.8038e-03,  1.0166e+00,  2.1248e+00],
        [-1.1810e+00, -6.1460e-01, -1.6467e+00],
        [ 8.8873e-01,  2.1858e+00, -5.6681e-01],
        [ 2.3295e+00,  1.0818e+00,  4.5783e-01],
        [ 1.3339e+00,  5.6343e-01,  1.3746e+00],
        [ 1.6550e+00, -4.1107e-01, -7.1404e-02],
        [ 1.8998e+00,  4.6738e-01,  1.0685e+00],
        [ 9.2858e-01,  6.8018e-01, -1.8812e-01],
        [ 1.2422e+00,  1.0938e+00,  3.2405e-01],
        [-3.9264e-01,  1.6957e+00, -2.2911e-01],
        [-1.2992e+00,  1.5664e+00, -1.2838e+00],
        [ 2.4688e+00,  1.2781e+00, -3.5406e-01],
        [ 8.7630e-01, -7.2883e-01,  1.2184e+00]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 02:07:36 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 02:07:36 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 02:07:36 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 02:07:41 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.519027, Accuracy = 28.91%, MSE = 1.1211
2025-03-01 02:09:46 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 2.267030, Accuracy = 38.46%, MSE = 0.9712
2025-03-01 02:11:23 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 1.558058, Accuracy = 38.86%, MSE = 1.0082
2025-03-01 02:13:44 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 1.164084, Accuracy = 39.49%, MSE = 0.9574
2025-03-01 02:16:23 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 1.077232, Accuracy = 40.29%, MSE = 0.9546
2025-03-01 02:18:34 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 1.138950, Accuracy = 41.44%, MSE = 0.9259
2025-03-01 02:20:10 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 1.159886, Accuracy = 42.00%, MSE = 0.9075
2025-03-01 02:22:07 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 1.043987, Accuracy = 42.33%, MSE = 0.9004
2025-03-01 02:22:38 - INFO - Epoch 2: Train Loss=1.3043, Train Acc=42.56%, Train MSE=0.8914
2025-03-01 02:22:54 - INFO - Epoch 2: Val Loss=1.1945, Val Acc=0.00%
2025-03-01 02:22:54 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 02:23:02 - INFO - Epoch : 2 , Batch [ 0 / 75 ] : Loss = 1.120631, Accuracy = 37.11%, MSE = 1.2617
2025-03-01 02:24:16 - INFO - Epoch : 2 , Batch [ 10 / 75 ] : Loss = 1.222971, Accuracy = 42.90%, MSE = 1.0142
2025-03-01 02:25:53 - INFO - Epoch : 2 , Batch [ 20 / 75 ] : Loss = 1.149772, Accuracy = 44.74%, MSE = 0.9628
2025-03-01 02:28:38 - INFO - Epoch : 2 , Batch [ 30 / 75 ] : Loss = 1.114508, Accuracy = 44.57%, MSE = 0.9399
2025-03-01 02:33:06 - INFO - Epoch : 2 , Batch [ 40 / 75 ] : Loss = 0.986156, Accuracy = 45.73%, MSE = 0.8877
2025-03-01 02:36:07 - INFO - Epoch : 2 , Batch [ 50 / 75 ] : Loss = 1.091491, Accuracy = 46.48%, MSE = 0.8653
2025-03-01 02:39:04 - INFO - Epoch : 2 , Batch [ 60 / 75 ] : Loss = 0.974157, Accuracy = 47.36%, MSE = 0.8432
2025-03-01 02:41:52 - INFO - Epoch : 2 , Batch [ 70 / 75 ] : Loss = 0.950722, Accuracy = 47.87%, MSE = 0.8321
2025-03-01 02:43:10 - INFO - Epoch 3: Train Loss=1.0643, Train Acc=48.11%, Train MSE=0.8293
2025-03-01 02:43:28 - INFO - Epoch 3: Val Loss=1.8259, Val Acc=0.00%
2025-03-01 02:43:28 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 02:43:36 - INFO - Epoch : 3 , Batch [ 0 / 75 ] : Loss = 1.001175, Accuracy = 51.95%, MSE = 0.6328
2025-03-01 02:45:45 - INFO - Epoch : 3 , Batch [ 10 / 75 ] : Loss = 1.093701, Accuracy = 48.26%, MSE = 0.7752
2025-03-01 02:51:38 - INFO - Epoch : 3 , Batch [ 20 / 75 ] : Loss = 1.201409, Accuracy = 49.33%, MSE = 0.7500
2025-03-01 02:55:17 - INFO - Epoch : 3 , Batch [ 30 / 75 ] : Loss = 1.020412, Accuracy = 49.68%, MSE = 0.7405
2025-03-01 02:57:36 - INFO - Epoch : 3 , Batch [ 40 / 75 ] : Loss = 1.045819, Accuracy = 50.37%, MSE = 0.7289
2025-03-01 03:00:33 - INFO - Epoch : 3 , Batch [ 50 / 75 ] : Loss = 1.000051, Accuracy = 50.13%, MSE = 0.7510
2025-03-01 03:04:21 - INFO - Epoch : 3 , Batch [ 60 / 75 ] : Loss = 1.014883, Accuracy = 50.42%, MSE = 0.7483
2025-03-01 03:06:58 - INFO - Epoch : 3 , Batch [ 70 / 75 ] : Loss = 0.983081, Accuracy = 50.41%, MSE = 0.7470
2025-03-01 03:07:36 - INFO - Epoch 4: Train Loss=1.0357, Train Acc=50.37%, Train MSE=0.7371
2025-03-01 03:07:53 - INFO - Epoch 4: Val Loss=2.2883, Val Acc=0.00%
2025-03-01 03:07:53 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 03:07:59 - INFO - Epoch : 4 , Batch [ 0 / 75 ] : Loss = 1.079387, Accuracy = 43.75%, MSE = 0.8672
2025-03-01 03:09:35 - INFO - Epoch : 4 , Batch [ 10 / 75 ] : Loss = 0.959882, Accuracy = 49.82%, MSE = 0.7031
2025-03-01 03:11:14 - INFO - Epoch : 4 , Batch [ 20 / 75 ] : Loss = 0.919920, Accuracy = 53.39%, MSE = 0.6659
2025-03-01 03:13:06 - INFO - Epoch : 4 , Batch [ 30 / 75 ] : Loss = 0.957348, Accuracy = 53.88%, MSE = 0.6699
2025-03-01 03:14:55 - INFO - Epoch : 4 , Batch [ 40 / 75 ] : Loss = 1.017389, Accuracy = 54.26%, MSE = 0.6669
2025-03-01 03:16:31 - INFO - Epoch : 4 , Batch [ 50 / 75 ] : Loss = 0.942739, Accuracy = 54.36%, MSE = 0.6674
2025-03-01 03:18:22 - INFO - Epoch : 4 , Batch [ 60 / 75 ] : Loss = 1.061013, Accuracy = 54.05%, MSE = 0.6714
2025-03-01 03:36:17 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 03:36:17 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 03:36:17 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 03:36:17 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 03:36:17 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 03:36:22 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 03:36:22 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 03:36:22 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 03:36:37 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 03:36:37 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 03:36:37 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 03:36:38 - INFO - Output s : tensor([[-1.0157e+00,  2.1743e+00,  2.3848e+00],
        [-8.2423e-01,  1.5405e+00,  1.4811e+00],
        [-6.4900e-01,  1.1682e+00,  5.0885e-01],
        [-1.8161e-01,  1.1955e+00,  1.7191e+00],
        [-8.1431e-01,  9.1004e-01,  8.8778e-01],
        [-9.6589e-01,  7.1465e-01,  1.4535e+00],
        [-1.5268e-01,  3.5660e-01, -1.3244e+00],
        [-2.4039e+00,  1.1601e+00, -2.1224e+00],
        [-2.9684e-01,  1.7182e+00,  1.2196e+00],
        [-7.1113e-01,  8.0529e-01, -8.8260e-01],
        [-3.8973e-01,  1.2805e-01, -2.1126e-01],
        [-1.2745e+00,  1.2874e+00, -3.4840e-01],
        [ 1.1150e+00,  2.1571e+00, -1.3105e-01],
        [-1.7219e+00,  2.1128e+00, -1.0168e+00],
        [-8.2775e-01,  1.4779e+00,  2.8530e+00],
        [-1.2718e+00,  1.1702e+00,  1.9495e+00],
        [-3.6227e-01,  1.5481e+00,  4.7551e-02],
        [-1.4588e+00,  1.2767e+00,  1.2793e+00],
        [-1.1255e+00, -3.8110e-01,  1.8916e+00],
        [-1.2005e+00,  1.5807e+00,  5.9611e-01],
        [-1.4176e+00,  1.7587e+00,  1.9980e+00],
        [-1.1435e+00,  1.7884e+00, -2.3212e-01],
        [-2.1986e+00,  1.8376e+00,  1.8010e+00],
        [-4.0263e-01,  1.9156e+00,  9.2987e-02],
        [-1.1407e+00,  1.6867e+00,  6.7035e-01],
        [-8.1067e-01,  8.3155e-01,  9.7205e-01],
        [-1.4061e+00,  4.0616e-01, -5.1740e-01],
        [-1.2071e+00,  6.2350e-01,  8.1045e-01],
        [-4.9620e-01,  1.1050e+00,  3.9798e-01],
        [-8.6738e-01,  2.0665e+00,  1.4610e+00],
        [-1.1958e+00,  7.3974e-01, -8.8072e-01],
        [-5.6256e-02,  2.3770e+00,  4.0402e-01],
        [-5.3806e-01,  7.3077e-01,  2.9938e+00],
        [-7.1155e-01,  2.3839e+00, -2.9262e-03],
        [-9.9478e-01,  4.4702e-01, -1.7030e+00],
        [-9.9230e-01,  7.7117e-01, -1.6990e-01],
        [-7.2894e-01,  9.1532e-01, -3.0495e-02],
        [-1.1132e+00,  2.2780e+00, -1.5507e+00],
        [-9.8409e-01,  1.8244e-01, -1.9489e+00],
        [ 4.0275e-02,  3.7873e-01,  1.9216e+00],
        [-3.4113e-01,  1.3023e+00, -1.4846e-03],
        [-1.1293e+00,  7.2883e-01,  2.0243e+00],
        [-1.6870e-01,  2.6128e+00, -1.0914e+00],
        [-8.2572e-01,  7.2988e-01,  3.1855e-01],
        [-8.0816e-01,  1.4763e+00,  1.0463e-01],
        [-8.0117e-01,  2.1932e+00, -3.6297e-01],
        [-1.2268e+00,  1.7601e+00,  5.2013e-01],
        [-1.1970e+00,  9.1605e-02,  2.6564e+00],
        [-6.5033e-01,  1.4282e+00,  1.5487e+00],
        [-6.0943e-01,  2.8713e+00, -1.3647e+00],
        [-1.4734e+00,  9.1420e-01,  2.2412e+00],
        [-3.8022e-01,  2.2084e+00, -7.2083e-01],
        [-5.9896e-01,  1.9157e+00, -1.9377e+00],
        [ 3.4250e-01,  1.5443e+00,  2.4441e+00],
        [ 5.8883e-02,  7.0121e-01,  6.3649e-01],
        [-3.4084e-01,  1.0035e+00, -3.5912e-01],
        [ 2.4552e-01,  1.0390e+00,  9.0337e-01],
        [-1.2420e-01,  6.6499e-01, -3.0811e-01],
        [-2.1073e+00,  1.4287e+00,  2.0465e+00],
        [-6.0147e-01,  7.4540e-01,  1.2805e-01],
        [-2.8041e-01,  2.2088e+00,  2.3552e-01],
        [-1.0414e+00,  3.2658e+00,  6.9907e-01],
        [-5.6076e-01,  1.6345e+00,  1.4094e+00],
        [ 3.9806e-01,  2.7459e+00, -1.6532e+00],
        [-1.6868e+00,  1.3409e+00,  1.3980e+00],
        [ 2.0372e-01,  8.8576e-01, -2.2725e-01],
        [-8.1249e-01, -6.0255e-01, -1.4021e-01],
        [-1.3986e+00,  1.3940e+00,  7.5124e-01],
        [-8.7571e-02,  1.3620e+00,  2.4153e-01],
        [-6.5623e-01,  6.5204e-01, -1.0896e-03],
        [-1.1603e+00,  2.4945e+00, -1.0434e+00],
        [-1.8323e+00,  1.8438e+00,  1.8582e+00],
        [-8.3961e-01,  1.7859e+00,  9.9688e-01],
        [-2.5129e+00,  9.5670e-01, -7.2308e-01],
        [-5.3816e-01,  1.4215e+00,  7.7240e-02],
        [-1.5013e+00,  1.0074e+00,  1.4272e+00],
        [-1.9534e+00,  1.7424e+00, -1.1125e+00],
        [-4.9908e-01,  1.7542e+00,  1.7859e-01],
        [-1.8422e+00,  1.1032e+00,  4.1764e-01],
        [-2.1931e+00,  1.1016e-01, -1.1527e+00],
        [-1.9057e+00,  1.5447e+00, -5.3962e-01],
        [-4.1726e-01,  1.3073e+00,  1.2873e+00],
        [-1.4092e+00,  1.1365e+00,  2.7062e+00],
        [-3.6340e-01,  1.8133e+00,  1.2261e+00],
        [-1.3814e+00,  1.3423e-02,  1.0037e+00],
        [-1.0303e+00,  1.6343e+00,  1.9402e+00],
        [-1.4170e+00,  1.2080e+00,  8.2341e-01],
        [-1.0150e+00,  2.6065e+00, -1.8588e-01],
        [-1.1735e+00,  2.4202e-01,  6.1296e-01],
        [-7.5775e-01,  1.8877e+00, -2.6562e-01],
        [-2.3268e-01,  1.9988e+00,  7.3577e-01],
        [-7.9080e-01,  2.1441e+00,  1.0376e-01],
        [-9.8880e-01,  1.1960e+00,  1.6366e+00],
        [-1.3398e+00,  3.5122e+00, -4.2691e-01],
        [-1.2671e+00, -1.8347e-01, -7.2621e-01],
        [-5.0855e-01,  1.1867e+00, -4.9561e-01],
        [-6.1102e-01,  1.8307e+00,  6.7116e-01],
        [-7.3375e-01,  1.5916e+00,  1.5306e+00],
        [-6.4225e-01,  1.3293e+00,  9.4967e-01],
        [-4.6857e-01,  4.4325e-01, -6.1285e-01],
        [-1.5372e+00,  1.0028e+00,  4.8779e-01],
        [-1.5109e+00,  1.8845e+00,  1.5536e+00],
        [-2.0950e+00,  1.5143e+00,  4.5380e-01],
        [-6.2340e-02,  1.3820e+00,  1.2321e+00],
        [-2.1628e+00,  3.3487e+00, -1.3231e+00],
        [-1.0814e+00,  7.0148e-01,  1.1041e+00],
        [-7.3826e-01,  3.0360e+00, -3.3029e-01],
        [-2.0352e+00,  1.5889e+00, -9.8431e-01],
        [-1.9688e+00,  2.5834e+00,  2.2660e+00],
        [-1.9706e+00,  4.4692e-01, -1.8323e+00],
        [-1.0231e+00,  1.1103e+00,  1.4088e+00],
        [-9.0558e-01, -5.0356e-02, -4.7865e-01],
        [-5.4019e-02,  2.5757e+00,  4.5132e-01],
        [-1.5207e+00,  2.0214e+00,  1.1096e+00],
        [-7.5940e-01,  1.2115e+00,  6.2720e-01],
        [-1.5567e+00,  3.1342e+00,  8.8279e-01],
        [-2.6056e-01,  6.7098e-01,  1.2596e+00],
        [-4.3619e-01,  3.2375e+00, -2.2523e-02],
        [-6.6647e-01,  5.4489e-01,  6.4622e-01],
        [-8.3294e-01,  1.6975e+00,  1.3231e+00],
        [-1.3981e+00,  1.5408e+00,  1.1147e+00],
        [-2.1032e+00, -1.4525e-01,  8.8115e-01],
        [-1.8364e+00,  1.2528e+00, -1.7517e+00],
        [-1.7344e+00,  9.1595e-01,  1.7784e+00],
        [-2.1407e+00,  1.4192e+00,  7.9300e-01],
        [-8.0709e-01,  2.0811e+00, -2.1571e+00],
        [-1.2802e+00,  2.3197e+00,  3.0095e-01],
        [-1.6475e-02,  1.8839e+00, -8.2482e-01],
        [-2.1964e+00, -2.5488e-01, -1.5762e-01],
        [-3.1666e-01,  2.6264e+00, -3.4822e-01],
        [-5.2064e-01,  5.6753e-01,  1.5781e+00],
        [-6.2564e-01,  8.9521e-01,  1.5941e-01],
        [-1.3111e+00,  3.3731e+00, -1.8002e-01],
        [-1.7174e+00,  1.0856e+00, -1.9075e+00],
        [-1.4920e+00,  1.0896e+00, -1.0375e+00],
        [-7.2441e-01,  9.1565e-01, -2.0983e-01],
        [-1.5303e+00,  1.9662e+00,  1.2462e+00],
        [-1.0000e+00,  7.9283e-01,  1.3185e+00],
        [-2.2184e+00,  1.3197e+00,  1.5065e+00],
        [-2.3481e-01,  2.0932e+00,  6.4925e-01],
        [-8.3558e-01,  8.5687e-01,  1.3421e+00],
        [-2.3114e-01,  1.1347e+00,  1.8058e+00],
        [-8.1440e-01,  1.4300e+00,  9.7299e-01],
        [ 5.2430e-02,  1.2086e+00,  1.8684e-01],
        [-1.1279e+00,  1.9611e+00,  1.2589e+00],
        [-2.3671e+00,  1.1822e+00,  2.9307e-02],
        [-1.7709e-01,  1.4675e+00,  1.8182e+00],
        [-9.7202e-01,  5.3127e-01,  3.2318e+00],
        [ 2.2172e-01,  2.3223e+00,  3.2037e-01],
        [-1.6573e+00, -3.2146e-01, -6.0206e-01],
        [-7.4259e-01,  9.3959e-01,  1.3561e+00],
        [-9.2339e-01,  1.4618e+00,  9.4268e-01],
        [-7.5226e-01,  8.0982e-01,  9.4188e-01],
        [-1.3694e+00,  1.8417e+00,  1.5008e+00],
        [-9.6713e-04,  1.7261e-01,  7.3186e-02],
        [ 2.1142e-01,  1.7794e+00, -1.4865e+00],
        [-9.0582e-01,  1.0513e+00,  1.9420e+00],
        [-8.1257e-01,  1.8793e+00,  1.9618e+00],
        [ 8.8711e-01,  2.6589e+00, -8.6262e-01],
        [-8.9042e-01,  2.1009e+00,  1.5157e+00],
        [-1.0832e+00,  2.2174e+00, -6.3723e-01],
        [-1.7206e+00,  7.4863e-01,  4.9032e-01],
        [-1.6973e+00,  1.1927e+00,  1.5166e+00],
        [-2.6844e-01,  1.7781e+00,  8.9703e-02],
        [-1.3588e+00,  7.4862e-01,  4.8263e-01],
        [-2.2002e+00,  9.4455e-01,  6.5436e-01],
        [-8.1590e-03,  1.5848e+00, -1.8039e-01],
        [-1.2475e+00,  1.6724e+00, -2.1259e+00],
        [-3.1157e-01,  1.0591e+00, -5.8345e-01],
        [-7.1503e-01,  1.2934e+00, -8.8241e-01],
        [-1.8895e+00,  1.3507e+00, -2.0015e+00],
        [-1.1127e-01, -2.9891e-01,  1.4066e-01],
        [-1.9853e+00,  2.4817e+00, -1.9656e+00],
        [-1.7978e+00,  1.2160e+00,  6.9524e-01],
        [-2.7271e+00,  1.3418e+00,  1.7943e+00],
        [-1.0450e+00,  1.5946e+00,  7.0423e-01],
        [-1.0605e+00,  1.1780e+00, -4.3400e-02],
        [-1.2350e+00,  2.0914e+00,  1.6796e+00],
        [-1.2102e+00,  1.6135e+00, -6.9466e-01],
        [-1.2618e+00,  4.9176e-01,  7.8370e-01],
        [-1.0446e+00,  2.8308e+00, -2.7233e+00],
        [-1.6703e+00,  1.2604e+00,  8.5551e-01],
        [-7.0263e-01,  1.3662e+00,  4.0946e-01],
        [-1.7439e+00,  1.2104e+00,  1.3121e+00],
        [-7.4537e-01,  1.9591e+00, -7.6594e-01],
        [-1.0233e+00,  3.4726e+00, -8.6331e-01],
        [-2.0068e+00,  3.2225e+00,  1.8973e+00],
        [-1.7709e+00,  2.3972e-01,  1.9985e+00],
        [-1.3737e+00,  1.8921e+00, -3.4581e-02],
        [-1.5979e+00, -3.8218e-02,  1.8712e+00],
        [-5.0196e-01,  3.8253e-01, -1.9507e+00],
        [-8.8646e-01,  1.7276e+00,  5.6320e-01],
        [-1.3831e+00,  1.6512e+00, -1.1687e+00],
        [-1.3435e+00,  2.6810e+00, -7.3165e-01],
        [-1.6176e+00,  6.0062e-01,  1.5997e+00],
        [-1.1498e+00,  1.6306e+00,  2.0837e+00],
        [-7.3381e-01,  2.3835e+00, -4.6511e-01],
        [-1.5453e+00,  1.2734e+00, -2.6524e+00],
        [-1.5154e+00,  2.2385e+00, -1.0502e+00],
        [-5.3229e-01,  1.5027e+00,  1.6135e+00],
        [-8.5957e-01,  1.8480e+00, -1.2566e+00],
        [-1.0111e+00,  9.4300e-01,  1.5038e+00],
        [-3.6754e-01,  6.1334e-01,  1.5631e+00],
        [-1.5344e+00,  7.0922e-02,  1.2650e+00],
        [ 1.1503e-01,  8.6418e-01,  6.5204e-01],
        [-1.3525e+00,  2.4263e+00,  4.9425e-01],
        [-1.0327e+00, -3.0426e-01, -4.4272e-02],
        [-1.0594e+00,  1.3883e+00,  6.3639e-01],
        [-1.9603e-01,  7.8687e-01,  1.2860e+00],
        [-3.1682e-02,  1.1935e+00, -1.3259e+00],
        [-7.4680e-01,  1.1217e+00,  8.7122e-01],
        [-9.8585e-01,  1.5401e+00,  7.3582e-01],
        [-2.8747e-01,  1.5122e+00,  7.9594e-01],
        [ 1.5301e-01,  1.7826e+00,  1.0083e+00],
        [-8.7741e-01, -5.2225e-01,  1.4403e+00],
        [-1.0340e+00,  2.8945e+00,  2.0969e+00],
        [-1.0212e+00,  6.1192e-01,  3.0672e+00],
        [ 6.3209e-02,  1.9242e+00,  1.1594e-01],
        [-1.6622e+00,  6.6278e-01,  1.4139e+00],
        [-4.6931e-01,  1.7481e+00,  1.2664e-01],
        [-2.0129e+00,  1.8708e+00,  2.5832e+00],
        [-1.5359e+00, -6.9289e-01, -1.1499e+00],
        [-1.1884e+00, -4.9420e-01, -5.5286e-01],
        [-1.0938e+00, -7.0721e-01, -1.3839e+00],
        [-3.3627e-01,  7.2948e-02,  2.1633e+00],
        [ 3.1211e-02,  8.8324e-01,  1.0101e+00],
        [-1.4036e+00,  1.4943e+00,  2.4966e+00],
        [-5.0165e-01,  1.2452e+00,  2.2495e+00],
        [-1.7580e+00,  1.2568e+00,  1.2727e+00],
        [-1.1790e-01, -1.6855e-01,  2.2308e+00],
        [-8.3546e-01,  2.6875e+00,  1.7488e+00],
        [-8.5482e-01,  1.3942e+00,  1.8574e+00],
        [-1.6393e+00,  1.1470e+00,  1.4071e+00],
        [-6.2622e-02,  1.3576e+00,  4.3353e-01],
        [-4.4670e-01,  7.9448e-01,  1.0426e-01],
        [-8.8185e-01,  2.6567e+00, -4.3914e-01],
        [-8.8360e-01,  1.5980e+00,  9.0165e-01],
        [ 1.6058e-01, -8.5354e-01, -7.3357e-01],
        [-5.3870e-01,  1.9401e+00, -9.4271e-01],
        [-1.2146e+00,  1.4175e+00, -4.1329e-01],
        [-1.8496e+00,  1.1049e+00,  7.4771e-01],
        [-6.4166e-01,  3.0333e+00,  6.9684e-01],
        [-5.4604e-01,  5.1700e-01,  5.3900e-01],
        [-5.0687e-01,  6.0669e-01,  1.2926e+00],
        [-1.4064e+00,  1.1155e+00, -2.1139e+00],
        [-1.3178e+00,  1.3249e+00,  1.0392e+00],
        [-8.9252e-03,  1.3418e+00, -7.4444e-01],
        [-1.3000e+00,  5.6085e-01,  6.3251e-01],
        [ 1.2657e-01,  2.2107e+00, -1.5081e-01],
        [-2.0057e-01,  5.2341e-01,  1.5373e+00],
        [ 1.4293e-01,  2.2842e+00, -3.0967e-01],
        [-3.2973e-01,  1.2299e+00,  5.3164e-01],
        [-8.8807e-01,  1.2918e+00,  1.7346e+00],
        [-8.9014e-01,  1.9738e+00, -3.8521e-01],
        [-3.8689e-01,  4.2265e-01,  2.1019e+00],
        [-2.4001e+00,  1.7118e+00,  7.0288e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 03:36:38 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 03:36:38 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 03:36:38 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 03:37:34 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 03:37:52 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 03:37:52 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 03:37:52 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 03:41:51 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 03:42:05 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 03:42:05 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 03:42:05 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 03:42:14 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.294953, Accuracy = 39.06%, MSE = 0.7500
2025-03-01 03:42:14 - INFO - Classwise accuracy : (0: 4.65% (43)), (1: 81.55% (103)), (2: 12.73% (110))
2025-03-01 03:44:21 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 1.267296, Accuracy = 39.67%, MSE = 1.0497
2025-03-01 03:44:21 - INFO - Classwise accuracy : (0: 0.00% (41)), (1: 39.25% (107)), (2: 70.37% (108))
2025-03-01 03:46:03 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 1.488505, Accuracy = 40.61%, MSE = 0.9684
2025-03-01 03:46:03 - INFO - Classwise accuracy : (0: 0.00% (51)), (1: 90.20% (102)), (2: 6.80% (103))
2025-03-01 03:47:58 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 1.592046, Accuracy = 40.70%, MSE = 0.9835
2025-03-01 03:47:58 - INFO - Classwise accuracy : (0: 0.00% (51)), (1: 100.00% (106)), (2: 0.00% (99))
2025-03-01 03:56:50 - INFO - Full sequence shape : (201, 81)
2025-03-01 03:56:50 - INFO - Decoder sequence length : 188
2025-03-01 03:56:50 - INFO - Slope value shape : (188,)
2025-03-01 03:56:50 - INFO - Slope values : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
 0 0 0]
2025-03-01 03:56:50 - INFO - Target bin max : 1
2025-03-01 03:56:50 - INFO - slope classes : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])
2025-03-01 03:56:50 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-03-01 03:56:50 - INFO - Future price : 2
2025-03-01 03:56:50 - INFO - X shape : torch.Size([200, 51])
2025-03-01 03:56:50 - INFO - y shape : torch.Size([3])
2025-03-01 03:56:50 - INFO - y : tensor([0., 0., 1.])
2025-03-01 03:56:50 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 03:56:50 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 03:56:50 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 03:56:50 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 03:56:50 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 03:56:57 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 03:56:57 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 03:56:57 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 03:57:08 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 03:57:08 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 03:57:08 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 03:57:10 - INFO - Output s : tensor([[-1.6202e+00, -1.2858e+00,  5.0660e-01],
        [ 1.1384e+00, -2.0664e+00,  5.2034e-01],
        [ 1.0661e+00, -1.2820e+00, -1.3951e+00],
        [ 3.6155e-01, -2.8483e+00, -5.9273e-01],
        [-8.1739e-01, -1.9285e+00, -2.5401e+00],
        [-5.7434e-01, -1.4862e+00,  3.9503e-01],
        [ 1.8537e-01, -1.4770e+00, -2.7392e+00],
        [-4.5666e-01, -1.4883e+00, -7.0060e-01],
        [-5.8578e-01, -3.4850e+00,  9.0117e-01],
        [ 9.4424e-01, -1.5132e+00, -3.0427e-01],
        [-2.0238e+00, -1.1998e+00,  3.5009e-01],
        [ 1.2245e+00, -2.1404e+00, -1.8281e-01],
        [-2.1624e-01, -2.6049e+00, -2.9502e-01],
        [ 6.7620e-01, -1.1509e+00, -2.3447e-01],
        [ 6.1896e-01, -2.2625e+00,  5.7383e-01],
        [-3.2519e-02, -1.7757e+00, -5.5713e-01],
        [-5.8749e-01, -2.9336e+00, -3.5571e-01],
        [ 9.5361e-02, -1.2193e+00, -4.0068e-02],
        [ 1.2655e+00, -2.2093e+00,  2.9602e-01],
        [-1.1087e+00, -2.2338e+00,  2.9792e-01],
        [ 6.9237e-01, -2.6376e+00, -1.2306e-01],
        [-6.6475e-01, -2.2185e+00, -9.3282e-01],
        [-7.4736e-01, -3.7131e+00,  1.0309e-01],
        [ 1.6380e+00, -1.9510e+00,  4.0384e-03],
        [ 2.9102e-01, -3.4139e+00,  1.2026e+00],
        [-1.8286e+00, -2.5569e+00, -2.2784e-01],
        [-1.3719e+00, -2.2915e+00, -1.5550e+00],
        [-3.1738e-02, -1.0619e+00,  4.7595e-01],
        [ 6.7400e-01, -2.2723e+00, -8.1143e-01],
        [ 4.8211e-01, -1.7904e+00, -1.0221e+00],
        [ 2.1806e-01, -3.0193e+00, -2.1822e-01],
        [ 4.1681e-02, -1.5038e+00, -8.8777e-01],
        [ 4.8149e-01, -2.6356e+00, -2.3688e-01],
        [ 3.0582e-02, -2.1092e+00,  9.2846e-02],
        [-4.6244e-01, -1.0538e+00,  2.9280e-01],
        [-2.2936e+00, -2.3555e+00, -1.1662e+00],
        [ 3.9798e-01, -2.5813e+00, -2.2550e+00],
        [-1.2967e+00, -1.4068e+00,  6.5780e-01],
        [ 2.8110e-01, -3.0667e+00, -7.8266e-01],
        [-9.1713e-01, -1.8941e+00,  2.5854e-02],
        [-2.7444e-01, -4.0548e+00,  4.4108e-01],
        [-2.1264e-01, -2.6227e+00,  2.6855e-01],
        [-5.4594e-02, -2.3878e+00,  9.7897e-01],
        [-5.4080e-01, -2.4911e+00, -6.1007e-01],
        [ 1.2517e-01, -1.4803e+00, -2.4452e-02],
        [ 5.5586e-01, -1.5504e+00, -8.1324e-01],
        [ 2.8553e-02, -2.9150e+00, -2.5693e-01],
        [ 6.9291e-01, -1.5382e+00, -4.8469e-01],
        [-3.3574e-01, -2.5078e+00, -4.6202e-01],
        [-2.4470e+00, -1.4447e+00, -1.4043e+00],
        [-7.6609e-01, -1.5974e+00, -8.6255e-01],
        [-1.0848e+00, -1.6212e+00,  4.2939e-02],
        [-3.3177e-01, -3.1336e+00, -1.1944e+00],
        [ 4.2114e-02, -3.6903e+00,  4.4767e-01],
        [ 1.9727e-01, -2.6063e+00,  8.4312e-01],
        [-1.9195e-01, -1.8203e+00,  1.5069e-01],
        [ 3.7859e-01, -3.1245e+00, -9.6827e-01],
        [-2.1882e-01, -1.4644e+00, -1.3313e+00],
        [ 2.0417e-01, -2.2920e+00,  2.8244e-02],
        [ 1.5799e-01, -1.8519e+00, -1.0275e+00],
        [ 2.5425e-01, -2.4407e+00, -6.0024e-01],
        [-7.1404e-01, -2.5510e+00,  8.4627e-01],
        [-7.0228e-01, -1.4218e+00, -6.9749e-01],
        [ 8.0070e-02, -3.1999e+00,  5.2973e-02],
        [-1.2476e+00, -2.0203e+00, -1.5322e+00],
        [ 1.5712e-01, -3.6623e+00, -7.3547e-01],
        [ 2.8105e-01, -3.3011e+00, -5.5972e-01],
        [ 1.4752e-01, -7.6387e-01, -2.1818e+00],
        [-1.2354e+00, -2.2317e+00, -1.5756e+00],
        [-1.6335e+00, -1.9900e+00, -1.4594e+00],
        [ 7.4248e-02, -1.8449e+00, -9.6608e-01],
        [-1.2231e-01, -2.2795e+00, -4.0343e-01],
        [ 1.2348e+00, -1.6300e+00,  3.8424e-01],
        [ 5.5065e-01, -3.9538e+00, -1.1273e+00],
        [ 4.9366e-01, -2.3860e+00,  8.9469e-01],
        [-2.4520e+00, -1.2845e+00, -1.2264e+00],
        [-9.4945e-01, -3.1303e+00, -3.0825e-01],
        [-7.1753e-01, -1.2767e+00, -1.3790e+00],
        [-4.4706e-01, -2.3018e+00, -2.1069e-01],
        [ 3.4335e-01, -2.4267e+00, -2.9348e-01],
        [ 1.6390e-01, -2.9566e+00, -7.3019e-01],
        [ 1.8261e-01, -1.1562e+00,  4.7548e-01],
        [-4.2823e-01, -1.2680e+00, -2.4288e+00],
        [-3.1272e-02, -2.8445e+00, -5.2330e-01],
        [ 9.5748e-01, -1.0463e-02, -8.3505e-02],
        [-8.8019e-01, -9.4798e-01, -1.4883e+00],
        [-5.0461e-01, -2.2048e+00, -1.3291e+00],
        [-6.6821e-01, -1.0388e+00, -2.4640e-01],
        [ 7.0368e-01, -1.8906e+00,  8.8981e-01],
        [ 1.3381e+00, -3.2412e+00, -1.7809e-01],
        [ 3.6215e-01, -2.8134e+00,  4.9806e-01],
        [-2.0490e+00, -1.4996e+00,  9.2206e-02],
        [ 1.3379e-02, -3.0845e+00, -9.5274e-01],
        [-3.6116e-01, -4.1014e+00, -3.2819e-01],
        [ 5.6406e-01, -1.9732e+00,  5.6110e-01],
        [ 9.4125e-01, -3.1737e+00,  3.6936e-01],
        [ 5.7083e-01, -2.6266e+00, -3.1685e-01],
        [ 5.8083e-01, -2.9574e+00,  2.4639e-01],
        [-3.5540e-01, -3.1204e+00, -5.4954e-01],
        [ 1.0157e+00, -2.5773e+00,  9.3456e-01],
        [ 6.1473e-01, -3.5025e+00,  6.5547e-01],
        [ 1.1468e+00, -2.5239e+00,  1.1460e+00],
        [ 9.4798e-02, -1.1799e+00, -8.4209e-01],
        [-2.1820e+00, -7.5412e-01, -7.9035e-01],
        [ 1.3396e-01, -2.4148e+00, -1.2395e+00],
        [-1.2090e+00, -2.1874e+00, -6.9139e-01],
        [ 7.8389e-01, -2.6760e+00,  1.5716e-01],
        [ 5.8842e-01, -1.3902e+00, -2.1816e+00],
        [ 7.7013e-01, -2.1947e+00,  6.0590e-01],
        [-3.2126e-01, -3.7368e+00,  2.7315e-01],
        [ 1.3793e+00, -1.5044e+00, -3.7239e-01],
        [ 9.7923e-01, -2.1415e+00, -3.6868e-01],
        [-1.6487e-01, -2.0347e+00,  1.8328e-02],
        [ 9.4074e-01, -1.9534e+00, -6.7176e-01],
        [-7.8294e-01, -3.1554e+00,  2.7970e-01],
        [ 8.2402e-01, -1.6750e+00, -7.7484e-01],
        [ 7.6368e-01, -2.9278e+00, -6.5128e-01],
        [-4.5883e-01, -2.8139e+00, -1.9483e+00],
        [-5.6695e-02, -2.4357e+00, -5.0406e-01],
        [-2.7346e+00, -2.2439e+00, -9.2512e-01],
        [ 3.9392e-01, -3.4387e+00, -3.0492e-01],
        [-1.7348e+00, -1.2013e+00, -6.2553e-01],
        [-1.6518e+00, -1.7937e+00, -9.2797e-01],
        [-5.7605e-01, -2.0646e+00, -1.5184e-02],
        [ 3.8288e-01, -2.1049e+00, -7.8790e-02],
        [-2.0508e-01, -2.3724e+00, -1.3842e+00],
        [-7.1567e-01, -1.7429e+00, -9.5154e-01],
        [ 3.1984e-01, -2.1838e+00,  1.4407e-02],
        [ 2.0439e+00, -1.2892e+00, -7.9011e-01],
        [ 3.9265e-01, -1.3491e+00, -4.9687e-01],
        [-8.9246e-01, -2.6552e+00, -1.3145e-01],
        [ 4.5821e-01, -2.4648e+00,  2.4016e-01],
        [ 1.6263e+00, -2.6662e+00, -8.7224e-01],
        [ 8.7520e-03, -3.0442e+00, -1.5397e+00],
        [-1.1632e+00, -1.4767e+00, -8.8848e-01],
        [ 1.1779e+00, -1.6359e+00, -2.9932e+00],
        [ 1.9643e-01, -2.9187e+00,  1.2167e-02],
        [ 1.5807e-01, -1.6279e+00,  5.5273e-01],
        [ 1.1312e+00, -3.2613e+00, -7.4146e-01],
        [-1.6701e+00, -2.1702e+00, -3.5720e-01],
        [-2.0646e+00, -1.9478e+00, -4.2557e-01],
        [ 1.3971e+00, -2.4082e+00, -6.2350e-01],
        [-4.2063e-01, -1.7881e+00, -2.3734e+00],
        [-4.0056e-01, -2.2430e+00, -6.4758e-01],
        [ 1.0439e+00, -2.7029e+00, -8.8198e-01],
        [ 9.6097e-02, -1.5764e+00,  9.7148e-01],
        [-2.6673e+00, -1.8922e+00,  3.8025e-02],
        [-7.0049e-01, -2.7032e+00,  3.7138e-01],
        [-6.3913e-01, -8.9643e-01, -1.1578e+00],
        [-6.1490e-01, -2.1884e+00, -4.5975e-01],
        [-9.9841e-01, -3.4259e+00,  6.0415e-01],
        [-1.3365e+00, -1.5418e+00,  2.1728e-01],
        [ 7.4505e-01, -1.2996e+00, -9.0746e-01],
        [-4.4570e-01, -8.1523e-01, -4.0264e-02],
        [-9.0313e-01, -1.4555e+00, -2.4856e-01],
        [ 9.6435e-01, -1.9113e+00, -4.7396e-01],
        [-5.5417e-01, -2.4932e+00, -1.4080e+00],
        [ 8.8979e-04, -1.3681e+00, -9.4647e-01],
        [ 3.8678e-01, -2.8901e+00, -1.9683e+00],
        [ 9.3221e-01, -2.7756e+00, -3.1243e-01],
        [ 1.7824e-01, -2.9800e+00, -1.9761e+00],
        [ 1.0242e+00, -3.5070e+00, -1.7364e+00],
        [ 1.2730e-01, -2.8590e+00, -4.5213e-01],
        [-6.1274e-01, -1.9778e+00, -3.0494e-01],
        [ 2.4936e-01, -3.0702e+00, -1.5196e+00],
        [-7.6613e-01, -2.1912e+00,  6.9208e-01],
        [-5.8901e-01, -2.9494e+00, -5.9536e-01],
        [ 9.4224e-01, -1.4784e+00, -6.7801e-01],
        [ 1.0764e+00, -3.9819e+00, -9.8713e-01],
        [-7.4843e-01, -1.2374e+00, -2.2228e+00],
        [ 7.8798e-01, -2.1335e+00, -1.6831e+00],
        [-7.8883e-01, -2.2224e+00,  6.7446e-02],
        [ 4.2971e-01, -2.3718e+00, -1.7753e+00],
        [-1.0836e+00, -1.9619e+00, -1.2951e+00],
        [ 6.5602e-01, -1.0680e+00, -4.9548e-01],
        [-5.6082e-01, -1.9099e+00, -1.9774e+00],
        [-7.4337e-02, -1.1689e+00,  3.1350e-01],
        [ 6.4035e-01, -1.7363e+00, -5.3906e-01],
        [ 2.1155e+00, -1.3515e+00, -6.7608e-01],
        [-8.7320e-01, -2.0136e+00,  5.5920e-01],
        [-2.0433e-01, -3.4275e+00,  5.4944e-01],
        [ 5.8102e-01, -2.7380e+00, -7.6380e-01],
        [ 1.2287e+00, -2.3835e+00,  7.1229e-01],
        [-2.3082e+00, -2.3295e+00, -5.1301e-01],
        [ 7.5674e-01, -2.4022e+00,  5.7697e-01],
        [-2.3356e+00, -1.9620e+00, -9.8369e-01],
        [ 1.6008e+00, -3.2232e+00,  1.5825e+00],
        [-1.2854e+00, -1.4109e+00, -3.3631e-01],
        [-8.0822e-01, -3.3005e+00, -5.7632e-02],
        [-1.6699e-01, -1.3550e+00, -1.6083e-01],
        [-1.5096e-01, -3.1267e+00,  9.5565e-01],
        [-7.6060e-01, -2.1339e+00,  3.6992e-01],
        [ 8.0545e-01, -2.0855e+00, -6.2812e-01],
        [ 8.0419e-01, -3.1375e+00, -1.0481e+00],
        [ 5.8759e-01, -2.4077e+00, -7.6134e-01],
        [ 9.6229e-01, -2.5818e+00, -1.8057e-01],
        [-3.9106e-02, -1.0485e+00, -8.1511e-01],
        [-2.0226e-01, -2.6905e+00,  5.4164e-01],
        [ 3.3176e-03, -1.6258e+00, -6.8063e-01],
        [ 3.8859e-02, -2.7294e+00, -2.4230e-01],
        [ 6.9310e-01, -2.1363e+00, -1.3594e+00],
        [-5.5624e-02, -9.6077e-01, -1.6774e+00],
        [ 1.6217e-01, -2.7244e+00, -1.0859e+00],
        [-1.1254e+00, -2.3472e+00,  3.1490e-03],
        [-6.4599e-01, -1.9740e+00, -9.2257e-01],
        [-4.1146e-01, -2.2296e+00,  5.2652e-01],
        [-5.6810e-01, -3.1192e+00, -9.5793e-02],
        [ 3.2750e-01, -2.5101e+00, -1.0353e+00],
        [-5.8032e-01, -1.2275e+00, -2.4028e+00],
        [-3.9726e-01, -2.6381e+00, -5.2427e-01],
        [ 7.8123e-01, -2.7754e+00,  2.8066e-01],
        [ 4.8206e-01, -1.7584e+00,  1.2574e-01],
        [ 2.1156e-01, -2.3950e+00, -5.9059e-01],
        [-5.5194e-01, -2.6704e+00,  3.4090e-01],
        [ 6.7340e-01, -2.3055e+00, -7.0974e-01],
        [-1.8040e+00, -1.5199e+00, -2.8413e-02],
        [-9.8731e-02, -2.5470e+00, -1.9443e+00],
        [-1.5125e+00,  6.5926e-01, -7.4214e-01],
        [-1.3094e+00, -1.3243e+00, -3.6186e-01],
        [-8.9724e-01, -2.1399e+00, -5.5784e-01],
        [ 3.3865e-01, -3.0367e+00,  1.4973e-01],
        [ 2.3127e-01, -2.0684e+00, -5.1733e-01],
        [-1.6786e+00, -1.1971e+00, -2.0872e+00],
        [-1.1706e+00, -2.9804e+00,  1.3232e+00],
        [-1.9198e+00, -1.5011e+00, -5.3973e-01],
        [-3.1604e-02, -3.3594e+00, -2.2574e-01],
        [-4.1258e-01, -2.0180e+00,  3.4073e-01],
        [ 3.3030e-01, -4.2474e+00,  1.0912e+00],
        [-7.5850e-01, -3.3714e+00, -6.3076e-02],
        [-1.0466e+00, -2.9285e+00,  8.3271e-01],
        [-1.1502e+00, -2.5044e+00, -8.7621e-02],
        [ 1.5652e-01, -2.2108e+00,  1.0186e+00],
        [-1.8470e-01, -2.9315e+00, -2.3470e+00],
        [-1.8132e-01, -1.3005e+00, -3.9832e-01],
        [-3.9608e-01, -1.2974e+00, -2.0736e-01],
        [-2.4097e-01, -2.5412e+00, -8.1965e-01],
        [-1.6983e-01, -2.4742e+00,  8.5451e-02],
        [ 1.0876e+00, -2.3491e+00, -7.6692e-01],
        [ 5.3604e-01, -3.0083e+00,  7.3537e-02],
        [-8.6718e-01, -7.8114e-01, -3.8539e-01],
        [-1.9766e+00, -9.2753e-01, -1.8147e-01],
        [-1.6864e-01, -1.9137e+00, -9.3474e-01],
        [ 5.6637e-02, -1.6245e+00,  7.8331e-01],
        [-1.9356e+00, -1.7594e+00, -8.4287e-01],
        [-2.4042e+00, -1.9112e+00, -1.0410e+00],
        [-3.8028e-04, -3.0635e+00,  1.1432e+00],
        [-5.3784e-01, -2.4905e+00, -2.3979e-01],
        [ 4.9568e-01, -2.5040e+00,  9.6225e-01],
        [-2.5333e-01, -3.1799e+00, -5.9311e-01],
        [-2.6293e+00, -1.6592e+00,  3.9587e-01],
        [-5.2483e-01, -2.0267e+00,  3.5670e-01],
        [-8.3454e-01, -2.7734e+00, -1.1237e+00],
        [-7.7385e-01, -2.2300e+00, -8.5179e-01],
        [-6.9317e-01, -2.3117e+00, -1.0671e+00],
        [-8.3746e-01, -1.9600e+00,  8.2857e-01],
        [ 6.9331e-01, -3.5243e+00, -4.4744e-02]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 03:57:10 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 03:57:10 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 03:57:10 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 03:57:22 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.170986, Accuracy = 26.95%, MSE = 1.7031
2025-03-01 03:57:22 - INFO - Classwise accuracy : (0: 56.90% (58)), (1: 0.00% (101)), (2: 37.11% (97))
2025-03-01 03:59:31 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 1.288037, Accuracy = 35.55%, MSE = 1.2933
2025-03-01 03:59:31 - INFO - Classwise accuracy : (0: 10.00% (40)), (1: 0.89% (112)), (2: 82.69% (104))
2025-03-01 04:01:05 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 1.455207, Accuracy = 33.61%, MSE = 1.3860
2025-03-01 04:01:05 - INFO - Classwise accuracy : (0: 89.58% (48)), (1: 2.17% (92)), (2: 6.03% (116))
2025-03-01 04:02:41 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 0.997171, Accuracy = 34.74%, MSE = 1.3281
2025-03-01 04:02:41 - INFO - Classwise accuracy : (0: 0.00% (43)), (1: 0.00% (101)), (2: 100.00% (112))
2025-03-01 04:03:59 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 0.868708, Accuracy = 35.89%, MSE = 1.2888
2025-03-01 04:03:59 - INFO - Classwise accuracy : (0: 5.13% (39)), (1: 0.00% (108)), (2: 97.25% (109))
2025-03-01 04:04:58 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 0.937975, Accuracy = 36.37%, MSE = 1.2755
2025-03-01 04:04:58 - INFO - Classwise accuracy : (0: 58.00% (50)), (1: 0.00% (90)), (2: 43.10% (116))
2025-03-01 04:05:52 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 0.893997, Accuracy = 37.33%, MSE = 1.2435
2025-03-01 04:05:52 - INFO - Classwise accuracy : (0: 0.00% (49)), (1: 0.00% (102)), (2: 99.05% (105))
2025-03-01 04:07:05 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 1.011557, Accuracy = 35.95%, MSE = 1.2977
2025-03-01 04:07:05 - INFO - Classwise accuracy : (0: 0.00% (38)), (1: 0.00% (113)), (2: 100.00% (105))
2025-03-01 04:07:26 - INFO - Epoch 2: Train Loss=1.1092, Train Acc=36.18%, Train MSE=1.2902
2025-03-01 04:14:18 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 04:14:24 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 04:14:24 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 04:14:24 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 04:14:29 - INFO - Epoch : 2 , Batch [ 0 / 75 ] : Loss = 0.881071, Accuracy = 42.58%, MSE = 1.1602
2025-03-01 04:14:29 - INFO - Classwise accuracy : (0: 0.00% (49)), (1: 0.00% (97)), (2: 99.09% (110))
2025-03-01 04:16:07 - INFO - Epoch : 2 , Batch [ 10 / 75 ] : Loss = 0.874770, Accuracy = 40.06%, MSE = 1.1428
2025-03-01 04:16:07 - INFO - Classwise accuracy : (0: 16.33% (49)), (1: 0.00% (111)), (2: 97.92% (96))
2025-03-01 04:17:40 - INFO - Epoch : 2 , Batch [ 20 / 75 ] : Loss = 0.861778, Accuracy = 40.22%, MSE = 1.1486
2025-03-01 04:17:40 - INFO - Classwise accuracy : (0: 6.82% (44)), (1: 0.00% (108)), (2: 99.04% (104))
2025-03-01 04:24:43 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 04:24:43 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 04:24:43 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 04:24:43 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 04:24:43 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 04:24:49 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 04:24:49 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 04:24:49 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 04:25:05 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 04:25:05 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 04:25:05 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 04:25:06 - INFO - Output s : tensor([[ 5.5107e-01, -6.8591e-01, -2.8189e+00],
        [ 2.1163e+00,  1.1914e+00, -2.4157e+00],
        [ 3.0553e-01,  7.0814e-02, -1.5003e+00],
        [ 9.2806e-01, -7.3737e-01, -3.4829e+00],
        [ 2.1199e+00,  1.0934e+00, -9.3981e-01],
        [ 1.6924e+00, -3.4642e-01, -3.2026e+00],
        [ 2.6606e+00, -3.6869e-01, -1.6970e+00],
        [ 2.2250e+00, -2.3781e-02, -3.9448e+00],
        [ 1.6491e+00, -1.7519e+00, -4.8419e-01],
        [ 1.6680e+00,  5.9001e-02, -2.5108e+00],
        [ 5.8731e-01, -5.8478e-01, -2.1107e+00],
        [-4.3562e-01,  1.7121e+00, -1.2336e+00],
        [ 2.5670e+00,  7.2218e-01, -2.0566e+00],
        [ 1.2044e-01, -4.1795e-01, -1.6478e+00],
        [ 8.4931e-01, -9.8069e-01, -1.3559e+00],
        [-1.2138e-02, -7.6894e-01, -1.8018e+00],
        [ 1.1895e+00, -2.9264e-01, -2.8615e+00],
        [ 3.1225e-01, -1.4293e+00, -2.7356e+00],
        [ 2.8074e-01,  2.0182e+00, -2.5416e+00],
        [ 1.6373e+00,  4.8718e-01, -2.1610e+00],
        [ 8.3472e-01, -4.3252e-01, -1.1285e+00],
        [ 2.3333e+00, -4.6622e-01, -1.7530e+00],
        [ 1.2459e+00,  1.1884e-02, -1.1341e+00],
        [ 7.3777e-01, -1.2198e-03, -3.4626e+00],
        [ 7.3090e-01, -3.1247e-01, -1.5845e+00],
        [ 1.9852e+00,  1.2582e+00, -2.5020e+00],
        [ 2.0784e-02, -2.5005e-02, -1.2004e+00],
        [ 8.9526e-02,  4.7230e-01, -1.7525e+00],
        [-1.3936e-01, -2.2779e+00, -6.9013e-01],
        [ 9.4592e-01, -2.9105e-01, -2.6928e+00],
        [-5.8942e-02,  1.1414e-02, -2.3845e+00],
        [ 1.2694e+00, -2.1348e+00, -2.2744e+00],
        [ 2.7734e+00,  2.7417e-01, -2.0529e+00],
        [ 1.7441e+00,  1.3590e+00, -2.4591e+00],
        [ 1.0457e+00, -6.9822e-01, -1.5121e+00],
        [ 8.4835e-01,  1.3289e-01, -1.6932e+00],
        [ 2.5609e+00,  1.8260e+00, -9.4061e-01],
        [ 1.3379e+00,  1.5245e+00, -1.1020e+00],
        [ 4.7309e-01, -1.2652e+00, -2.0684e+00],
        [-1.3582e-01, -9.9166e-01, -2.6086e+00],
        [ 1.7796e+00,  7.3989e-01, -3.1363e+00],
        [ 6.6650e-01, -1.5688e+00, -3.3476e+00],
        [ 2.1945e+00, -4.2952e-02, -2.5908e+00],
        [ 8.4478e-01,  9.6581e-01, -2.2911e+00],
        [ 1.3599e+00,  1.5251e+00, -2.3748e+00],
        [ 2.6097e+00, -6.2488e-01, -6.6250e-01],
        [ 9.7855e-01,  2.9333e+00, -2.5595e+00],
        [ 1.5934e+00, -4.9009e-01, -8.1986e-01],
        [ 2.3964e+00,  1.2857e+00, -1.1949e+00],
        [ 2.0540e+00,  2.2937e+00, -1.9544e+00],
        [ 1.5567e+00,  2.0522e-01,  4.6781e-03],
        [-1.6196e-01, -5.7592e-01, -1.9431e+00],
        [ 2.9474e+00,  2.7740e+00, -5.9096e-02],
        [ 7.7387e-01,  3.2209e-01, -2.2880e+00],
        [ 1.0722e+00, -3.2996e-01, -2.2047e+00],
        [ 6.6322e-01, -2.7255e-01, -2.5069e+00],
        [ 8.9693e-01,  7.5076e-01, -2.3601e+00],
        [ 6.6309e-01, -3.8187e-01,  9.5041e-02],
        [ 1.1565e+00, -4.4506e-02, -8.2500e-01],
        [ 2.2582e-01, -7.9317e-01, -4.9933e-01],
        [ 2.1486e+00, -1.8474e+00, -1.8936e+00],
        [ 1.1685e+00, -3.1151e-02, -5.1229e-01],
        [ 1.7342e+00,  1.8228e+00, -2.4757e+00],
        [ 8.2869e-01, -1.7466e-01, -2.4359e+00],
        [ 1.8921e+00, -1.2890e+00, -1.3150e+00],
        [-4.0446e-02, -1.5108e-01, -1.5783e+00],
        [ 4.6352e-01,  1.0745e+00, -6.4233e-01],
        [-9.0088e-02,  3.2156e-01, -3.4681e+00],
        [ 2.6625e+00, -7.8500e-01, -1.0175e+00],
        [ 3.9560e+00,  1.2854e+00, -7.3153e-01],
        [ 1.9371e+00, -5.9707e-01, -1.9435e-01],
        [ 8.9813e-01, -1.3618e+00, -2.8503e+00],
        [ 1.7575e+00, -2.9686e-01, -2.3559e+00],
        [ 1.7573e-01,  2.2086e-01, -5.1992e-02],
        [ 1.5178e+00,  7.9134e-01, -1.8040e+00],
        [ 7.6588e-01,  6.2500e-01, -2.2394e+00],
        [ 2.3295e+00, -2.4005e-01, -2.6696e+00],
        [ 2.5857e+00,  5.8856e-01, -5.2658e-01],
        [ 1.0888e-01, -6.6620e-01, -3.1417e+00],
        [ 1.2872e+00, -1.9324e+00, -3.4651e+00],
        [ 2.3622e+00,  2.0723e+00, -1.8744e+00],
        [-3.9349e-02,  7.6582e-01, -1.5432e+00],
        [ 1.8691e+00, -9.8742e-01, -1.4571e+00],
        [-3.5690e-01, -6.0227e-01, -3.1830e+00],
        [ 1.4862e+00, -1.4237e+00, -3.4995e+00],
        [ 1.8014e-01, -4.1886e-01, -1.9741e+00],
        [ 1.9518e-01, -8.6997e-01, -1.7516e+00],
        [ 1.7027e+00, -6.2548e-01, -1.8803e+00],
        [ 1.5991e+00, -3.7569e-01, -8.2095e-01],
        [ 3.8811e+00,  2.3220e+00, -9.7547e-01],
        [ 1.1901e+00, -5.7720e-01, -2.6792e+00],
        [ 7.4336e-01, -1.4019e-01, -2.8837e+00],
        [ 1.5957e+00,  6.0250e-01, -1.6738e+00],
        [ 2.6561e+00,  2.7930e+00, -7.1758e-01],
        [ 1.0731e+00, -5.7264e-01, -1.0219e+00],
        [ 1.9589e+00, -6.1863e-01, -1.7485e+00],
        [ 1.8234e+00,  2.6294e-01, -3.6567e+00],
        [ 6.4406e-01,  4.4539e-01, -2.7981e+00],
        [ 2.6647e+00, -1.9247e-01, -2.7174e+00],
        [ 9.2887e-01,  4.8996e-01, -3.3997e+00],
        [ 1.6505e+00, -2.4844e-01, -9.8890e-01],
        [ 1.1836e+00, -2.8030e-01, -2.8244e+00],
        [-2.6265e-01,  6.7000e-01, -7.8472e-01],
        [ 1.4157e+00,  5.3637e-01, -2.5259e+00],
        [ 1.2994e+00, -3.2687e-01, -2.7218e+00],
        [ 1.0763e+00,  1.3707e+00, -2.7192e+00],
        [ 9.0623e-01, -5.4931e-01, -2.6719e+00],
        [ 1.6757e+00,  2.3161e-01, -1.8882e+00],
        [ 1.3630e+00,  7.2172e-01, -2.6214e+00],
        [ 1.0324e+00,  6.4553e-01, -2.6123e+00],
        [ 1.1585e+00, -1.2852e+00, -2.5069e+00],
        [ 6.5564e-01, -6.3514e-01, -1.7448e+00],
        [ 1.0447e+00, -2.8832e-01, -1.5579e+00],
        [ 9.6848e-01,  6.2576e-02, -3.0965e+00],
        [ 1.5131e-01, -2.8859e-01, -1.3845e+00],
        [ 1.0331e+00,  1.3258e-01, -1.8758e+00],
        [-1.1456e-01, -1.6688e+00, -2.3232e-01],
        [ 1.0980e+00,  1.6701e+00, -1.2336e+00],
        [ 1.5648e+00,  4.5724e-01, -3.1711e+00],
        [ 5.6469e-01, -1.4046e+00, -9.7777e-01],
        [ 8.1668e-01, -7.8392e-01, -2.9187e+00],
        [ 2.3929e+00, -6.9056e-01, -2.6783e+00],
        [ 4.1238e+00,  1.4013e+00, -3.6094e-01],
        [ 2.0518e-01,  2.1022e+00, -2.9583e+00],
        [ 1.2653e+00,  8.3557e-01, -2.1092e+00],
        [ 3.9790e-01, -1.1394e+00, -1.3696e+00],
        [ 8.0445e-01,  4.6970e-01, -1.9935e+00],
        [ 6.8327e-01, -1.1947e+00, -1.7689e+00],
        [ 9.9552e-01, -8.2873e-03, -2.4545e+00],
        [ 3.9787e+00,  1.6171e+00, -1.2090e+00],
        [ 1.6596e+00,  2.1155e+00, -1.9427e+00],
        [ 4.4016e-01,  9.6418e-01, -1.8741e+00],
        [ 2.0693e+00,  1.6412e+00, -7.5125e-01],
        [ 1.2901e+00, -2.4247e-01, -2.8677e+00],
        [ 8.5130e-01,  8.9715e-01, -2.1870e+00],
        [ 1.3657e+00,  7.9584e-01, -8.2008e-01],
        [ 1.2654e+00, -1.6506e+00, -2.1530e+00],
        [ 8.5542e-02,  6.3339e-02, -1.7929e+00],
        [ 1.1847e+00,  6.4696e-01, -2.2096e+00],
        [ 1.0230e+00, -9.0394e-02, -1.3554e+00],
        [ 1.6971e+00, -1.3303e+00, -1.8252e+00],
        [-5.4554e-02,  3.2788e-01, -1.9022e+00],
        [ 1.0405e+00, -1.0591e+00, -3.2761e+00],
        [ 6.4488e-01, -1.3890e+00, -2.4960e+00],
        [-3.9159e-01,  1.2732e+00, -7.8974e-01],
        [ 3.4272e-01,  2.7723e-01,  6.4021e-02],
        [ 3.1394e+00,  1.7332e+00,  8.3853e-03],
        [ 1.6120e-01,  1.6829e-01,  5.1408e-01],
        [ 1.5484e+00, -1.2639e-01, -2.3433e+00],
        [ 6.0767e-01, -6.5480e-01, -2.6366e+00],
        [-7.4398e-01, -5.6874e-01, -2.2736e+00],
        [ 1.0368e-01, -1.3390e+00, -2.2032e+00],
        [ 7.5934e-01,  4.0393e-01, -3.0401e+00],
        [ 1.7357e+00,  8.6948e-01, -3.0157e+00],
        [ 7.0576e-01, -1.1511e+00, -1.4361e+00],
        [ 1.7221e+00,  1.5871e+00, -1.8408e+00],
        [ 2.7072e+00,  2.6406e-02, -1.7943e+00],
        [ 2.8918e+00, -4.4262e-01, -1.6791e+00],
        [ 1.2885e+00, -1.2541e-01, -1.1820e+00],
        [ 8.7360e-01,  6.3505e-01, -1.7393e+00],
        [ 3.1488e+00,  9.4629e-01, -6.4888e-01],
        [ 1.8502e+00, -5.5140e-01, -6.9495e-01],
        [ 1.4995e+00, -4.5197e-01, -3.0830e+00],
        [ 3.1903e-01, -9.8500e-01, -1.4278e+00],
        [ 1.5718e+00, -3.2161e-01, -3.4309e+00],
        [ 1.1921e+00,  8.1509e-02, -2.7363e+00],
        [ 8.1528e-02, -7.2178e-01, -3.3028e+00],
        [ 2.9317e+00,  6.8623e-01, -6.5704e-01],
        [ 1.8859e+00,  7.4397e-01, -3.0823e+00],
        [ 1.0212e+00,  3.7698e-02, -2.4327e+00],
        [ 1.3451e+00,  5.4913e-01, -2.8897e+00],
        [ 4.0302e+00,  1.8998e+00, -9.2944e-01],
        [ 1.5515e+00,  1.1872e+00, -2.3852e+00],
        [ 6.3257e-01,  1.6554e+00, -9.5581e-01],
        [ 3.2346e+00,  8.3966e-01, -1.9374e-01],
        [-1.7834e-01, -1.0084e+00, -2.4357e+00],
        [ 1.8877e+00,  4.4729e-01, -4.8515e-01],
        [ 1.6779e+00,  1.0510e+00, -1.5705e+00],
        [-2.3327e-01, -1.5289e+00, -2.4933e+00],
        [ 7.5329e-01, -1.5262e+00, -2.8579e+00],
        [ 2.3945e+00,  3.6051e-01, -2.4842e+00],
        [ 2.0681e-01,  2.9423e-01, -1.9435e+00],
        [ 6.6976e-01, -5.6414e-01, -2.7173e+00],
        [-1.9243e-01, -1.6593e+00, -1.3354e+00],
        [ 3.0042e+00,  1.8762e+00, -1.6175e+00],
        [ 1.1627e+00, -1.2841e-01, -2.0204e+00],
        [ 1.5925e+00, -2.6403e+00, -1.5893e+00],
        [ 1.9018e+00, -1.0518e+00, -1.8810e+00],
        [ 2.6891e+00,  9.0811e-01,  1.0292e-01],
        [ 7.8764e-01, -1.1138e+00, -1.0777e+00],
        [-1.9064e-01, -7.3187e-01, -3.2865e+00],
        [ 1.2421e+00,  1.0035e+00, -3.6223e+00],
        [ 1.5581e+00,  1.0969e+00, -5.3316e-01],
        [ 6.2183e-01, -7.3690e-01, -3.4762e+00],
        [ 1.6557e+00, -8.4255e-01, -2.2156e+00],
        [ 3.3462e-01, -4.3490e-01, -5.1859e-01],
        [ 1.1153e+00,  8.1529e-01, -2.3136e+00],
        [ 2.2996e+00,  1.4861e+00, -1.7280e+00],
        [-1.5706e-01, -3.7372e-01, -2.8867e+00],
        [ 9.5528e-01, -1.9194e+00, -1.7899e+00],
        [ 2.3219e+00,  2.8332e-01, -1.6081e+00],
        [-1.2258e-01, -2.2046e+00, -1.4297e+00],
        [ 6.9444e-01, -1.0863e+00, -2.2183e+00],
        [-5.5167e-01,  1.0634e+00, -1.2032e+00],
        [ 1.5476e-01, -3.0679e+00, -3.0171e+00],
        [ 1.4817e-02, -9.1728e-01, -3.7672e+00],
        [ 6.8181e-01, -1.4850e+00, -1.1847e+00],
        [ 1.3193e+00,  8.7438e-01, -2.0014e+00],
        [-1.2293e-01, -8.3893e-01, -2.3807e+00],
        [ 2.0487e+00, -9.5179e-01, -2.4299e+00],
        [ 9.5042e-01,  2.8253e-01, -1.9991e+00],
        [ 2.6486e-01, -9.7888e-01, -2.1177e+00],
        [ 1.1507e+00, -1.6537e-01, -1.1068e+00],
        [ 8.1257e-01, -7.3171e-01, -1.2272e+00],
        [ 1.2458e+00,  4.0627e-01, -1.5787e+00],
        [-2.0938e-01,  3.9616e-01, -7.1131e-01],
        [ 1.0210e+00, -8.6868e-01, -1.2562e+00],
        [ 4.3134e-01, -8.2786e-01, -1.1027e+00],
        [-6.5991e-03, -4.8859e-01, -2.9067e+00],
        [ 1.5125e+00,  8.5362e-01, -1.7381e+00],
        [ 6.3555e-01, -3.0399e-02, -1.9470e+00],
        [ 2.0237e+00,  1.3546e+00, -1.6682e+00],
        [ 1.9155e+00, -3.0539e-01, -2.8811e+00],
        [ 1.4952e+00,  4.8752e-01, -3.3639e+00],
        [ 6.8702e-01, -1.4937e+00, -2.3392e+00],
        [ 1.1974e+00,  3.3703e-01, -2.2943e+00],
        [ 2.1161e+00, -7.8144e-01, -2.0611e+00],
        [ 2.5336e-01,  8.6995e-01, -1.4380e+00],
        [ 8.9222e-01, -7.7934e-01, -1.7609e+00],
        [ 7.1270e-01, -4.0273e-01, -2.7467e+00],
        [ 1.5943e+00, -2.6802e-01, -3.3199e+00],
        [ 1.0295e+00,  4.7125e-01, -1.4508e+00],
        [-3.6117e-01,  2.5357e-01, -1.0125e+00],
        [ 1.2937e+00,  4.4415e-01, -2.7512e+00],
        [ 2.7485e-01, -1.2039e+00, -1.5643e+00],
        [ 6.6349e-01, -1.8091e+00, -2.8298e+00],
        [-5.0395e-01,  4.6317e-01, -2.3642e+00],
        [ 1.9548e+00,  1.4151e+00, -2.4482e+00],
        [-3.4158e-01, -7.8967e-01, -1.9757e+00],
        [ 7.9462e-01, -4.9066e-02, -1.3047e+00],
        [ 1.6921e+00, -6.0610e-01, -1.9127e+00],
        [ 2.0652e+00,  6.3096e-01, -2.1745e+00],
        [ 1.9461e+00, -1.3387e+00, -2.9980e+00],
        [-5.5606e-02, -9.1431e-01, -2.1167e+00],
        [ 5.1581e-01,  2.0691e-01, -2.7287e+00],
        [ 1.9609e+00, -5.7819e-01, -2.5868e+00],
        [ 2.5198e+00,  2.3485e+00, -4.5706e-01],
        [ 1.2687e+00, -3.8682e-01, -1.1106e+00],
        [ 2.6943e+00,  1.0885e-01, -1.6704e+00],
        [ 1.3518e+00, -5.7109e-01, -1.2313e+00],
        [ 1.7652e+00, -3.4918e-01, -2.2982e+00],
        [ 1.3604e+00, -1.9082e+00, -1.4458e+00],
        [ 8.5842e-02,  2.1159e+00, -6.2713e-01],
        [-6.6131e-01,  6.1895e-02, -1.8775e+00],
        [ 7.1580e-01, -1.4459e+00, -3.0637e+00],
        [-5.3466e-01,  7.3389e-01, -1.1891e+00]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 04:25:06 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 04:25:06 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 04:25:06 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 04:25:16 - INFO - Epoch : 1 , Batch [ 0 / 75 ] : Loss = 1.291078, Accuracy = 19.92%, MSE = 1.7969
2025-03-01 04:25:16 - INFO - Classwise accuracy : (0: 89.74% (39)), (1: 13.04% (115)), (2: 0.98% (102))
2025-03-01 04:27:19 - INFO - Epoch : 1 , Batch [ 10 / 75 ] : Loss = 0.668150, Accuracy = 34.98%, MSE = 1.2830
2025-03-01 04:27:19 - INFO - Classwise accuracy : (0: 10.81% (37)), (1: 16.36% (110)), (2: 89.91% (109))
2025-03-01 04:29:24 - INFO - Epoch : 1 , Batch [ 20 / 75 ] : Loss = 1.193274, Accuracy = 34.80%, MSE = 1.2759
2025-03-01 04:29:24 - INFO - Classwise accuracy : (0: 7.69% (52)), (1: 92.71% (96)), (2: 0.00% (108))
2025-03-01 04:31:21 - INFO - Epoch : 1 , Batch [ 30 / 75 ] : Loss = 0.335698, Accuracy = 34.55%, MSE = 1.2714
2025-03-01 04:31:21 - INFO - Classwise accuracy : (0: 94.34% (53)), (1: 0.00% (118)), (2: 27.06% (85))
2025-03-01 04:33:10 - INFO - Epoch : 1 , Batch [ 40 / 75 ] : Loss = 0.356528, Accuracy = 34.60%, MSE = 1.2922
2025-03-01 04:33:10 - INFO - Classwise accuracy : (0: 0.00% (45)), (1: 0.00% (103)), (2: 100.00% (108))
2025-03-01 04:35:08 - INFO - Epoch : 1 , Batch [ 50 / 75 ] : Loss = 0.412434, Accuracy = 34.31%, MSE = 1.3169
2025-03-01 04:35:08 - INFO - Classwise accuracy : (0: 100.00% (41)), (1: 0.00% (103)), (2: 0.00% (112))
2025-03-01 04:36:49 - INFO - Epoch : 1 , Batch [ 60 / 75 ] : Loss = 0.332226, Accuracy = 34.10%, MSE = 1.3337
2025-03-01 04:36:49 - INFO - Classwise accuracy : (0: 0.00% (57)), (1: 0.00% (103)), (2: 100.00% (96))
2025-03-01 04:38:15 - INFO - Epoch : 1 , Batch [ 70 / 75 ] : Loss = 0.413025, Accuracy = 32.91%, MSE = 1.3837
2025-03-01 04:38:15 - INFO - Classwise accuracy : (0: 0.00% (39)), (1: 0.00% (110)), (2: 100.00% (107))
2025-03-01 04:38:54 - INFO - Epoch 2: Train Loss=0.5458, Train Acc=33.29%, Train MSE=1.3725
2025-03-01 04:39:11 - INFO - Epoch 2: Val Loss=1.8065, Val Acc=0.00%
2025-03-01 04:39:12 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 04:39:18 - INFO - Epoch : 2 , Batch [ 0 / 75 ] : Loss = 0.250174, Accuracy = 30.47%, MSE = 1.6562
2025-03-01 04:39:18 - INFO - Classwise accuracy : (0: 48.28% (58)), (1: 0.00% (96)), (2: 49.02% (102))
2025-03-01 04:40:40 - INFO - Epoch : 2 , Batch [ 10 / 75 ] : Loss = 0.260103, Accuracy = 31.39%, MSE = 1.4808
2025-03-01 04:40:40 - INFO - Classwise accuracy : (0: 72.92% (48)), (1: 0.00% (112)), (2: 27.08% (96))
2025-03-01 04:42:21 - INFO - Epoch : 2 , Batch [ 20 / 75 ] : Loss = 0.207156, Accuracy = 35.53%, MSE = 1.3144
2025-03-01 04:42:21 - INFO - Classwise accuracy : (0: 2.86% (35)), (1: 0.00% (118)), (2: 96.12% (103))
2025-03-01 04:44:14 - INFO - Epoch : 2 , Batch [ 30 / 75 ] : Loss = 0.217409, Accuracy = 36.82%, MSE = 1.2635
2025-03-01 04:44:14 - INFO - Classwise accuracy : (0: 19.51% (41)), (1: 0.00% (105)), (2: 89.09% (110))
2025-03-01 04:46:11 - INFO - Epoch : 2 , Batch [ 40 / 75 ] : Loss = 0.214824, Accuracy = 36.37%, MSE = 1.2932
2025-03-01 04:46:11 - INFO - Classwise accuracy : (0: 10.26% (39)), (1: 0.00% (99)), (2: 87.29% (118))
2025-03-01 04:48:21 - INFO - Epoch : 2 , Batch [ 50 / 75 ] : Loss = 0.220817, Accuracy = 36.31%, MSE = 1.2861
2025-03-01 04:48:21 - INFO - Classwise accuracy : (0: 12.82% (39)), (1: 0.00% (110)), (2: 91.59% (107))
2025-03-01 04:50:08 - INFO - Epoch : 2 , Batch [ 60 / 75 ] : Loss = 0.216426, Accuracy = 36.65%, MSE = 1.2715
2025-03-01 04:50:08 - INFO - Classwise accuracy : (0: 23.08% (39)), (1: 0.00% (110)), (2: 81.31% (107))
2025-03-01 04:52:26 - INFO - Epoch : 2 , Batch [ 70 / 75 ] : Loss = 0.214540, Accuracy = 37.20%, MSE = 1.2585
2025-03-01 04:52:26 - INFO - Classwise accuracy : (0: 12.77% (47)), (1: 0.00% (94)), (2: 90.43% (115))
2025-03-01 04:53:03 - INFO - Epoch 3: Train Loss=0.2506, Train Acc=37.41%, Train MSE=1.2513
2025-03-01 04:53:19 - INFO - Epoch 3: Val Loss=0.7622, Val Acc=0.00%
2025-03-01 04:53:19 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 04:53:25 - INFO - Epoch : 3 , Batch [ 0 / 75 ] : Loss = 0.217299, Accuracy = 42.58%, MSE = 1.0430
2025-03-01 04:53:25 - INFO - Classwise accuracy : (0: 7.32% (41)), (1: 0.00% (107)), (2: 98.15% (108))
2025-03-01 04:54:58 - INFO - Epoch : 3 , Batch [ 10 / 75 ] : Loss = 0.235170, Accuracy = 40.80%, MSE = 1.1236
2025-03-01 04:54:58 - INFO - Classwise accuracy : (0: 1.96% (51)), (1: 0.00% (94)), (2: 100.00% (111))
2025-03-01 04:56:17 - INFO - Epoch : 3 , Batch [ 20 / 75 ] : Loss = 0.229053, Accuracy = 40.16%, MSE = 1.1542
2025-03-01 04:56:17 - INFO - Classwise accuracy : (0: 2.13% (47)), (1: 0.00% (112)), (2: 98.97% (97))
2025-03-01 04:58:19 - INFO - Epoch : 3 , Batch [ 30 / 75 ] : Loss = 0.211793, Accuracy = 38.89%, MSE = 1.1910
2025-03-01 04:58:19 - INFO - Classwise accuracy : (0: 16.67% (42)), (1: 0.00% (111)), (2: 89.32% (103))
2025-03-01 05:00:15 - INFO - Epoch : 3 , Batch [ 40 / 75 ] : Loss = 0.253349, Accuracy = 39.62%, MSE = 1.1626
2025-03-01 05:00:15 - INFO - Classwise accuracy : (0: 0.00% (54)), (1: 0.00% (110)), (2: 100.00% (92))
2025-03-01 05:02:37 - INFO - Epoch : 3 , Batch [ 50 / 75 ] : Loss = 0.236898, Accuracy = 40.22%, MSE = 1.1385
2025-03-01 05:02:37 - INFO - Classwise accuracy : (0: 28.57% (49)), (1: 0.00% (99)), (2: 72.22% (108))
2025-03-01 05:05:01 - INFO - Epoch : 3 , Batch [ 60 / 75 ] : Loss = 0.219123, Accuracy = 40.00%, MSE = 1.1548
2025-03-01 05:05:01 - INFO - Classwise accuracy : (0: 16.00% (50)), (1: 0.00% (101)), (2: 90.48% (105))
2025-03-01 05:07:02 - INFO - Epoch : 3 , Batch [ 70 / 75 ] : Loss = 0.222471, Accuracy = 40.10%, MSE = 1.1486
2025-03-01 05:07:02 - INFO - Classwise accuracy : (0: 2.27% (44)), (1: 0.00% (99)), (2: 100.00% (113))
2025-03-01 05:07:34 - INFO - Epoch 4: Train Loss=0.2213, Train Acc=40.10%, Train MSE=1.1500
2025-03-01 05:07:52 - INFO - Epoch 4: Val Loss=0.2003, Val Acc=100.00%
2025-03-01 05:07:52 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       1.00      1.00      1.00      4767
        Hold       0.00      0.00      0.00         0
         Buy       0.00      0.00      0.00         0

    accuracy                           1.00      4767
   macro avg       0.33      0.33      0.33      4767
weighted avg       1.00      1.00      1.00      4767

2025-03-01 05:07:58 - INFO - Epoch : 4 , Batch [ 0 / 75 ] : Loss = 0.214866, Accuracy = 37.89%, MSE = 1.1953
2025-03-01 05:07:58 - INFO - Classwise accuracy : (0: 73.21% (56)), (1: 0.00% (110)), (2: 62.22% (90))
2025-03-01 05:09:23 - INFO - Epoch : 4 , Batch [ 10 / 75 ] : Loss = 0.236623, Accuracy = 40.87%, MSE = 1.1037
2025-03-01 05:09:23 - INFO - Classwise accuracy : (0: 75.56% (45)), (1: 0.00% (111)), (2: 40.00% (100))
2025-03-01 05:10:45 - INFO - Epoch : 4 , Batch [ 20 / 75 ] : Loss = 0.186713, Accuracy = 40.83%, MSE = 1.1062
2025-03-01 05:10:45 - INFO - Classwise accuracy : (0: 31.82% (44)), (1: 0.00% (101)), (2: 97.30% (111))
2025-03-01 05:13:04 - INFO - Epoch : 4 , Batch [ 30 / 75 ] : Loss = 0.218112, Accuracy = 41.92%, MSE = 1.0631
2025-03-01 05:13:04 - INFO - Classwise accuracy : (0: 57.14% (42)), (1: 1.83% (109)), (2: 81.90% (105))
2025-03-01 05:15:18 - INFO - Epoch : 4 , Batch [ 40 / 75 ] : Loss = 0.212808, Accuracy = 41.91%, MSE = 1.0765
2025-03-01 05:15:18 - INFO - Classwise accuracy : (0: 34.55% (55)), (1: 0.00% (98)), (2: 89.32% (103))
2025-03-01 05:17:33 - INFO - Epoch : 4 , Batch [ 50 / 75 ] : Loss = 0.195857, Accuracy = 42.24%, MSE = 1.0668
2025-03-01 05:17:33 - INFO - Classwise accuracy : (0: 22.92% (48)), (1: 0.00% (99)), (2: 95.41% (109))
2025-03-01 05:19:22 - INFO - Epoch : 4 , Batch [ 60 / 75 ] : Loss = 0.190985, Accuracy = 42.41%, MSE = 1.0585
2025-03-01 05:19:22 - INFO - Classwise accuracy : (0: 19.51% (41)), (1: 0.00% (101)), (2: 93.86% (114))
2025-03-01 05:21:17 - INFO - Epoch : 4 , Batch [ 70 / 75 ] : Loss = 0.189806, Accuracy = 42.88%, MSE = 1.0415
2025-03-01 05:21:17 - INFO - Classwise accuracy : (0: 23.26% (43)), (1: 0.93% (107)), (2: 99.06% (106))
2025-03-01 05:22:00 - INFO - Epoch 5: Train Loss=0.2038, Train Acc=42.65%, Train MSE=1.0458
2025-03-01 05:22:17 - INFO - Epoch 5: Val Loss=0.6158, Val Acc=0.00%
2025-03-01 05:22:17 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 05:22:24 - INFO - Epoch : 5 , Batch [ 0 / 75 ] : Loss = 0.236455, Accuracy = 37.89%, MSE = 1.2422
2025-03-01 05:22:24 - INFO - Classwise accuracy : (0: 8.77% (57)), (1: 0.00% (106)), (2: 98.92% (93))
2025-03-01 05:23:43 - INFO - Epoch : 5 , Batch [ 10 / 75 ] : Loss = 0.207809, Accuracy = 40.31%, MSE = 1.1019
2025-03-01 05:23:43 - INFO - Classwise accuracy : (0: 65.91% (44)), (1: 0.88% (114)), (2: 76.53% (98))
2025-03-01 05:24:59 - INFO - Epoch : 5 , Batch [ 20 / 75 ] : Loss = 0.180480, Accuracy = 41.67%, MSE = 1.0666
2025-03-01 05:24:59 - INFO - Classwise accuracy : (0: 23.53% (34)), (1: 0.00% (118)), (2: 99.04% (104))
2025-03-01 05:26:22 - INFO - Epoch : 5 , Batch [ 30 / 75 ] : Loss = 0.190193, Accuracy = 42.44%, MSE = 1.0304
2025-03-01 05:26:22 - INFO - Classwise accuracy : (0: 63.41% (41)), (1: 0.88% (113)), (2: 73.53% (102))
2025-03-01 05:27:35 - INFO - Epoch : 5 , Batch [ 40 / 75 ] : Loss = 0.182807, Accuracy = 43.31%, MSE = 1.0099
2025-03-01 05:27:35 - INFO - Classwise accuracy : (0: 39.13% (46)), (1: 0.00% (109)), (2: 94.06% (101))
2025-03-01 05:29:34 - INFO - Epoch : 5 , Batch [ 50 / 75 ] : Loss = 0.197248, Accuracy = 43.79%, MSE = 1.0003
2025-03-01 05:29:34 - INFO - Classwise accuracy : (0: 29.55% (44)), (1: 7.32% (123)), (2: 93.26% (89))
2025-03-01 05:31:12 - INFO - Epoch : 5 , Batch [ 60 / 75 ] : Loss = 0.177727, Accuracy = 44.29%, MSE = 0.9894
2025-03-01 05:31:12 - INFO - Classwise accuracy : (0: 38.89% (54)), (1: 5.36% (112)), (2: 97.78% (90))
2025-03-01 05:32:43 - INFO - Epoch : 5 , Batch [ 70 / 75 ] : Loss = 0.202775, Accuracy = 43.88%, MSE = 1.0046
2025-03-01 05:32:43 - INFO - Classwise accuracy : (0: 0.00% (36)), (1: 0.00% (116)), (2: 100.00% (104))
2025-03-01 05:33:11 - INFO - Epoch 6: Train Loss=0.1983, Train Acc=43.79%, Train MSE=1.0048
2025-03-01 05:33:27 - INFO - Epoch 6: Val Loss=0.3179, Val Acc=0.00%
2025-03-01 05:33:28 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 05:33:34 - INFO - Epoch : 6 , Batch [ 0 / 75 ] : Loss = 0.206304, Accuracy = 39.84%, MSE = 1.1406
2025-03-01 05:33:34 - INFO - Classwise accuracy : (0: 19.05% (42)), (1: 0.00% (108)), (2: 88.68% (106))
2025-03-01 05:34:56 - INFO - Epoch : 6 , Batch [ 10 / 75 ] : Loss = 0.208358, Accuracy = 42.83%, MSE = 1.0373
2025-03-01 05:34:56 - INFO - Classwise accuracy : (0: 41.86% (43)), (1: 0.00% (109)), (2: 89.42% (104))
2025-03-01 05:36:06 - INFO - Epoch : 6 , Batch [ 20 / 75 ] : Loss = 0.194280, Accuracy = 43.69%, MSE = 1.0095
2025-03-01 05:36:06 - INFO - Classwise accuracy : (0: 43.24% (37)), (1: 0.00% (107)), (2: 90.18% (112))
2025-03-01 05:37:30 - INFO - Epoch : 6 , Batch [ 30 / 75 ] : Loss = 0.212593, Accuracy = 43.72%, MSE = 1.0039
2025-03-01 05:37:30 - INFO - Classwise accuracy : (0: 30.23% (43)), (1: 0.00% (91)), (2: 91.80% (122))
2025-03-01 05:39:15 - INFO - Epoch : 6 , Batch [ 40 / 75 ] : Loss = 0.188977, Accuracy = 44.32%, MSE = 0.9844
2025-03-01 05:39:15 - INFO - Classwise accuracy : (0: 27.78% (36)), (1: 0.00% (112)), (2: 98.15% (108))
2025-03-01 05:40:59 - INFO - Epoch : 6 , Batch [ 50 / 75 ] : Loss = 0.214910, Accuracy = 44.22%, MSE = 0.9810
2025-03-01 05:40:59 - INFO - Classwise accuracy : (0: 0.00% (47)), (1: 0.00% (99)), (2: 100.00% (110))
2025-03-01 05:42:44 - INFO - Epoch : 6 , Batch [ 60 / 75 ] : Loss = 0.186383, Accuracy = 44.32%, MSE = 0.9754
2025-03-01 05:42:44 - INFO - Classwise accuracy : (0: 66.67% (36)), (1: 0.00% (103)), (2: 80.34% (117))
2025-03-01 05:44:46 - INFO - Epoch : 6 , Batch [ 70 / 75 ] : Loss = 0.191766, Accuracy = 44.31%, MSE = 0.9732
2025-03-01 05:44:46 - INFO - Classwise accuracy : (0: 48.84% (43)), (1: 0.00% (121)), (2: 79.35% (92))
2025-03-01 05:45:20 - INFO - Epoch 7: Train Loss=0.1967, Train Acc=44.52%, Train MSE=0.9672
2025-03-01 05:45:37 - INFO - Epoch 7: Val Loss=0.4298, Val Acc=0.00%
2025-03-01 05:45:37 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 05:45:41 - INFO - Epoch : 7 , Batch [ 0 / 75 ] : Loss = 0.236014, Accuracy = 33.20%, MSE = 1.4062
2025-03-01 05:45:41 - INFO - Classwise accuracy : (0: 65.12% (43)), (1: 0.00% (108)), (2: 54.29% (105))
2025-03-01 05:46:59 - INFO - Epoch : 7 , Batch [ 10 / 75 ] : Loss = 0.179015, Accuracy = 43.39%, MSE = 0.9805
2025-03-01 05:46:59 - INFO - Classwise accuracy : (0: 37.14% (35)), (1: 0.00% (113)), (2: 95.37% (108))
2025-03-01 05:49:11 - INFO - Epoch : 7 , Batch [ 20 / 75 ] : Loss = 0.201020, Accuracy = 45.35%, MSE = 0.9528
2025-03-01 05:49:11 - INFO - Classwise accuracy : (0: 72.22% (36)), (1: 0.00% (105)), (2: 73.91% (115))
2025-03-01 05:51:07 - INFO - Epoch : 7 , Batch [ 30 / 75 ] : Loss = 0.177594, Accuracy = 44.98%, MSE = 0.9512
2025-03-01 05:51:07 - INFO - Classwise accuracy : (0: 40.00% (45)), (1: 0.00% (115)), (2: 97.92% (96))
2025-03-01 05:52:55 - INFO - Epoch : 7 , Batch [ 40 / 75 ] : Loss = 0.156163, Accuracy = 45.23%, MSE = 0.9422
2025-03-01 05:52:55 - INFO - Classwise accuracy : (0: 67.65% (34)), (1: 0.00% (112)), (2: 84.55% (110))
2025-03-01 05:54:36 - INFO - Epoch : 7 , Batch [ 50 / 75 ] : Loss = 0.161330, Accuracy = 45.65%, MSE = 0.9217
2025-03-01 05:54:36 - INFO - Classwise accuracy : (0: 54.05% (37)), (1: 0.00% (106)), (2: 95.58% (113))
2025-03-01 05:56:16 - INFO - Epoch : 7 , Batch [ 60 / 75 ] : Loss = 0.165751, Accuracy = 46.02%, MSE = 0.9123
2025-03-01 05:56:16 - INFO - Classwise accuracy : (0: 56.00% (50)), (1: 0.00% (102)), (2: 98.08% (104))
2025-03-01 05:57:59 - INFO - Epoch : 7 , Batch [ 70 / 75 ] : Loss = 0.187945, Accuracy = 45.99%, MSE = 0.9113
2025-03-01 05:57:59 - INFO - Classwise accuracy : (0: 35.00% (40)), (1: 0.00% (119)), (2: 92.78% (97))
2025-03-01 05:58:29 - INFO - Epoch 8: Train Loss=0.1825, Train Acc=46.10%, Train MSE=0.9065
2025-03-01 05:58:46 - INFO - Epoch 8: Val Loss=0.7834, Val Acc=0.00%
2025-03-01 05:58:46 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 05:58:52 - INFO - Epoch : 8 , Batch [ 0 / 75 ] : Loss = 0.189267, Accuracy = 41.80%, MSE = 1.0625
2025-03-01 05:58:52 - INFO - Classwise accuracy : (0: 27.78% (36)), (1: 0.00% (108)), (2: 86.61% (112))
2025-03-01 06:00:23 - INFO - Epoch : 8 , Batch [ 10 / 75 ] : Loss = 0.205844, Accuracy = 39.24%, MSE = 1.1925
2025-03-01 06:00:23 - INFO - Classwise accuracy : (0: 0.00% (46)), (1: 0.00% (99)), (2: 100.00% (111))
2025-03-01 06:03:33 - INFO - Epoch : 8 , Batch [ 20 / 75 ] : Loss = 0.194103, Accuracy = 42.43%, MSE = 1.0958
2025-03-01 06:03:33 - INFO - Classwise accuracy : (0: 56.00% (50)), (1: 0.00% (114)), (2: 91.30% (92))
2025-03-01 06:05:29 - INFO - Epoch : 8 , Batch [ 30 / 75 ] : Loss = 0.159886, Accuracy = 43.86%, MSE = 1.0248
2025-03-01 06:05:29 - INFO - Classwise accuracy : (0: 72.73% (33)), (1: 0.00% (109)), (2: 90.35% (114))
2025-03-01 06:07:11 - INFO - Epoch : 8 , Batch [ 40 / 75 ] : Loss = 0.159446, Accuracy = 44.64%, MSE = 0.9795
2025-03-01 06:07:11 - INFO - Classwise accuracy : (0: 58.54% (41)), (1: 0.00% (108)), (2: 97.20% (107))
2025-03-01 06:09:20 - INFO - Epoch : 8 , Batch [ 50 / 75 ] : Loss = 0.176686, Accuracy = 44.91%, MSE = 0.9583
2025-03-01 06:09:20 - INFO - Classwise accuracy : (0: 36.59% (41)), (1: 9.26% (108)), (2: 96.26% (107))
2025-03-01 06:11:46 - INFO - Epoch : 8 , Batch [ 60 / 75 ] : Loss = 0.158016, Accuracy = 45.25%, MSE = 0.9360
2025-03-01 06:11:46 - INFO - Classwise accuracy : (0: 71.05% (38)), (1: 0.00% (111)), (2: 91.59% (107))
2025-03-01 06:13:55 - INFO - Epoch : 8 , Batch [ 70 / 75 ] : Loss = 0.145265, Accuracy = 46.13%, MSE = 0.9067
2025-03-01 06:13:55 - INFO - Classwise accuracy : (0: 75.00% (36)), (1: 0.00% (111)), (2: 94.50% (109))
2025-03-01 06:14:41 - INFO - Epoch 9: Train Loss=0.1876, Train Acc=46.20%, Train MSE=0.9023
2025-03-01 06:14:58 - INFO - Epoch 9: Val Loss=0.7812, Val Acc=0.00%
2025-03-01 06:14:58 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 06:15:04 - INFO - Epoch : 9 , Batch [ 0 / 75 ] : Loss = 0.203398, Accuracy = 44.14%, MSE = 1.0508
2025-03-01 06:15:04 - INFO - Classwise accuracy : (0: 8.82% (34)), (1: 0.00% (101)), (2: 90.91% (121))
2025-03-01 06:16:28 - INFO - Epoch : 9 , Batch [ 10 / 75 ] : Loss = 0.173123, Accuracy = 43.32%, MSE = 1.0451
2025-03-01 06:16:28 - INFO - Classwise accuracy : (0: 39.39% (33)), (1: 0.00% (107)), (2: 94.83% (116))
2025-03-01 06:18:12 - INFO - Epoch : 9 , Batch [ 20 / 75 ] : Loss = 0.165795, Accuracy = 44.85%, MSE = 0.9790
2025-03-01 06:18:12 - INFO - Classwise accuracy : (0: 23.53% (34)), (1: 0.00% (110)), (2: 99.11% (112))
2025-03-01 06:20:07 - INFO - Epoch : 9 , Batch [ 30 / 75 ] : Loss = 0.177069, Accuracy = 45.68%, MSE = 0.9299
2025-03-01 06:20:07 - INFO - Classwise accuracy : (0: 79.07% (43)), (1: 0.00% (100)), (2: 84.96% (113))
2025-03-01 06:22:22 - INFO - Epoch : 9 , Batch [ 40 / 75 ] : Loss = 0.182008, Accuracy = 46.31%, MSE = 0.9124
2025-03-01 06:22:22 - INFO - Classwise accuracy : (0: 51.11% (45)), (1: 0.00% (100)), (2: 90.09% (111))
2025-03-01 06:24:32 - INFO - Epoch : 9 , Batch [ 50 / 75 ] : Loss = 0.151889, Accuracy = 46.67%, MSE = 0.8881
2025-03-01 06:24:32 - INFO - Classwise accuracy : (0: 82.93% (41)), (1: 0.00% (117)), (2: 88.78% (98))
2025-03-01 06:26:30 - INFO - Epoch : 9 , Batch [ 60 / 75 ] : Loss = 0.147100, Accuracy = 47.18%, MSE = 0.8648
2025-03-01 06:26:30 - INFO - Classwise accuracy : (0: 72.97% (37)), (1: 0.00% (103)), (2: 94.83% (116))
2025-03-01 06:28:16 - INFO - Epoch : 9 , Batch [ 70 / 75 ] : Loss = 0.144556, Accuracy = 47.47%, MSE = 0.8508
2025-03-01 06:28:16 - INFO - Classwise accuracy : (0: 85.00% (40)), (1: 0.00% (118)), (2: 91.84% (98))
2025-03-01 06:28:59 - INFO - Epoch 10: Train Loss=0.1735, Train Acc=47.53%, Train MSE=0.8474
2025-03-01 06:29:16 - INFO - Epoch 10: Val Loss=0.7316, Val Acc=0.00%
2025-03-01 06:29:16 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 06:29:23 - INFO - Epoch : 10 , Batch [ 0 / 75 ] : Loss = 0.220399, Accuracy = 42.58%, MSE = 1.0195
2025-03-01 06:29:23 - INFO - Classwise accuracy : (0: 68.18% (44)), (1: 0.00% (109)), (2: 76.70% (103))
2025-03-01 06:31:07 - INFO - Epoch : 10 , Batch [ 10 / 75 ] : Loss = 0.168310, Accuracy = 42.05%, MSE = 1.0376
2025-03-01 06:31:07 - INFO - Classwise accuracy : (0: 52.50% (40)), (1: 0.00% (97)), (2: 94.12% (119))
2025-03-01 06:32:56 - INFO - Epoch : 10 , Batch [ 20 / 75 ] : Loss = 0.163725, Accuracy = 45.98%, MSE = 0.9146
2025-03-01 06:32:56 - INFO - Classwise accuracy : (0: 84.62% (39)), (1: 0.00% (116)), (2: 87.13% (101))
2025-03-01 06:34:46 - INFO - Epoch : 10 , Batch [ 30 / 75 ] : Loss = 0.196736, Accuracy = 47.15%, MSE = 0.8645
2025-03-01 06:34:46 - INFO - Classwise accuracy : (0: 82.69% (52)), (1: 0.00% (114)), (2: 85.56% (90))
2025-03-01 06:37:13 - INFO - Epoch : 10 , Batch [ 40 / 75 ] : Loss = 0.224146, Accuracy = 47.37%, MSE = 0.8527
2025-03-01 06:37:13 - INFO - Classwise accuracy : (0: 63.27% (49)), (1: 0.00% (112)), (2: 90.53% (95))
2025-03-01 06:39:42 - INFO - Epoch : 10 , Batch [ 50 / 75 ] : Loss = 0.170381, Accuracy = 47.96%, MSE = 0.8377
2025-03-01 06:39:42 - INFO - Classwise accuracy : (0: 52.94% (34)), (1: 0.00% (108)), (2: 95.61% (114))
2025-03-01 06:42:06 - INFO - Epoch : 10 , Batch [ 60 / 75 ] : Loss = 0.159575, Accuracy = 48.04%, MSE = 0.8275
2025-03-01 06:42:06 - INFO - Classwise accuracy : (0: 47.37% (38)), (1: 0.00% (108)), (2: 97.27% (110))
2025-03-01 06:44:36 - INFO - Epoch : 10 , Batch [ 70 / 75 ] : Loss = 0.134987, Accuracy = 48.38%, MSE = 0.8120
2025-03-01 06:44:36 - INFO - Classwise accuracy : (0: 77.50% (40)), (1: 0.00% (100)), (2: 97.41% (116))
2025-03-01 06:45:22 - INFO - Epoch 11: Train Loss=0.1856, Train Acc=48.58%, Train MSE=0.8045
2025-03-01 06:45:38 - INFO - Epoch 11: Val Loss=0.9398, Val Acc=0.00%
2025-03-01 06:45:39 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 06:45:44 - INFO - Epoch : 11 , Batch [ 0 / 75 ] : Loss = 0.148069, Accuracy = 50.00%, MSE = 0.6875
2025-03-01 06:45:44 - INFO - Classwise accuracy : (0: 77.50% (40)), (1: 0.00% (112)), (2: 93.27% (104))
2025-03-01 06:47:09 - INFO - Epoch : 11 , Batch [ 10 / 75 ] : Loss = 0.140563, Accuracy = 49.25%, MSE = 0.7536
2025-03-01 06:47:09 - INFO - Classwise accuracy : (0: 72.97% (37)), (1: 0.00% (108)), (2: 96.40% (111))
2025-03-01 06:49:22 - INFO - Epoch : 11 , Batch [ 20 / 75 ] : Loss = 0.142783, Accuracy = 51.47%, MSE = 0.7041
2025-03-01 06:49:22 - INFO - Classwise accuracy : (0: 80.49% (41)), (1: 0.00% (97)), (2: 94.92% (118))
2025-03-01 06:51:28 - INFO - Epoch : 11 , Batch [ 30 / 75 ] : Loss = 0.135988, Accuracy = 51.85%, MSE = 0.6883
2025-03-01 06:51:28 - INFO - Classwise accuracy : (0: 80.85% (47)), (1: 0.00% (109)), (2: 97.00% (100))
2025-03-01 06:53:36 - INFO - Epoch : 11 , Batch [ 40 / 75 ] : Loss = 0.125965, Accuracy = 52.03%, MSE = 0.6772
2025-03-01 06:53:36 - INFO - Classwise accuracy : (0: 91.11% (45)), (1: 0.00% (109)), (2: 96.08% (102))
2025-03-01 06:55:20 - INFO - Epoch : 11 , Batch [ 50 / 75 ] : Loss = 0.141840, Accuracy = 51.91%, MSE = 0.6811
2025-03-01 06:55:20 - INFO - Classwise accuracy : (0: 86.84% (38)), (1: 0.00% (106)), (2: 91.07% (112))
2025-03-01 06:56:52 - INFO - Epoch : 11 , Batch [ 60 / 75 ] : Loss = 0.133902, Accuracy = 51.58%, MSE = 0.6858
2025-03-01 06:56:52 - INFO - Classwise accuracy : (0: 84.78% (46)), (1: 0.00% (117)), (2: 97.85% (93))
2025-03-01 06:58:38 - INFO - Epoch : 11 , Batch [ 70 / 75 ] : Loss = 0.130540, Accuracy = 51.96%, MSE = 0.6723
2025-03-01 06:58:38 - INFO - Classwise accuracy : (0: 78.12% (32)), (1: 0.00% (106)), (2: 97.46% (118))
2025-03-01 06:59:15 - INFO - Epoch 12: Train Loss=0.1476, Train Acc=51.99%, Train MSE=0.6695
2025-03-01 06:59:32 - INFO - Epoch 12: Val Loss=0.8386, Val Acc=13.45%
2025-03-01 06:59:32 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       1.00      0.13      0.24      4767
        Hold       0.00      0.00      0.00         0
         Buy       0.00      0.00      0.00         0

    accuracy                           0.13      4767
   macro avg       0.33      0.04      0.08      4767
weighted avg       1.00      0.13      0.24      4767

2025-03-01 06:59:38 - INFO - Epoch : 12 , Batch [ 0 / 75 ] : Loss = 0.294788, Accuracy = 39.84%, MSE = 1.1406
2025-03-01 06:59:38 - INFO - Classwise accuracy : (0: 0.00% (46)), (1: 0.00% (108)), (2: 100.00% (102))
2025-03-01 07:01:11 - INFO - Epoch : 12 , Batch [ 10 / 75 ] : Loss = 0.165322, Accuracy = 46.95%, MSE = 0.8640
2025-03-01 07:01:11 - INFO - Classwise accuracy : (0: 71.79% (39)), (1: 0.00% (115)), (2: 90.20% (102))
2025-03-01 07:02:58 - INFO - Epoch : 12 , Batch [ 20 / 75 ] : Loss = 0.202590, Accuracy = 45.67%, MSE = 0.9390
2025-03-01 07:02:58 - INFO - Classwise accuracy : (0: 15.69% (51)), (1: 0.00% (111)), (2: 100.00% (94))
2025-03-01 07:05:22 - INFO - Epoch : 12 , Batch [ 30 / 75 ] : Loss = 0.150413, Accuracy = 46.99%, MSE = 0.8938
2025-03-01 07:05:22 - INFO - Classwise accuracy : (0: 82.50% (40)), (1: 0.00% (96)), (2: 89.17% (120))
2025-03-01 07:07:30 - INFO - Epoch : 12 , Batch [ 40 / 75 ] : Loss = 0.154627, Accuracy = 46.95%, MSE = 0.8758
2025-03-01 07:07:30 - INFO - Classwise accuracy : (0: 65.91% (44)), (1: 0.00% (107)), (2: 97.14% (105))
2025-03-01 07:09:19 - INFO - Epoch : 12 , Batch [ 50 / 75 ] : Loss = 0.124600, Accuracy = 48.07%, MSE = 0.8286
2025-03-01 07:09:19 - INFO - Classwise accuracy : (0: 74.36% (39)), (1: 0.00% (105)), (2: 99.11% (112))
2025-03-01 07:11:18 - INFO - Epoch : 12 , Batch [ 60 / 75 ] : Loss = 0.122504, Accuracy = 48.94%, MSE = 0.7959
2025-03-01 07:11:18 - INFO - Classwise accuracy : (0: 89.58% (48)), (1: 0.00% (110)), (2: 92.86% (98))
2025-03-01 07:12:55 - INFO - Epoch : 12 , Batch [ 70 / 75 ] : Loss = 0.130898, Accuracy = 49.35%, MSE = 0.7767
2025-03-01 07:12:55 - INFO - Classwise accuracy : (0: 85.42% (48)), (1: 0.00% (105)), (2: 97.09% (103))
2025-03-01 07:13:31 - INFO - Epoch 13: Train Loss=0.1622, Train Acc=49.44%, Train MSE=0.7726
2025-03-01 07:13:48 - INFO - Epoch 13: Val Loss=0.5304, Val Acc=4.78%
2025-03-01 07:13:48 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       1.00      0.05      0.09      4767
        Hold       0.00      0.00      0.00         0
         Buy       0.00      0.00      0.00         0

    accuracy                           0.05      4767
   macro avg       0.33      0.02      0.03      4767
weighted avg       1.00      0.05      0.09      4767

2025-03-01 07:13:56 - INFO - Epoch : 13 , Batch [ 0 / 75 ] : Loss = 0.303658, Accuracy = 45.70%, MSE = 1.0938
2025-03-01 07:13:56 - INFO - Classwise accuracy : (0: 0.00% (47)), (1: 0.00% (92)), (2: 100.00% (117))
2025-03-01 07:15:23 - INFO - Epoch : 13 , Batch [ 10 / 75 ] : Loss = 0.147932, Accuracy = 50.25%, MSE = 0.7468
2025-03-01 07:15:23 - INFO - Classwise accuracy : (0: 78.26% (46)), (1: 0.00% (114)), (2: 93.75% (96))
2025-03-01 07:16:59 - INFO - Epoch : 13 , Batch [ 20 / 75 ] : Loss = 0.145916, Accuracy = 51.66%, MSE = 0.6916
2025-03-01 07:16:59 - INFO - Classwise accuracy : (0: 72.50% (40)), (1: 0.00% (112)), (2: 99.04% (104))
2025-03-01 07:18:32 - INFO - Epoch : 13 , Batch [ 30 / 75 ] : Loss = 0.133320, Accuracy = 51.58%, MSE = 0.6816
2025-03-01 07:18:32 - INFO - Classwise accuracy : (0: 86.36% (44)), (1: 0.00% (116)), (2: 95.83% (96))
2025-03-01 07:20:09 - INFO - Epoch : 13 , Batch [ 40 / 75 ] : Loss = 0.225446, Accuracy = 50.69%, MSE = 0.7115
2025-03-01 07:20:09 - INFO - Classwise accuracy : (0: 70.59% (51)), (1: 0.00% (122)), (2: 73.49% (83))
2025-03-01 07:21:56 - INFO - Epoch : 13 , Batch [ 50 / 75 ] : Loss = 0.179700, Accuracy = 50.35%, MSE = 0.7350
2025-03-01 07:21:56 - INFO - Classwise accuracy : (0: 53.66% (41)), (1: 0.00% (110)), (2: 97.14% (105))
2025-03-01 07:24:23 - INFO - Epoch : 13 , Batch [ 60 / 75 ] : Loss = 0.150173, Accuracy = 50.50%, MSE = 0.7303
2025-03-01 07:24:23 - INFO - Classwise accuracy : (0: 60.61% (33)), (1: 0.00% (111)), (2: 98.21% (112))
2025-03-01 07:26:53 - INFO - Epoch : 13 , Batch [ 70 / 75 ] : Loss = 0.123686, Accuracy = 50.74%, MSE = 0.7217
2025-03-01 07:26:53 - INFO - Classwise accuracy : (0: 80.56% (36)), (1: 0.00% (114)), (2: 99.06% (106))
2025-03-01 07:27:35 - INFO - Epoch 14: Train Loss=0.1632, Train Acc=50.81%, Train MSE=0.7168
2025-03-01 07:27:52 - INFO - Epoch 14: Val Loss=0.2122, Val Acc=100.00%
2025-03-01 07:27:52 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       1.00      1.00      1.00      4767
        Hold       0.00      0.00      0.00         0
         Buy       0.00      0.00      0.00         0

    accuracy                           1.00      4767
   macro avg       0.33      0.33      0.33      4767
weighted avg       1.00      1.00      1.00      4767

2025-03-01 07:27:58 - INFO - Epoch : 14 , Batch [ 0 / 75 ] : Loss = 0.233232, Accuracy = 45.31%, MSE = 1.0273
2025-03-01 07:27:58 - INFO - Classwise accuracy : (0: 0.00% (39)), (1: 0.00% (99)), (2: 98.31% (118))
2025-03-01 07:29:38 - INFO - Epoch : 14 , Batch [ 10 / 75 ] : Loss = 0.155450, Accuracy = 45.35%, MSE = 0.9162
2025-03-01 07:29:38 - INFO - Classwise accuracy : (0: 63.64% (44)), (1: 0.00% (106)), (2: 99.06% (106))
2025-03-01 07:31:58 - INFO - Epoch : 14 , Batch [ 20 / 75 ] : Loss = 0.134781, Accuracy = 48.46%, MSE = 0.7989
2025-03-01 07:31:58 - INFO - Classwise accuracy : (0: 85.71% (42)), (1: 0.00% (93)), (2: 95.04% (121))
2025-03-01 07:33:49 - INFO - Epoch : 14 , Batch [ 30 / 75 ] : Loss = 0.130533, Accuracy = 50.18%, MSE = 0.7428
2025-03-01 07:33:49 - INFO - Classwise accuracy : (0: 81.08% (37)), (1: 0.00% (113)), (2: 95.28% (106))
2025-03-01 07:35:21 - INFO - Epoch : 14 , Batch [ 40 / 75 ] : Loss = 0.141858, Accuracy = 50.83%, MSE = 0.7127
2025-03-01 07:35:21 - INFO - Classwise accuracy : (0: 74.07% (54)), (1: 0.00% (100)), (2: 100.00% (102))
2025-03-01 07:37:11 - INFO - Epoch : 14 , Batch [ 50 / 75 ] : Loss = 0.103951, Accuracy = 51.00%, MSE = 0.6945
2025-03-01 07:37:11 - INFO - Classwise accuracy : (0: 94.29% (35)), (1: 0.00% (111)), (2: 97.27% (110))
2025-03-01 07:39:20 - INFO - Epoch : 14 , Batch [ 60 / 75 ] : Loss = 0.158726, Accuracy = 51.78%, MSE = 0.6701
2025-03-01 07:39:20 - INFO - Classwise accuracy : (0: 86.79% (53)), (1: 0.00% (97)), (2: 93.40% (106))
2025-03-01 07:41:50 - INFO - Epoch : 14 , Batch [ 70 / 75 ] : Loss = 0.135826, Accuracy = 52.29%, MSE = 0.6591
2025-03-01 07:41:51 - INFO - Classwise accuracy : (0: 72.00% (50)), (1: 0.00% (94)), (2: 98.21% (112))
2025-03-01 07:42:42 - INFO - Epoch 15: Train Loss=0.1435, Train Acc=52.19%, Train MSE=0.6643
2025-03-01 07:42:58 - INFO - Epoch 15: Val Loss=0.2765, Val Acc=100.00%
2025-03-01 07:42:59 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       1.00      1.00      1.00      4767
        Hold       0.00      0.00      0.00         0
         Buy       0.00      0.00      0.00         0

    accuracy                           1.00      4767
   macro avg       0.33      0.33      0.33      4767
weighted avg       1.00      1.00      1.00      4767

2025-03-01 07:43:04 - INFO - Epoch : 15 , Batch [ 0 / 75 ] : Loss = 0.233214, Accuracy = 44.14%, MSE = 0.9570
2025-03-01 07:43:04 - INFO - Classwise accuracy : (0: 2.94% (34)), (1: 0.00% (109)), (2: 99.12% (113))
2025-03-01 07:44:28 - INFO - Epoch : 15 , Batch [ 10 / 75 ] : Loss = 0.172589, Accuracy = 38.78%, MSE = 1.1598
2025-03-01 07:44:28 - INFO - Classwise accuracy : (0: 80.56% (36)), (1: 0.00% (117)), (2: 92.23% (103))
2025-03-01 07:46:13 - INFO - Epoch : 15 , Batch [ 20 / 75 ] : Loss = 0.174984, Accuracy = 43.38%, MSE = 0.9981
2025-03-01 07:46:13 - INFO - Classwise accuracy : (0: 69.23% (39)), (1: 0.00% (96)), (2: 94.21% (121))
2025-03-01 07:47:46 - INFO - Epoch : 15 , Batch [ 30 / 75 ] : Loss = 0.166430, Accuracy = 45.84%, MSE = 0.8973
2025-03-01 07:47:46 - INFO - Classwise accuracy : (0: 68.42% (38)), (1: 0.00% (123)), (2: 92.63% (95))
2025-03-01 07:49:23 - INFO - Epoch : 15 , Batch [ 40 / 75 ] : Loss = 0.208297, Accuracy = 46.90%, MSE = 0.8651
2025-03-01 07:49:23 - INFO - Classwise accuracy : (0: 44.90% (49)), (1: 0.00% (101)), (2: 93.40% (106))
2025-03-01 07:50:47 - INFO - Epoch : 15 , Batch [ 50 / 75 ] : Loss = 0.160108, Accuracy = 47.77%, MSE = 0.8341
2025-03-01 07:50:47 - INFO - Classwise accuracy : (0: 70.45% (44)), (1: 0.00% (111)), (2: 93.07% (101))
2025-03-01 07:52:11 - INFO - Epoch : 15 , Batch [ 60 / 75 ] : Loss = 0.154865, Accuracy = 48.68%, MSE = 0.8025
2025-03-01 07:52:11 - INFO - Classwise accuracy : (0: 69.05% (42)), (1: 0.00% (108)), (2: 94.34% (106))
2025-03-01 07:53:31 - INFO - Epoch : 15 , Batch [ 70 / 75 ] : Loss = 0.092930, Accuracy = 49.52%, MSE = 0.7722
2025-03-01 07:53:31 - INFO - Classwise accuracy : (0: 92.86% (42)), (1: 0.00% (105)), (2: 100.00% (109))
2025-03-01 07:54:00 - INFO - Epoch 16: Train Loss=0.1695, Train Acc=49.70%, Train MSE=0.7604
2025-03-01 07:54:17 - INFO - Epoch 16: Val Loss=0.7613, Val Acc=0.00%
2025-03-01 07:54:17 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 07:54:22 - INFO - Epoch : 16 , Batch [ 0 / 75 ] : Loss = 0.269348, Accuracy = 39.45%, MSE = 1.1680
2025-03-01 07:54:22 - INFO - Classwise accuracy : (0: 16.67% (36)), (1: 0.00% (107)), (2: 84.07% (113))
2025-03-01 07:56:00 - INFO - Epoch : 16 , Batch [ 10 / 75 ] : Loss = 0.125739, Accuracy = 46.38%, MSE = 0.9208
2025-03-01 07:56:00 - INFO - Classwise accuracy : (0: 94.74% (38)), (1: 0.00% (95)), (2: 93.50% (123))
2025-03-01 07:57:44 - INFO - Epoch : 16 , Batch [ 20 / 75 ] : Loss = 0.178085, Accuracy = 47.75%, MSE = 0.8400
2025-03-01 07:57:44 - INFO - Classwise accuracy : (0: 78.05% (41)), (1: 0.00% (117)), (2: 82.65% (98))
2025-03-01 07:59:43 - INFO - Epoch : 16 , Batch [ 30 / 75 ] : Loss = 0.176966, Accuracy = 47.79%, MSE = 0.8184
2025-03-01 07:59:43 - INFO - Classwise accuracy : (0: 60.00% (35)), (1: 0.00% (119)), (2: 92.16% (102))
2025-03-01 08:02:02 - INFO - Epoch : 16 , Batch [ 40 / 75 ] : Loss = 0.178760, Accuracy = 47.78%, MSE = 0.8420
2025-03-01 08:02:02 - INFO - Classwise accuracy : (0: 29.03% (31)), (1: 0.00% (112)), (2: 99.12% (113))
2025-03-01 08:03:57 - INFO - Epoch : 16 , Batch [ 50 / 75 ] : Loss = 0.216018, Accuracy = 46.92%, MSE = 0.8775
2025-03-01 08:03:57 - INFO - Classwise accuracy : (0: 32.50% (40)), (1: 0.00% (106)), (2: 88.18% (110))
2025-03-01 08:05:49 - INFO - Epoch : 16 , Batch [ 60 / 75 ] : Loss = 0.196787, Accuracy = 46.40%, MSE = 0.8985
2025-03-01 08:05:49 - INFO - Classwise accuracy : (0: 21.62% (37)), (1: 0.00% (112)), (2: 91.59% (107))
2025-03-01 08:07:30 - INFO - Epoch : 16 , Batch [ 70 / 75 ] : Loss = 0.203475, Accuracy = 45.68%, MSE = 0.9262
2025-03-01 08:07:30 - INFO - Classwise accuracy : (0: 2.33% (43)), (1: 0.00% (99)), (2: 98.25% (114))
2025-03-01 08:08:09 - INFO - Epoch 17: Train Loss=0.1971, Train Acc=45.40%, Train MSE=0.9347
2025-03-01 08:08:26 - INFO - Epoch 17: Val Loss=0.4300, Val Acc=0.00%
2025-03-01 08:08:26 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 08:08:33 - INFO - Epoch : 17 , Batch [ 0 / 75 ] : Loss = 0.222055, Accuracy = 38.28%, MSE = 1.1562
2025-03-01 08:08:33 - INFO - Classwise accuracy : (0: 0.00% (46)), (1: 0.00% (112)), (2: 100.00% (98))
2025-03-01 08:09:53 - INFO - Epoch : 17 , Batch [ 10 / 75 ] : Loss = 0.222568, Accuracy = 42.08%, MSE = 1.0895
2025-03-01 08:09:53 - INFO - Classwise accuracy : (0: 0.00% (47)), (1: 0.00% (108)), (2: 100.00% (101))
2025-03-01 08:11:26 - INFO - Epoch : 17 , Batch [ 20 / 75 ] : Loss = 0.220013, Accuracy = 42.06%, MSE = 1.0967
2025-03-01 08:11:26 - INFO - Classwise accuracy : (0: 10.64% (47)), (1: 0.00% (105)), (2: 91.35% (104))
2025-03-01 08:12:55 - INFO - Epoch : 17 , Batch [ 30 / 75 ] : Loss = 0.216606, Accuracy = 41.90%, MSE = 1.0974
2025-03-01 08:12:55 - INFO - Classwise accuracy : (0: 8.51% (47)), (1: 0.00% (101)), (2: 97.22% (108))
2025-03-01 08:14:32 - INFO - Epoch : 17 , Batch [ 40 / 75 ] : Loss = 0.225561, Accuracy = 41.65%, MSE = 1.0834
2025-03-01 08:14:32 - INFO - Classwise accuracy : (0: 6.00% (50)), (1: 0.00% (112)), (2: 97.87% (94))
2025-03-01 08:16:12 - INFO - Epoch : 17 , Batch [ 50 / 75 ] : Loss = 0.207292, Accuracy = 41.71%, MSE = 1.0831
2025-03-01 08:16:12 - INFO - Classwise accuracy : (0: 2.78% (36)), (1: 0.00% (108)), (2: 96.43% (112))
2025-03-01 08:17:54 - INFO - Epoch : 17 , Batch [ 60 / 75 ] : Loss = 0.208287, Accuracy = 41.50%, MSE = 1.0868
2025-03-01 08:17:54 - INFO - Classwise accuracy : (0: 2.63% (38)), (1: 0.00% (105)), (2: 100.00% (113))
2025-03-01 08:19:46 - INFO - Epoch : 17 , Batch [ 70 / 75 ] : Loss = 0.215263, Accuracy = 41.62%, MSE = 1.0830
2025-03-01 08:19:46 - INFO - Classwise accuracy : (0: 0.00% (45)), (1: 0.00% (97)), (2: 99.12% (114))
2025-03-01 08:20:27 - INFO - Epoch 18: Train Loss=0.2127, Train Acc=41.62%, Train MSE=1.0839
2025-03-01 08:20:44 - INFO - Epoch 18: Val Loss=0.3637, Val Acc=0.00%
2025-03-01 08:20:44 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 08:20:50 - INFO - Epoch : 18 , Batch [ 0 / 75 ] : Loss = 0.215476, Accuracy = 44.92%, MSE = 1.0078
2025-03-01 08:20:50 - INFO - Classwise accuracy : (0: 0.00% (39)), (1: 0.00% (102)), (2: 100.00% (115))
2025-03-01 08:22:25 - INFO - Epoch : 18 , Batch [ 10 / 75 ] : Loss = 0.202670, Accuracy = 41.41%, MSE = 1.0909
2025-03-01 08:22:25 - INFO - Classwise accuracy : (0: 3.23% (31)), (1: 0.00% (121)), (2: 100.00% (104))
2025-03-01 08:24:20 - INFO - Epoch : 18 , Batch [ 20 / 75 ] : Loss = 0.220941, Accuracy = 41.46%, MSE = 1.0787
2025-03-01 08:24:20 - INFO - Classwise accuracy : (0: 0.00% (46)), (1: 0.00% (113)), (2: 98.97% (97))
2025-03-01 08:26:29 - INFO - Epoch : 18 , Batch [ 30 / 75 ] : Loss = 0.204206, Accuracy = 41.19%, MSE = 1.0871
2025-03-01 08:26:29 - INFO - Classwise accuracy : (0: 12.12% (33)), (1: 0.00% (113)), (2: 95.45% (110))
2025-03-01 08:28:20 - INFO - Epoch : 18 , Batch [ 40 / 75 ] : Loss = 0.214789, Accuracy = 41.82%, MSE = 1.0803
2025-03-01 08:28:20 - INFO - Classwise accuracy : (0: 18.37% (49)), (1: 0.00% (104)), (2: 98.06% (103))
2025-03-01 08:29:52 - INFO - Epoch : 18 , Batch [ 50 / 75 ] : Loss = 0.205549, Accuracy = 41.87%, MSE = 1.0717
2025-03-01 08:29:52 - INFO - Classwise accuracy : (0: 8.33% (36)), (1: 0.00% (101)), (2: 95.80% (119))
2025-03-01 08:31:32 - INFO - Epoch : 18 , Batch [ 60 / 75 ] : Loss = 0.213110, Accuracy = 41.96%, MSE = 1.0655
2025-03-01 08:31:32 - INFO - Classwise accuracy : (0: 4.65% (43)), (1: 0.00% (101)), (2: 99.11% (112))
2025-03-01 08:33:19 - INFO - Epoch : 18 , Batch [ 70 / 75 ] : Loss = 0.206456, Accuracy = 42.17%, MSE = 1.0644
2025-03-01 08:33:19 - INFO - Classwise accuracy : (0: 13.95% (43)), (1: 0.00% (86)), (2: 97.64% (127))
2025-03-01 08:33:53 - INFO - Epoch 19: Train Loss=0.2120, Train Acc=42.12%, Train MSE=1.0653
2025-03-01 08:34:10 - INFO - Epoch 19: Val Loss=0.3249, Val Acc=0.00%
2025-03-01 08:34:10 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 08:34:16 - INFO - Epoch : 19 , Batch [ 0 / 75 ] : Loss = 0.218346, Accuracy = 39.45%, MSE = 1.1328
2025-03-01 08:34:16 - INFO - Classwise accuracy : (0: 0.00% (45)), (1: 0.00% (110)), (2: 100.00% (101))
2025-03-01 08:35:49 - INFO - Epoch : 19 , Batch [ 10 / 75 ] : Loss = 0.210028, Accuracy = 41.69%, MSE = 1.0838
2025-03-01 08:35:49 - INFO - Classwise accuracy : (0: 5.13% (39)), (1: 0.00% (112)), (2: 96.19% (105))
2025-03-01 08:37:33 - INFO - Epoch : 19 , Batch [ 20 / 75 ] : Loss = 0.208607, Accuracy = 41.48%, MSE = 1.0830
2025-03-01 08:37:33 - INFO - Classwise accuracy : (0: 0.00% (39)), (1: 0.00% (103)), (2: 100.00% (114))
2025-03-01 08:39:30 - INFO - Epoch : 19 , Batch [ 30 / 75 ] : Loss = 0.212327, Accuracy = 41.22%, MSE = 1.0906
2025-03-01 08:39:30 - INFO - Classwise accuracy : (0: 2.33% (43)), (1: 0.00% (107)), (2: 99.06% (106))
2025-03-01 08:41:32 - INFO - Epoch : 19 , Batch [ 40 / 75 ] : Loss = 0.208452, Accuracy = 41.33%, MSE = 1.0915
2025-03-01 08:41:32 - INFO - Classwise accuracy : (0: 0.00% (40)), (1: 0.00% (121)), (2: 100.00% (95))
2025-03-01 08:43:31 - INFO - Epoch : 19 , Batch [ 50 / 75 ] : Loss = 0.214151, Accuracy = 41.63%, MSE = 1.0835
2025-03-01 08:43:31 - INFO - Classwise accuracy : (0: 0.00% (45)), (1: 0.00% (101)), (2: 100.00% (110))
2025-03-01 08:45:22 - INFO - Epoch : 19 , Batch [ 60 / 75 ] : Loss = 0.218133, Accuracy = 41.68%, MSE = 1.0846
2025-03-01 08:45:22 - INFO - Classwise accuracy : (0: 0.00% (48)), (1: 0.00% (102)), (2: 100.00% (106))
2025-03-01 08:47:07 - INFO - Epoch : 19 , Batch [ 70 / 75 ] : Loss = 0.226295, Accuracy = 41.52%, MSE = 1.0886
2025-03-01 08:47:07 - INFO - Classwise accuracy : (0: 0.00% (54)), (1: 0.00% (98)), (2: 100.00% (104))
2025-03-01 08:47:48 - INFO - Epoch 20: Train Loss=0.2126, Train Acc=41.60%, Train MSE=1.0868
2025-03-01 08:48:04 - INFO - Epoch 20: Val Loss=0.3400, Val Acc=0.00%
2025-03-01 08:48:04 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 10:20:54 - INFO - Full sequence shape : (201, 81)
2025-03-01 10:20:54 - INFO - Decoder sequence length : 188
2025-03-01 10:20:54 - INFO - Slope value shape : (188,)
2025-03-01 10:20:54 - INFO - Slope values : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 1 0 0 1 1 1 1 1 1 2
 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1]
2025-03-01 10:20:54 - INFO - Target bin max : 1
2025-03-01 10:20:54 - INFO - slope classes : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,
        2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
2025-03-01 10:20:54 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-03-01 10:20:54 - INFO - Future price : 2
2025-03-01 10:20:54 - INFO - X shape : torch.Size([200, 51])
2025-03-01 10:20:54 - INFO - y shape : torch.Size([3])
2025-03-01 10:20:54 - INFO - y : tensor([0., 0., 1.])
2025-03-01 10:20:55 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 10:20:55 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 10:20:55 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 10:20:55 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 10:20:55 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 10:20:56 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 10:20:56 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 10:20:56 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 10:20:59 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 10:20:59 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 10:21:00 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 10:21:00 - INFO - Output s : tensor([[-2.2700, -2.1733, -0.8062],
        [-1.9510, -1.8647, -0.4140],
        [-2.2904, -2.2712, -0.8369],
        [-2.2145, -2.1744, -0.7660],
        [-2.0079, -1.9278, -0.5392],
        [-2.1367, -2.1805, -0.7487],
        [-2.2181, -2.1300, -0.6847],
        [-2.1230, -1.9647, -0.4250],
        [-2.1838, -2.1419, -0.7153],
        [-1.9608, -2.0118, -0.6410],
        [-2.0815, -1.9827, -0.5668],
        [-1.9974, -1.8553, -0.5270],
        [-2.0913, -2.1576, -0.6716],
        [-2.7731, -2.6597, -1.2503],
        [-2.2284, -2.1892, -0.7628],
        [-2.1774, -2.0734, -0.6718],
        [-2.1217, -2.1346, -0.6967],
        [-2.3726, -2.3246, -0.8858],
        [-2.7612, -2.6814, -1.2644],
        [-2.0526, -2.0180, -0.6125],
        [-1.6940, -1.5105, -0.0366],
        [-2.1626, -2.1816, -0.7037],
        [-1.9934, -1.9524, -0.5300],
        [-2.0086, -1.8822, -0.4586],
        [-2.5685, -2.3641, -0.8210],
        [-1.8328, -1.7630, -0.3234],
        [-2.1093, -2.0185, -0.5181],
        [-2.1957, -2.1838, -0.7662],
        [-2.1375, -2.1158, -0.6595],
        [-2.6469, -2.4741, -1.0029],
        [-2.1140, -1.9889, -0.4616],
        [-2.2166, -2.1527, -0.7175],
        [-2.0826, -2.1345, -0.6657],
        [-1.9293, -1.9218, -0.4051],
        [-2.0873, -2.1221, -0.6854],
        [-2.0484, -2.0138, -0.5542],
        [-1.7279, -1.6452, -0.1525],
        [-2.7146, -2.5991, -1.1654],
        [-2.2128, -2.2333, -0.7702],
        [-1.8190, -1.8502, -0.4671],
        [-2.4328, -2.4332, -0.9662],
        [-2.1611, -2.0582, -0.6283],
        [-2.1926, -2.1485, -0.6882],
        [-1.7278, -1.6250, -0.1540],
        [-2.1781, -2.1445, -0.7796],
        [-2.8049, -2.7135, -1.2697],
        [-2.0710, -1.9748, -0.5888],
        [-2.6275, -2.4793, -1.1014],
        [-1.9776, -1.9950, -0.5115],
        [-2.1237, -2.0564, -0.6332],
        [-1.9814, -1.8430, -0.3928],
        [-2.2203, -2.1453, -0.7165],
        [-2.7925, -2.7435, -1.2850],
        [-2.7894, -2.6794, -1.1885],
        [-2.1876, -2.2017, -0.7448],
        [-2.0659, -1.9782, -0.5997],
        [-2.1134, -2.0914, -0.6683],
        [-2.2155, -2.1080, -0.7426],
        [-2.1014, -2.0530, -0.6958],
        [-1.9614, -1.9060, -0.4360],
        [-3.2829, -3.0910, -1.6356],
        [-1.4402, -1.2995,  0.0822],
        [-2.2320, -2.1954, -0.7356],
        [-1.7123, -1.7121, -0.2841],
        [-1.6282, -1.4881, -0.1452],
        [-3.2213, -3.0458, -1.5939],
        [-2.2580, -2.2219, -0.8674],
        [-2.0715, -2.0521, -0.6818],
        [-2.0890, -2.0496, -0.6195],
        [-1.9425, -1.8734, -0.5492],
        [-2.0864, -2.0543, -0.6207],
        [-2.1030, -2.0871, -0.6125],
        [-1.9358, -1.8732, -0.5353],
        [-2.2152, -2.1245, -0.7473],
        [-2.1459, -2.0796, -0.6736],
        [-2.1229, -2.1236, -0.6913],
        [-2.1007, -2.0658, -0.6194],
        [-2.0745, -2.0633, -0.6327],
        [-2.1771, -2.1539, -0.7071],
        [-2.6247, -2.5945, -1.2078],
        [-2.0671, -2.1161, -0.6809],
        [-2.1064, -2.0225, -0.5439],
        [-1.6859, -1.6296, -0.1971],
        [-1.9712, -1.9432, -0.4790],
        [-1.9587, -1.8840, -0.4341],
        [-2.1296, -2.1657, -0.7021],
        [-2.3722, -2.3597, -1.0401],
        [-2.0689, -2.1674, -0.7269],
        [-2.1834, -2.2001, -0.7553],
        [-2.0605, -1.8620, -0.4615],
        [-2.1209, -2.2165, -0.7646],
        [-1.6289, -1.4887,  0.0678],
        [-2.0736, -1.9725, -0.5556],
        [-2.5453, -2.4656, -0.9798],
        [-2.0816, -1.9494, -0.5514],
        [-2.2178, -2.2097, -0.7469],
        [-1.7484, -1.8004, -0.4186],
        [-1.6622, -1.5929, -0.1932],
        [-1.9402, -1.8999, -0.4989],
        [-2.2733, -2.1614, -0.7214],
        [-2.2679, -2.1707, -0.7440],
        [-2.0558, -2.1019, -0.7404],
        [-2.1752, -2.2146, -0.7413],
        [-2.1276, -2.0775, -0.6057],
        [-2.1443, -2.1028, -0.8182],
        [-2.1797, -2.1404, -0.6483],
        [-1.9816, -1.9665, -0.4711],
        [-2.1431, -2.1680, -0.7678],
        [-2.0750, -1.8497, -0.3946],
        [-2.0442, -2.0758, -0.6648],
        [-2.0222, -1.9233, -0.4700],
        [-1.9075, -1.9166, -0.4533],
        [-1.7258, -1.7176, -0.2987],
        [-2.0591, -2.0430, -0.5834],
        [-2.5423, -2.4080, -1.0162],
        [-2.0565, -2.0213, -0.5894],
        [-1.9215, -1.7930, -0.4542],
        [-1.9540, -2.0162, -0.6242],
        [-2.0047, -1.8688, -0.4546],
        [-1.9706, -1.9552, -0.4485],
        [-2.8378, -2.7640, -1.3162],
        [-2.0539, -2.0380, -0.5203],
        [-2.6844, -2.5651, -1.0977],
        [-1.8807, -1.5442,  0.0097],
        [-2.0666, -1.9988, -0.5311],
        [-1.9552, -1.8200, -0.3962],
        [-2.1034, -2.0743, -0.5874],
        [-1.8400, -1.7693, -0.2844],
        [-2.0719, -1.9296, -0.5507],
        [-2.7424, -2.5923, -1.1434],
        [-2.0971, -2.0170, -0.6487],
        [-2.2102, -2.2471, -0.8078],
        [-1.2585, -1.1687,  0.2567],
        [-2.0493, -2.0870, -0.6342],
        [-1.9785, -2.0436, -0.5305],
        [-2.1819, -2.1342, -0.6511],
        [-2.1529, -2.1975, -0.7436],
        [-2.2634, -2.1694, -0.6899],
        [-2.1626, -2.1814, -0.7304],
        [-2.1477, -2.1745, -0.7967],
        [-2.0399, -1.9388, -0.5102],
        [-2.3579, -2.2106, -0.8476],
        [-2.1376, -2.0237, -0.5158],
        [-2.2663, -2.2327, -0.8304],
        [-1.8791, -1.7912, -0.2813],
        [-2.1426, -2.1793, -0.7841],
        [-2.6292, -2.4473, -0.9735],
        [-2.5535, -2.4471, -1.0226],
        [-1.6478, -1.3038,  0.1230],
        [-2.1514, -2.1155, -0.7515],
        [-1.9349, -1.9107, -0.4558],
        [-2.2985, -2.2109, -0.7641],
        [-2.8929, -2.6979, -1.1812],
        [-2.6252, -2.5641, -1.0677],
        [-1.9659, -1.8882, -0.4357],
        [-2.8028, -2.7683, -1.3488],
        [-1.3195, -1.2156,  0.1774],
        [-2.0300, -2.0982, -0.5852],
        [-1.8225, -1.7356, -0.3138],
        [-2.2748, -2.1608, -0.7454],
        [-2.0762, -2.0303, -0.5932],
        [-2.1018, -1.9793, -0.5274],
        [-2.0058, -1.9736, -0.4644],
        [-2.1157, -1.9793, -0.5727],
        [-1.8618, -1.7913, -0.3823],
        [-1.9894, -1.9375, -0.5071],
        [-2.0659, -2.0781, -0.6723],
        [-2.1984, -2.1704, -0.6798],
        [-2.2291, -2.1869, -0.6741],
        [-1.5391, -1.4449, -0.0737],
        [-2.1837, -2.1419, -0.7138],
        [-2.0833, -2.0310, -0.6391],
        [-1.4818, -1.2312,  0.2344],
        [-1.6258, -1.5787, -0.1355],
        [-2.1699, -2.1162, -0.7313],
        [-1.7303, -1.5972, -0.1972],
        [-2.0279, -1.9295, -0.5540],
        [-2.1074, -2.0328, -0.6504],
        [-2.0220, -1.9658, -0.5144],
        [-2.1370, -2.0861, -0.7391],
        [-2.1190, -2.0945, -0.6563],
        [-1.9938, -1.8683, -0.3947],
        [-2.8026, -2.5677, -1.0186],
        [-1.9893, -1.8897, -0.5293],
        [-2.2405, -2.1621, -0.7881],
        [-1.7118, -1.6247, -0.1845],
        [-2.2062, -2.0506, -0.6287],
        [-1.7827, -1.7967, -0.4946],
        [-1.8773, -1.8187, -0.3928],
        [-2.5687, -2.5109, -1.0865],
        [-0.9876, -0.7005,  0.8899],
        [-1.4997, -1.2246,  0.2189],
        [-2.1162, -2.0995, -0.7433],
        [-2.1770, -2.2775, -0.8712],
        [-1.9877, -2.0117, -0.5158],
        [-1.9098, -1.8065, -0.4342],
        [-1.9909, -1.8734, -0.4212],
        [-2.0704, -2.0981, -0.7449],
        [-2.1736, -2.1010, -0.6722],
        [-1.5058, -1.3651,  0.1327],
        [-2.2934, -2.2320, -0.7802],
        [-2.5283, -2.4565, -1.0732],
        [-1.9656, -1.8480, -0.3596],
        [-2.6476, -2.4987, -0.9957],
        [-2.1908, -2.1170, -0.7193],
        [-2.5433, -2.5113, -1.0395],
        [-2.1748, -2.2034, -0.7964],
        [-2.0637, -2.0834, -0.6624],
        [-2.8250, -2.6499, -1.1909],
        [-2.3113, -2.2141, -0.7689],
        [-2.0981, -2.0493, -0.5874],
        [-2.1601, -2.1855, -0.8151],
        [-2.0795, -1.9824, -0.5683],
        [-2.8749, -2.7822, -1.3172],
        [-2.1698, -2.1471, -0.7561],
        [-1.6349, -1.5024, -0.1029],
        [-2.0077, -1.9390, -0.4512],
        [-2.2212, -2.0527, -0.6189],
        [-2.0701, -2.0950, -0.7012],
        [-2.0679, -2.0796, -0.6817],
        [-1.9651, -1.9190, -0.4958],
        [-2.2150, -2.0956, -0.6471],
        [-2.6119, -2.4109, -0.9506],
        [-2.4956, -2.4324, -1.0387],
        [-1.9738, -1.8454, -0.4735],
        [-2.2732, -2.3028, -0.8075],
        [-2.0804, -1.9836, -0.5918],
        [-2.5128, -2.4371, -0.9969],
        [-1.8155, -1.6686, -0.1616],
        [-1.8434, -1.7465, -0.3836],
        [-2.0641, -2.0278, -0.6716],
        [-2.0483, -1.9551, -0.5462],
        [-1.6132, -1.5429, -0.1387],
        [-2.1652, -2.1578, -0.7534],
        [-2.0914, -2.0321, -0.5016],
        [-1.9424, -1.8628, -0.3872],
        [-1.9114, -1.8216, -0.4585],
        [-2.1901, -2.2099, -0.7479],
        [-2.1158, -2.0754, -0.6780],
        [-1.8895, -1.9239, -0.4449],
        [-2.7372, -2.5754, -1.1403],
        [-1.8216, -1.6523, -0.2073],
        [-2.1520, -2.1536, -0.7126],
        [-2.5496, -2.4808, -0.9725],
        [-2.0321, -2.1167, -0.6567],
        [-2.1503, -2.1311, -0.6597],
        [-2.8696, -2.6472, -1.1706],
        [-1.7915, -1.8593, -0.5061],
        [-2.1639, -2.1234, -0.6774],
        [-2.0497, -2.1198, -0.6976],
        [-2.7039, -2.5832, -1.1200],
        [-1.7388, -1.6913, -0.3031],
        [-2.1743, -2.1234, -0.7339],
        [-1.9995, -1.9806, -0.5766],
        [-1.9417, -1.8228, -0.3785],
        [-2.8485, -2.7903, -1.3509]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 10:21:00 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 10:21:00 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 10:21:00 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 10:21:05 - INFO - Epoch : 15 , Batch [ 0 / 75 ] : Loss = 0.356445, Accuracy = 41.02%, MSE = 1.2227
2025-03-01 10:21:05 - INFO - Classwise accuracy : (0: 0.00% (54)), (1: 0.00% (97)), (2: 100.00% (105))
2025-03-01 10:22:04 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 10:22:16 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 10:22:16 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 10:22:16 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 10:22:22 - INFO - Epoch : 15 , Batch [ 0 / 75 ] : Loss = 0.259790, Accuracy = 38.28%, MSE = 1.1211
2025-03-01 10:22:22 - INFO - Classwise accuracy : (0: 0.00% (43)), (1: 0.00% (115)), (2: 100.00% (98))
2025-03-01 10:24:41 - INFO - Epoch : 15 , Batch [ 10 / 75 ] : Loss = 0.142705, Accuracy = 48.30%, MSE = 0.7930
2025-03-01 10:24:41 - INFO - Classwise accuracy : (0: 86.00% (50)), (1: 0.00% (111)), (2: 92.63% (95))
2025-03-01 10:27:41 - INFO - Epoch : 15 , Batch [ 20 / 75 ] : Loss = 0.145812, Accuracy = 49.31%, MSE = 0.7312
2025-03-01 10:27:41 - INFO - Classwise accuracy : (0: 75.51% (49)), (1: 0.00% (99)), (2: 93.52% (108))
2025-03-01 10:46:11 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 10:46:11 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 10:46:11 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 10:46:11 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 10:46:11 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 10:46:18 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 10:46:18 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 10:46:18 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 10:46:36 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 10:46:36 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 10:46:36 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 10:46:36 - INFO - Output s : tensor([[-1.7338e+00, -1.9308e+00, -5.7457e-01],
        [-1.9607e+00, -2.1222e+00, -7.8205e-01],
        [-1.9420e+00, -2.1819e+00, -8.5533e-01],
        [-2.1264e+00, -2.1267e+00, -7.5576e-01],
        [-2.2104e+00, -2.1751e+00, -7.3332e-01],
        [-2.0864e+00, -2.1133e+00, -7.3672e-01],
        [-1.9031e+00, -2.0643e+00, -7.7436e-01],
        [-2.7860e+00, -2.6729e+00, -1.2314e+00],
        [-2.4118e+00, -2.2099e+00, -7.9743e-01],
        [-1.9996e+00, -2.0529e+00, -5.8333e-01],
        [-2.1819e+00, -2.1184e+00, -6.5239e-01],
        [-2.1374e+00, -2.1777e+00, -8.4254e-01],
        [-2.1311e+00, -2.1654e+00, -8.1547e-01],
        [-2.2677e+00, -2.2964e+00, -9.0925e-01],
        [-2.0583e+00, -1.9843e+00, -5.6715e-01],
        [-2.2705e+00, -2.2681e+00, -9.0634e-01],
        [-2.5841e+00, -2.4672e+00, -1.0659e+00],
        [-2.3344e+00, -2.2179e+00, -8.0627e-01],
        [-1.6416e+00, -1.7634e+00, -5.0373e-01],
        [-1.5882e+00, -1.8431e+00, -5.5733e-01],
        [-1.5766e+00, -1.5973e+00, -2.0800e-01],
        [-2.0724e+00, -2.1412e+00, -7.5683e-01],
        [-2.0063e+00, -2.1638e+00, -7.8972e-01],
        [-2.3948e+00, -2.4273e+00, -1.0558e+00],
        [-2.0675e+00, -2.1150e+00, -7.2493e-01],
        [-1.8053e+00, -2.1019e+00, -8.0928e-01],
        [-2.1760e+00, -2.1271e+00, -6.7140e-01],
        [-2.2473e+00, -2.3569e+00, -1.0341e+00],
        [-2.1945e+00, -1.9221e+00, -4.3007e-01],
        [-1.5029e+00, -1.4714e+00, -1.5436e-01],
        [-1.7978e+00, -1.9165e+00, -5.9808e-01],
        [-1.8498e+00, -1.9442e+00, -5.2669e-01],
        [-1.8201e+00, -1.9466e+00, -5.5311e-01],
        [-1.9967e+00, -2.0278e+00, -6.4740e-01],
        [-2.0520e+00, -2.0546e+00, -7.5832e-01],
        [-2.2215e+00, -2.2256e+00, -8.1202e-01],
        [-1.9251e+00, -1.9605e+00, -6.4416e-01],
        [-2.3445e+00, -2.4218e+00, -1.0546e+00],
        [-2.3859e+00, -2.6390e+00, -1.4479e+00],
        [-2.6255e+00, -2.7088e+00, -1.3432e+00],
        [-1.8016e+00, -1.7423e+00, -3.3410e-01],
        [-1.9276e+00, -1.8009e+00, -4.6841e-01],
        [-1.9365e+00, -2.1314e+00, -7.8812e-01],
        [-2.1311e+00, -2.1802e+00, -8.5348e-01],
        [-2.1868e+00, -2.1061e+00, -6.6288e-01],
        [-1.9389e+00, -2.0425e+00, -5.9364e-01],
        [-1.8939e+00, -1.8141e+00, -4.8765e-01],
        [-2.0174e+00, -2.0600e+00, -7.4348e-01],
        [-1.9211e+00, -2.1427e+00, -8.7588e-01],
        [-2.0988e+00, -2.0746e+00, -7.1320e-01],
        [-1.9784e+00, -1.9505e+00, -4.8662e-01],
        [-1.7857e+00, -1.8062e+00, -4.5704e-01],
        [-1.8292e+00, -1.8878e+00, -5.4513e-01],
        [-2.0140e+00, -2.0930e+00, -7.8894e-01],
        [-2.0062e+00, -1.9111e+00, -4.8665e-01],
        [-1.6025e+00, -1.6053e+00, -2.1941e-01],
        [-2.0769e+00, -2.1755e+00, -7.8829e-01],
        [-1.8716e+00, -1.8192e+00, -3.2112e-01],
        [-2.0369e+00, -2.1807e+00, -8.2162e-01],
        [-2.2196e+00, -2.1259e+00, -6.9512e-01],
        [-2.1785e+00, -2.1519e+00, -6.7378e-01],
        [-2.2129e+00, -2.1481e+00, -7.4098e-01],
        [-2.7546e+00, -2.6918e+00, -1.1190e+00],
        [-2.1308e+00, -2.1670e+00, -7.5623e-01],
        [-1.9930e+00, -2.2391e+00, -9.4617e-01],
        [-1.6937e+00, -2.1001e+00, -8.6372e-01],
        [-1.8882e+00, -1.8687e+00, -4.6944e-01],
        [-1.8354e+00, -2.0219e+00, -6.4919e-01],
        [-1.9641e+00, -2.0129e+00, -5.1828e-01],
        [-1.9285e+00, -1.9248e+00, -6.3872e-01],
        [-2.0464e+00, -2.0919e+00, -6.8802e-01],
        [-2.1437e+00, -2.1323e+00, -6.9208e-01],
        [-2.1131e+00, -2.1005e+00, -6.1023e-01],
        [-1.9101e+00, -2.0548e+00, -7.0767e-01],
        [-2.2451e+00, -2.1196e+00, -7.7010e-01],
        [-1.8416e+00, -1.9493e+00, -7.3293e-01],
        [-1.6310e+00, -1.4591e+00, -3.1350e-02],
        [-2.0746e+00, -2.1661e+00, -8.0922e-01],
        [-1.9757e+00, -1.9578e+00, -5.1614e-01],
        [-2.0165e+00, -2.1689e+00, -8.2298e-01],
        [-2.6609e+00, -2.3609e+00, -9.5794e-01],
        [-2.0133e+00, -2.0878e+00, -6.8140e-01],
        [-1.8870e+00, -1.9906e+00, -5.9043e-01],
        [-1.9996e+00, -2.0497e+00, -6.6783e-01],
        [-2.0486e+00, -2.0946e+00, -7.4645e-01],
        [-2.0501e+00, -2.1087e+00, -6.9118e-01],
        [-2.3348e+00, -2.3184e+00, -9.5944e-01],
        [-1.8999e+00, -2.0713e+00, -7.4602e-01],
        [-2.0446e+00, -2.1079e+00, -7.6221e-01],
        [-1.9827e+00, -2.1016e+00, -7.0774e-01],
        [-1.9347e+00, -2.0142e+00, -6.6045e-01],
        [-2.0545e+00, -2.0541e+00, -6.2606e-01],
        [-2.5648e+00, -2.6093e+00, -1.1890e+00],
        [-1.5802e+00, -1.8130e+00, -4.8076e-01],
        [-1.9520e+00, -1.9245e+00, -4.9589e-01],
        [-1.7284e+00, -1.6141e+00, -2.5290e-01],
        [-2.1008e+00, -2.2529e+00, -8.8633e-01],
        [-1.8383e+00, -1.8560e+00, -4.7935e-01],
        [-2.1275e+00, -2.2512e+00, -8.0484e-01],
        [-2.0806e+00, -2.0968e+00, -6.5365e-01],
        [-2.1680e+00, -2.1233e+00, -6.0633e-01],
        [-2.5286e+00, -2.4530e+00, -1.1348e+00],
        [-2.0575e+00, -2.1856e+00, -8.4978e-01],
        [-1.8284e+00, -1.9401e+00, -4.7746e-01],
        [-2.5834e+00, -2.7026e+00, -1.3152e+00],
        [-1.5878e+00, -1.8027e+00, -4.5213e-01],
        [-2.0550e+00, -2.1559e+00, -8.0588e-01],
        [-1.9301e+00, -1.9967e+00, -6.4439e-01],
        [-2.1823e+00, -2.0973e+00, -7.1384e-01],
        [-2.5428e+00, -2.5824e+00, -1.2065e+00],
        [-1.8685e+00, -2.1439e+00, -8.0170e-01],
        [-1.9526e+00, -2.1463e+00, -7.9839e-01],
        [-1.6658e+00, -1.5264e+00, -1.2708e-01],
        [-2.1362e+00, -2.0785e+00, -6.2199e-01],
        [-2.1187e+00, -2.1423e+00, -8.4179e-01],
        [-1.7911e+00, -1.8205e+00, -3.2863e-01],
        [-2.0376e+00, -1.9724e+00, -5.0741e-01],
        [-2.1196e+00, -2.0837e+00, -6.6213e-01],
        [-2.7437e+00, -2.7141e+00, -1.2674e+00],
        [-1.3124e+00, -1.6282e+00, -3.9244e-01],
        [-1.9357e+00, -1.8762e+00, -5.2129e-01],
        [-1.7650e+00, -1.8476e+00, -4.7892e-01],
        [-2.4318e+00, -2.3282e+00, -8.5219e-01],
        [-1.9225e+00, -2.0069e+00, -5.1933e-01],
        [-1.8778e+00, -2.0006e+00, -5.8977e-01],
        [-1.9780e+00, -2.1747e+00, -8.1648e-01],
        [-2.3184e+00, -2.4615e+00, -1.1462e+00],
        [-2.2420e+00, -2.1631e+00, -7.2027e-01],
        [-2.0258e+00, -2.1117e+00, -7.7686e-01],
        [-2.6106e+00, -2.7173e+00, -1.2467e+00],
        [-2.2098e+00, -2.2236e+00, -8.1494e-01],
        [-1.9592e+00, -1.9348e+00, -5.5171e-01],
        [-2.2600e+00, -2.2052e+00, -7.4150e-01],
        [-1.9858e+00, -2.1039e+00, -7.5644e-01],
        [-2.1328e+00, -2.1137e+00, -6.6992e-01],
        [-1.9934e+00, -2.0630e+00, -7.9574e-01],
        [-2.0689e+00, -2.0857e+00, -5.9272e-01],
        [-1.8600e+00, -2.1566e+00, -9.5158e-01],
        [-2.1137e+00, -2.1838e+00, -6.9911e-01],
        [-1.5866e+00, -1.7811e+00, -4.9125e-01],
        [-1.9486e+00, -1.9919e+00, -5.8422e-01],
        [-1.9519e+00, -2.1341e+00, -7.6393e-01],
        [-1.9682e+00, -1.9306e+00, -5.2123e-01],
        [-1.4620e+00, -1.5998e+00, -1.4883e-01],
        [-1.6739e+00, -2.0417e+00, -7.8660e-01],
        [-1.2405e+00, -1.4374e+00, -5.9481e-02],
        [-2.0694e+00, -2.0041e+00, -6.4445e-01],
        [-2.0440e+00, -2.4443e+00, -1.2769e+00],
        [-1.9181e+00, -1.9364e+00, -5.4012e-01],
        [-2.0132e+00, -2.1865e+00, -8.1554e-01],
        [-1.7165e+00, -1.7401e+00, -3.5035e-01],
        [-1.6644e+00, -1.6359e+00, -3.3156e-01],
        [-1.9767e+00, -2.0409e+00, -6.0918e-01],
        [-1.6537e+00, -1.7176e+00, -2.8805e-01],
        [-1.9763e+00, -2.1228e+00, -7.6607e-01],
        [-2.3959e+00, -2.3379e+00, -9.2148e-01],
        [-1.7344e+00, -1.5791e+00, -5.5691e-02],
        [-1.8351e+00, -1.9372e+00, -6.8232e-01],
        [-1.8134e+00, -2.0027e+00, -6.1090e-01],
        [-1.4868e+00, -1.4288e+00,  1.0329e-03],
        [-1.9244e+00, -2.0430e+00, -7.7030e-01],
        [-1.9194e+00, -1.9696e+00, -5.2105e-01],
        [-1.9258e+00, -1.7552e+00, -2.8925e-01],
        [-1.8803e+00, -1.8919e+00, -4.7460e-01],
        [-1.8521e+00, -1.9396e+00, -4.8117e-01],
        [-1.8296e+00, -1.8674e+00, -5.3830e-01],
        [-2.8475e+00, -2.7863e+00, -1.3810e+00],
        [-1.9381e+00, -1.9865e+00, -5.5253e-01],
        [-1.9552e+00, -1.9692e+00, -6.4498e-01],
        [-2.9290e+00, -2.9775e+00, -1.5311e+00],
        [-2.2223e+00, -2.1798e+00, -7.0753e-01],
        [-2.2634e+00, -2.4120e+00, -1.1929e+00],
        [-2.0650e+00, -2.0941e+00, -6.8790e-01],
        [-2.5532e+00, -2.4539e+00, -1.0000e+00],
        [-2.4398e+00, -2.4349e+00, -9.7693e-01],
        [-1.5037e+00, -1.5542e+00, -2.4747e-01],
        [-2.0780e+00, -2.2486e+00, -8.9019e-01],
        [-1.5521e+00, -1.3842e+00,  1.2774e-01],
        [-2.1982e+00, -2.3345e+00, -9.9169e-01],
        [-1.8894e+00, -1.8391e+00, -4.8662e-01],
        [-1.7646e+00, -1.6008e+00, -1.9394e-01],
        [-1.9141e+00, -1.8554e+00, -3.7137e-01],
        [-1.8036e+00, -1.6858e+00, -2.6211e-01],
        [-1.7431e+00, -1.6322e+00, -2.4663e-01],
        [-2.0501e+00, -2.1186e+00, -8.5939e-01],
        [-1.2018e+00, -1.2475e+00,  2.2083e-01],
        [-1.5727e+00, -1.5697e+00, -1.2393e-01],
        [-1.8832e+00, -2.1970e+00, -9.9962e-01],
        [-1.6667e+00, -1.6205e+00, -2.1981e-01],
        [-1.8347e+00, -1.8399e+00, -4.0761e-01],
        [-2.0031e+00, -1.9472e+00, -5.0949e-01],
        [-1.8259e+00, -1.9147e+00, -5.6732e-01],
        [-1.9645e+00, -1.9662e+00, -5.4064e-01],
        [-1.9048e+00, -2.0302e+00, -6.4035e-01],
        [-2.0232e+00, -2.0746e+00, -6.8832e-01],
        [-1.9253e+00, -1.9716e+00, -6.5455e-01],
        [-2.2241e+00, -2.2261e+00, -8.0399e-01],
        [-2.7465e+00, -2.6446e+00, -1.2484e+00],
        [-1.9254e+00, -1.8888e+00, -4.8390e-01],
        [-1.8398e+00, -1.9739e+00, -5.6845e-01],
        [-3.0122e+00, -2.8622e+00, -1.4850e+00],
        [-2.1677e+00, -2.1341e+00, -8.0208e-01],
        [-1.6537e+00, -1.5787e+00, -2.0285e-01],
        [-2.1408e+00, -2.1952e+00, -8.1212e-01],
        [-2.1176e+00, -2.2279e+00, -7.9529e-01],
        [-2.1423e+00, -2.2173e+00, -8.2755e-01],
        [-2.1126e+00, -2.1991e+00, -7.8456e-01],
        [-2.5194e+00, -2.4345e+00, -1.0058e+00],
        [-2.0635e+00, -2.2065e+00, -8.4134e-01],
        [-2.1751e+00, -2.2302e+00, -9.6971e-01],
        [-1.4681e+00, -1.4762e+00, -1.8369e-01],
        [-1.6628e+00, -1.6917e+00, -3.4853e-01],
        [-2.1136e+00, -2.2482e+00, -8.0588e-01],
        [-2.2746e+00, -2.3331e+00, -1.0255e+00],
        [-1.6777e+00, -1.5651e+00, -1.6442e-01],
        [-1.9243e+00, -2.1409e+00, -7.6391e-01],
        [-2.0256e+00, -1.9479e+00, -4.7023e-01],
        [-2.1416e+00, -2.2849e+00, -9.4042e-01],
        [-1.9410e+00, -2.1379e+00, -7.4917e-01],
        [-3.0460e+00, -3.1512e+00, -1.8547e+00],
        [-1.5927e+00, -1.5535e+00, -1.3431e-01],
        [-2.6146e+00, -2.5115e+00, -1.1259e+00],
        [-2.0945e+00, -1.9981e+00, -6.1861e-01],
        [-1.4217e+00, -1.4369e+00,  9.0747e-02],
        [-1.8425e+00, -1.8939e+00, -5.3693e-01],
        [-1.7879e+00, -1.9308e+00, -5.0963e-01],
        [-1.2977e+00, -1.1576e+00,  3.6644e-01],
        [-2.1690e+00, -2.1342e+00, -6.8006e-01],
        [-1.9083e+00, -1.8367e+00, -3.9001e-01],
        [-2.1869e+00, -2.1485e+00, -6.5939e-01],
        [-1.7788e+00, -1.8463e+00, -4.6033e-01],
        [-2.1352e+00, -2.1436e+00, -7.6396e-01],
        [-1.6780e+00, -1.6085e+00, -3.0358e-01],
        [-2.0637e+00, -2.1609e+00, -8.1348e-01],
        [-2.2019e+00, -2.1422e+00, -7.3128e-01],
        [-2.0670e+00, -2.1559e+00, -7.8483e-01],
        [-1.9718e+00, -1.9391e+00, -6.1452e-01],
        [-2.2373e+00, -2.2219e+00, -7.7410e-01],
        [-2.1531e+00, -2.1290e+00, -7.2023e-01],
        [-2.1116e+00, -2.1907e+00, -7.7423e-01],
        [-1.9149e+00, -2.1327e+00, -8.4117e-01],
        [-1.7099e+00, -1.6660e+00, -3.7798e-01],
        [-1.9816e+00, -1.8870e+00, -4.4056e-01],
        [-1.8946e+00, -1.9098e+00, -5.4171e-01],
        [-1.6776e+00, -1.6401e+00, -2.1188e-01],
        [-1.9761e+00, -2.0018e+00, -6.6309e-01],
        [-2.0051e+00, -2.0244e+00, -6.4073e-01],
        [-1.8328e+00, -2.0619e+00, -7.4027e-01],
        [-1.8474e+00, -1.8290e+00, -4.0960e-01],
        [-2.8880e+00, -2.9706e+00, -1.6205e+00],
        [-2.1414e+00, -2.1548e+00, -7.6722e-01],
        [-3.0243e+00, -3.0224e+00, -1.6438e+00],
        [-2.1665e+00, -2.1482e+00, -6.7689e-01],
        [-2.0593e+00, -2.1218e+00, -7.0360e-01],
        [-1.2563e+00, -1.1879e+00,  3.1944e-01],
        [-2.1163e+00, -2.2588e+00, -9.2171e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 10:46:36 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 10:46:36 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 10:46:36 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 10:46:59 - INFO - Epoch : 15 , Batch [ 0 / 75 ] : Loss = 0.327771, Accuracy = 44.14%, MSE = 0.9805
2025-03-01 10:46:59 - INFO - Classwise accuracy : (0: 0.00% (36)), (1: 0.00% (107)), (2: 100.00% (113))
2025-03-01 10:57:17 - INFO - Epoch : 15 , Batch [ 10 / 75 ] : Loss = 0.188771, Accuracy = 43.61%, MSE = 0.9986
2025-03-01 10:57:17 - INFO - Classwise accuracy : (0: 81.08% (37)), (1: 0.00% (103)), (2: 93.97% (116))
2025-03-01 11:07:28 - INFO - Epoch : 15 , Batch [ 20 / 75 ] : Loss = 0.259567, Accuracy = 45.09%, MSE = 0.9336
2025-03-01 11:07:28 - INFO - Classwise accuracy : (0: 29.55% (44)), (1: 0.00% (106)), (2: 100.00% (106))
2025-03-01 11:17:25 - INFO - Epoch : 15 , Batch [ 30 / 75 ] : Loss = 0.189159, Accuracy = 47.24%, MSE = 0.8535
2025-03-01 11:17:25 - INFO - Classwise accuracy : (0: 79.55% (44)), (1: 0.00% (119)), (2: 94.62% (93))
2025-03-01 11:27:40 - INFO - Epoch : 15 , Batch [ 40 / 75 ] : Loss = 0.177682, Accuracy = 48.69%, MSE = 0.8067
2025-03-01 11:27:40 - INFO - Classwise accuracy : (0: 83.33% (36)), (1: 0.00% (113)), (2: 95.33% (107))
2025-03-01 11:38:04 - INFO - Epoch : 15 , Batch [ 50 / 75 ] : Loss = 0.164768, Accuracy = 49.54%, MSE = 0.7739
2025-03-01 11:38:04 - INFO - Classwise accuracy : (0: 80.49% (41)), (1: 0.00% (107)), (2: 96.30% (108))
2025-03-01 11:47:32 - INFO - Epoch : 15 , Batch [ 60 / 75 ] : Loss = 0.151811, Accuracy = 50.01%, MSE = 0.7501
2025-03-01 11:47:32 - INFO - Classwise accuracy : (0: 87.10% (31)), (1: 0.00% (109)), (2: 97.41% (116))
2025-03-01 11:59:16 - INFO - Epoch : 15 , Batch [ 70 / 75 ] : Loss = 0.158275, Accuracy = 50.56%, MSE = 0.7286
2025-03-01 11:59:16 - INFO - Classwise accuracy : (0: 89.58% (48)), (1: 0.00% (99)), (2: 97.25% (109))
2025-03-01 12:03:12 - INFO - Epoch 15: Train Loss=0.2083, Train Acc=50.70%, Train MSE=0.7203
2025-03-01 12:03:32 - INFO - Epoch 15: Val Loss=0.3348, Val Acc=100.00%
2025-03-01 12:03:32 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       1.00      1.00      1.00      4767
        Hold       0.00      0.00      0.00         0
         Buy       0.00      0.00      0.00         0

    accuracy                           1.00      4767
   macro avg       0.33      0.33      0.33      4767
weighted avg       1.00      1.00      1.00      4767

2025-03-01 12:04:02 - INFO - Epoch : 16 , Batch [ 0 / 75 ] : Loss = 0.191921, Accuracy = 52.34%, MSE = 0.5820
2025-03-01 12:04:02 - INFO - Classwise accuracy : (0: 86.84% (38)), (1: 0.00% (113)), (2: 96.19% (105))
2025-03-01 12:16:02 - INFO - Epoch : 16 , Batch [ 10 / 75 ] : Loss = 0.187423, Accuracy = 45.95%, MSE = 0.8675
2025-03-01 12:16:02 - INFO - Classwise accuracy : (0: 85.45% (55)), (1: 1.00% (100)), (2: 99.01% (101))
2025-03-01 12:26:53 - INFO - Epoch : 16 , Batch [ 20 / 75 ] : Loss = 0.183228, Accuracy = 49.74%, MSE = 0.7470
2025-03-01 12:26:53 - INFO - Classwise accuracy : (0: 91.30% (46)), (1: 0.00% (105)), (2: 95.24% (105))
2025-03-01 12:37:12 - INFO - Epoch : 16 , Batch [ 30 / 75 ] : Loss = 0.297593, Accuracy = 51.61%, MSE = 0.6823
2025-03-01 12:37:12 - INFO - Classwise accuracy : (0: 100.00% (44)), (1: 1.96% (102)), (2: 74.55% (110))
2025-03-01 12:49:21 - INFO - Epoch : 16 , Batch [ 40 / 75 ] : Loss = 0.179433, Accuracy = 52.32%, MSE = 0.6466
2025-03-01 12:49:21 - INFO - Classwise accuracy : (0: 83.33% (48)), (1: 0.98% (102)), (2: 98.11% (106))
2025-03-01 13:00:39 - INFO - Epoch : 16 , Batch [ 50 / 75 ] : Loss = 0.215465, Accuracy = 53.08%, MSE = 0.6287
2025-03-01 13:00:39 - INFO - Classwise accuracy : (0: 65.38% (52)), (1: 6.60% (106)), (2: 98.98% (98))
2025-03-01 13:10:21 - INFO - Epoch : 16 , Batch [ 60 / 75 ] : Loss = 0.156779, Accuracy = 53.52%, MSE = 0.6128
2025-03-01 13:10:21 - INFO - Classwise accuracy : (0: 89.47% (38)), (1: 0.85% (118)), (2: 100.00% (100))
2025-03-01 13:22:03 - INFO - Epoch : 16 , Batch [ 70 / 75 ] : Loss = 0.178514, Accuracy = 53.89%, MSE = 0.5983
2025-03-01 13:22:03 - INFO - Classwise accuracy : (0: 84.21% (38)), (1: 3.74% (107)), (2: 93.69% (111))
2025-03-01 13:25:25 - INFO - Epoch 16: Train Loss=0.1972, Train Acc=54.05%, Train MSE=0.5940
2025-03-01 13:25:42 - INFO - Epoch 16: Val Loss=0.3772, Val Acc=0.00%
2025-03-01 13:25:42 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 13:26:11 - INFO - Epoch : 17 , Batch [ 0 / 75 ] : Loss = 0.148300, Accuracy = 59.38%, MSE = 0.4297
2025-03-01 13:26:11 - INFO - Classwise accuracy : (0: 91.89% (37)), (1: 3.88% (103)), (2: 98.28% (116))
2025-03-01 13:36:56 - INFO - Epoch : 17 , Batch [ 10 / 75 ] : Loss = 0.188801, Accuracy = 55.86%, MSE = 0.5586
2025-03-01 13:36:56 - INFO - Classwise accuracy : (0: 64.44% (45)), (1: 12.26% (106)), (2: 99.05% (105))
2025-03-01 13:47:24 - INFO - Epoch : 17 , Batch [ 20 / 75 ] : Loss = 0.171984, Accuracy = 56.88%, MSE = 0.5433
2025-03-01 13:47:24 - INFO - Classwise accuracy : (0: 86.11% (36)), (1: 7.34% (109)), (2: 99.10% (111))
2025-03-01 13:57:24 - INFO - Epoch : 17 , Batch [ 30 / 75 ] : Loss = 0.214032, Accuracy = 56.70%, MSE = 0.5282
2025-03-01 13:57:24 - INFO - Classwise accuracy : (0: 92.59% (54)), (1: 2.75% (109)), (2: 94.62% (93))
2025-03-01 14:07:05 - INFO - Epoch : 17 , Batch [ 40 / 75 ] : Loss = 0.197866, Accuracy = 56.99%, MSE = 0.5192
2025-03-01 14:07:05 - INFO - Classwise accuracy : (0: 86.67% (45)), (1: 10.62% (113)), (2: 89.80% (98))
2025-03-01 14:17:45 - INFO - Epoch : 17 , Batch [ 50 / 75 ] : Loss = 0.224290, Accuracy = 57.15%, MSE = 0.5138
2025-03-01 14:17:45 - INFO - Classwise accuracy : (0: 57.78% (45)), (1: 13.64% (110)), (2: 99.01% (101))
2025-03-01 14:27:22 - INFO - Epoch : 17 , Batch [ 60 / 75 ] : Loss = 0.227558, Accuracy = 57.05%, MSE = 0.5123
2025-03-01 14:27:22 - INFO - Classwise accuracy : (0: 76.60% (47)), (1: 9.48% (116)), (2: 98.92% (93))
2025-03-01 14:38:27 - INFO - Epoch : 17 , Batch [ 70 / 75 ] : Loss = 0.223466, Accuracy = 57.15%, MSE = 0.5074
2025-03-01 14:38:27 - INFO - Classwise accuracy : (0: 51.22% (41)), (1: 30.48% (105)), (2: 95.45% (110))
2025-03-01 14:42:32 - INFO - Epoch 17: Train Loss=0.1959, Train Acc=57.24%, Train MSE=0.5057
2025-03-01 14:42:50 - INFO - Epoch 17: Val Loss=0.4350, Val Acc=0.00%
2025-03-01 14:42:50 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 14:43:26 - INFO - Epoch : 18 , Batch [ 0 / 75 ] : Loss = 0.186369, Accuracy = 54.69%, MSE = 0.4883
2025-03-01 14:43:26 - INFO - Classwise accuracy : (0: 76.92% (39)), (1: 9.65% (114)), (2: 96.12% (103))
2025-03-01 14:54:09 - INFO - Epoch : 18 , Batch [ 10 / 75 ] : Loss = 0.191535, Accuracy = 53.76%, MSE = 0.5913
2025-03-01 14:54:09 - INFO - Classwise accuracy : (0: 67.86% (56)), (1: 15.84% (101)), (2: 100.00% (99))
2025-03-01 15:05:41 - INFO - Epoch : 18 , Batch [ 20 / 75 ] : Loss = 0.199569, Accuracy = 56.81%, MSE = 0.5407
2025-03-01 15:05:41 - INFO - Classwise accuracy : (0: 51.11% (45)), (1: 35.42% (96)), (2: 99.13% (115))
2025-03-01 15:18:09 - INFO - Epoch : 18 , Batch [ 30 / 75 ] : Loss = 0.166155, Accuracy = 58.51%, MSE = 0.5110
2025-03-01 15:18:09 - INFO - Classwise accuracy : (0: 33.33% (42)), (1: 43.40% (106)), (2: 98.15% (108))
2025-03-01 15:29:10 - INFO - Epoch : 18 , Batch [ 40 / 75 ] : Loss = 0.195217, Accuracy = 58.42%, MSE = 0.5144
2025-03-01 15:29:10 - INFO - Classwise accuracy : (0: 68.89% (45)), (1: 17.31% (104)), (2: 96.26% (107))
2025-03-01 15:38:49 - INFO - Epoch : 18 , Batch [ 50 / 75 ] : Loss = 0.255951, Accuracy = 57.99%, MSE = 0.5189
2025-03-01 15:38:49 - INFO - Classwise accuracy : (0: 27.66% (47)), (1: 35.71% (98)), (2: 91.89% (111))
2025-03-01 15:51:43 - INFO - Epoch : 18 , Batch [ 60 / 75 ] : Loss = 0.198931, Accuracy = 58.24%, MSE = 0.5086
2025-03-01 15:51:43 - INFO - Classwise accuracy : (0: 60.53% (38)), (1: 23.76% (101)), (2: 96.58% (117))
2025-03-01 16:03:53 - INFO - Epoch : 18 , Batch [ 70 / 75 ] : Loss = 0.183419, Accuracy = 58.64%, MSE = 0.4979
2025-03-01 16:03:53 - INFO - Classwise accuracy : (0: 53.85% (52)), (1: 22.61% (115)), (2: 100.00% (89))
2025-03-01 16:07:38 - INFO - Epoch 18: Train Loss=0.2156, Train Acc=58.69%, Train MSE=0.4952
2025-03-01 16:07:56 - INFO - Epoch 18: Val Loss=0.5397, Val Acc=0.00%
2025-03-01 16:07:56 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00    4767.0
        Hold       0.00      0.00      0.00       0.0
         Buy       0.00      0.00      0.00       0.0

    accuracy                           0.00    4767.0
   macro avg       0.00      0.00      0.00    4767.0
weighted avg       0.00      0.00      0.00    4767.0

2025-03-01 16:15:51 - INFO - Val Loss=nan, Val Acc=83.72%
2025-03-01 16:15:51 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.04      0.06      0.05        65
        Hold       0.90      0.93      0.91      1282
         Buy       0.00      0.00      0.00        78

    accuracy                           0.84      1425
   macro avg       0.31      0.33      0.32      1425
weighted avg       0.81      0.84      0.82      1425

2025-03-01 16:25:58 - INFO - Full sequence shape : (201, 81)
2025-03-01 16:25:58 - INFO - Decoder sequence length : 188
2025-03-01 16:25:58 - INFO - Slope value shape : (188,)
2025-03-01 16:25:58 - INFO - Slope values : [0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1
 1 2 2 2 2 2 2 2 2 1 1 1 1 0 0 1 1 1 2 2 2 2 2 2 2 1 1 1 2 1 1 2 2 2 1 1 1
 1 0 0 0 0 0 0 0 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 0
 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2 2 1 0 0 0 0 1 1 1
 1 1 2 2 2 2 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1]
2025-03-01 16:25:58 - INFO - Target bin max : 1
2025-03-01 16:25:58 - INFO - slope classes : tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
2025-03-01 16:25:58 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-03-01 16:25:58 - INFO - Future price : 2
2025-03-01 16:25:58 - INFO - X shape : torch.Size([200, 51])
2025-03-01 16:25:58 - INFO - y shape : torch.Size([3])
2025-03-01 16:25:58 - INFO - y : tensor([0., 0., 1.])
2025-03-01 16:25:59 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 16:25:59 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 16:25:59 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 16:25:59 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 16:26:04 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 16:26:04 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 16:26:04 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 16:26:09 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 16:26:09 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 16:26:09 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 16:26:11 - INFO - Output s : tensor([[ 1.0331e+00, -1.2353e+00, -1.3259e-01],
        [-1.4972e-01,  4.2742e-02, -6.8358e-02],
        [ 9.6439e-01, -4.4956e-01,  1.7751e+00],
        [ 1.6942e+00, -2.4260e+00, -1.7575e+00],
        [ 1.0469e+00, -6.4575e-01, -1.4747e+00],
        [ 1.6461e+00, -1.8605e+00, -4.8564e-01],
        [ 1.7524e+00, -1.4614e+00, -8.6213e-01],
        [ 2.6987e+00, -1.2862e+00, -1.2117e+00],
        [ 4.8824e-01, -9.8026e-01, -8.1068e-01],
        [ 5.8630e-01, -2.0903e+00, -2.4191e-01],
        [ 1.9907e+00, -2.5108e+00, -1.0636e+00],
        [ 1.1556e+00, -1.3151e+00, -1.1303e+00],
        [ 6.6804e-01, -1.4375e+00, -2.3486e-01],
        [-1.3516e-02, -6.9470e-01,  8.0353e-02],
        [ 9.6375e-01, -1.7653e+00, -7.1122e-01],
        [ 1.2399e+00, -2.4199e+00,  1.4216e-01],
        [ 9.2399e-01, -6.2968e-01, -2.1065e+00],
        [-4.7550e-01, -1.9274e+00, -7.4443e-02],
        [ 6.6671e-01, -9.2018e-01, -6.0622e-01],
        [ 1.7645e+00, -4.8366e-02, -4.5947e-01],
        [ 2.1788e+00, -2.4062e+00, -1.7659e+00],
        [ 2.5200e+00, -1.5963e+00, -1.1244e+00],
        [ 7.6304e-01, -2.5333e+00,  1.0991e+00],
        [ 8.4968e-01, -1.9620e+00,  3.7000e-01],
        [ 1.1303e+00, -6.8105e-02, -1.6660e-01],
        [ 1.1892e+00, -2.8083e+00,  4.4294e-02],
        [ 4.8840e-01, -2.6883e+00, -7.1561e-01],
        [-5.1480e-01, -1.7283e+00, -2.8930e-01],
        [ 5.8526e-01, -7.1495e-01, -1.1398e+00],
        [-8.9050e-01, -7.2792e-01, -4.6318e-01],
        [ 1.4247e-01, -6.7451e-01, -1.1608e+00],
        [ 2.1405e+00, -1.3665e+00, -1.1111e+00],
        [-9.7123e-01, -1.9073e+00, -5.2523e-02],
        [ 3.1270e-01, -1.8298e+00,  1.7398e-02],
        [ 2.8015e+00, -2.6185e+00, -1.7541e+00],
        [ 6.6184e-01, -1.4069e+00, -3.2699e-01],
        [ 3.6571e-01, -1.5751e-01, -8.8948e-01],
        [ 1.2467e+00, -1.2642e+00, -7.1408e-01],
        [ 2.5855e+00, -2.4220e+00, -7.8075e-01],
        [ 6.3603e-01, -2.3962e+00, -1.1529e+00],
        [-3.6128e-01, -9.3407e-01, -8.0491e-01],
        [ 1.2127e+00, -6.1692e-01, -1.1857e-01],
        [ 3.3219e+00, -2.0432e+00, -1.5624e+00],
        [ 1.1228e+00, -9.7766e-01,  4.6278e-02],
        [ 3.7945e-01, -3.0628e+00, -9.2361e-01],
        [-6.9834e-01, -8.2923e-02, -3.5814e-01],
        [ 1.2670e+00, -2.4694e+00, -7.9564e-01],
        [ 8.2443e-01, -8.7655e-01, -3.8850e-01],
        [ 1.7072e+00, -3.3968e+00, -8.6351e-01],
        [ 3.3461e-01,  7.9853e-02,  4.2720e-01],
        [ 9.2667e-01, -9.8664e-01,  3.0501e-03],
        [ 8.6856e-01, -6.8282e-01, -3.8568e-01],
        [ 2.2960e+00, -1.1155e+00, -6.6120e-01],
        [ 2.3243e+00, -1.5550e+00, -5.7447e-01],
        [-2.5654e-01, -6.7545e-01,  5.4887e-01],
        [ 1.3094e+00, -1.0195e+00, -4.3259e-01],
        [ 2.5999e-01, -1.0129e-01, -4.2446e-01],
        [ 3.9725e-01, -2.1280e-01, -3.7943e-01],
        [-9.0021e-01, -1.0138e+00, -2.1319e+00],
        [ 5.4032e-01, -8.8481e-01, -6.2161e-01],
        [ 2.2944e+00, -1.9298e+00, -1.0066e+00],
        [-5.7248e-01, -1.7710e-01, -7.6543e-01],
        [ 3.4814e+00, -2.3827e+00, -5.1486e-01],
        [ 6.1494e-02, -1.3960e+00, -4.2537e-01],
        [ 4.5274e-01, -1.8165e+00, -4.1778e-01],
        [ 2.0402e+00, -2.2060e+00, -1.7523e+00],
        [ 5.3407e-01, -1.9265e+00, -1.2734e+00],
        [-6.3395e-01, -1.6916e+00, -8.0720e-01],
        [ 1.1760e+00, -1.1201e+00,  1.0137e-01],
        [ 9.2047e-01,  7.2331e-02,  1.5824e+00],
        [ 4.6956e-01, -3.2283e-01, -1.0798e+00],
        [ 1.7440e+00, -1.9460e+00, -7.1826e-01],
        [ 2.2275e+00, -3.5508e+00, -1.1181e+00],
        [ 1.1968e+00, -1.0523e+00, -6.8488e-01],
        [ 3.7762e+00, -2.1312e+00,  1.5306e-01],
        [-1.3980e+00, -1.4555e+00, -1.2356e+00],
        [ 1.3815e+00, -3.8729e-01, -1.5763e+00],
        [ 9.1616e-02, -7.7455e-01, -9.3528e-01],
        [ 1.6938e+00, -1.3544e+00,  2.0802e-01],
        [ 1.7286e-01, -5.1176e-01,  1.0551e+00],
        [ 5.0493e-01, -2.0060e+00, -9.6901e-01],
        [ 1.0409e+00, -1.3179e+00,  3.2880e-01],
        [ 2.8563e+00, -1.8268e+00, -5.7729e-01],
        [ 7.2748e-01, -7.1434e-01,  1.2382e-01],
        [-1.1084e+00, -6.2384e-01, -4.5467e-01],
        [ 3.8506e-01, -9.8759e-01, -4.1949e-01],
        [ 8.1461e-01, -1.4613e+00, -4.4393e-01],
        [-1.4249e-01, -1.3057e+00,  6.4411e-01],
        [ 1.8291e+00,  3.9802e-01, -2.0187e+00],
        [-5.4337e-01, -2.0586e-01, -2.7896e-01],
        [ 2.5461e+00, -2.2329e+00, -1.7061e+00],
        [-6.6887e-02, -2.6133e-01, -2.0227e+00],
        [ 1.6278e+00, -1.2893e+00, -9.3842e-02],
        [-1.5951e+00, -9.5471e-02, -1.3555e+00],
        [ 9.3521e-01, -2.4127e+00, -1.7886e+00],
        [-4.4413e-01, -1.0213e+00, -1.7429e+00],
        [ 3.1967e-01, -1.2498e+00,  4.2988e-01],
        [ 1.0488e+00, -8.3453e-01, -1.9194e+00],
        [ 1.5141e+00, -1.8283e+00, -3.6876e-01],
        [ 1.0270e+00, -1.6598e-01, -1.3059e+00],
        [ 2.6163e+00, -1.2991e+00, -9.8278e-01],
        [-6.5503e-01, -7.4825e-01, -1.0010e+00],
        [-1.3109e-01, -1.1386e+00, -2.4644e+00],
        [ 1.8383e+00, -1.5457e-01, -3.9794e-01],
        [ 1.6582e-02, -2.6616e-01,  7.5921e-01],
        [ 6.2304e-01, -9.7168e-01, -8.9824e-02],
        [ 5.6662e-01, -3.2652e+00, -9.3318e-01],
        [-8.9516e-01, -1.5506e+00, -1.3220e+00],
        [ 9.4272e-01,  2.9886e-01, -1.1309e+00],
        [ 1.6851e-01, -2.0127e+00, -5.3807e-01],
        [ 5.3498e-01, -2.0537e+00, -4.5253e-01],
        [ 8.7372e-01, -1.6913e+00, -6.9230e-01],
        [ 1.2074e+00, -3.1179e+00, -3.0489e+00],
        [ 9.4062e-01,  1.2126e+00,  4.0909e-01],
        [-4.1799e-01,  6.6643e-02, -1.1024e+00],
        [ 2.6453e+00, -9.9778e-02, -4.2385e-01],
        [-7.7135e-02, -7.4964e-01, -6.5614e-01],
        [ 1.8360e+00, -1.1921e+00, -6.8757e-01],
        [ 6.9712e-01, -1.2564e+00, -9.9772e-01],
        [ 1.2245e+00, -1.8065e+00, -1.4621e+00],
        [ 5.8172e-01, -2.6901e+00, -6.4701e-01],
        [ 7.2600e-01, -1.3241e+00,  7.3845e-01],
        [-1.5741e+00, -8.0003e-01, -1.3657e+00],
        [-5.7037e-01, -1.4033e+00, -4.1562e-01],
        [-1.7135e-01, -8.3612e-01, -1.3820e-01],
        [ 7.6855e-01, -1.7229e+00, -2.1082e-01],
        [-1.8810e-01, -1.0366e+00, -8.9545e-01],
        [ 3.1413e-01, -1.8122e+00, -2.5059e-01],
        [ 1.8406e+00,  4.0012e-01,  1.9044e-01],
        [ 1.2889e+00, -2.3174e+00, -2.2901e+00],
        [ 2.6682e+00, -1.5834e+00, -1.6673e+00],
        [ 3.0055e+00, -2.2941e+00, -6.8758e-01],
        [ 1.1766e+00, -1.0397e+00, -8.2512e-01],
        [ 1.1021e+00, -1.3147e-01, -1.4031e+00],
        [-1.2068e-01, -2.0571e-01, -5.0129e-02],
        [ 2.1215e+00, -1.4847e+00,  8.6641e-01],
        [ 1.3588e+00, -1.8391e+00, -1.4095e+00],
        [ 1.3275e+00, -5.4224e-01, -1.1343e+00],
        [ 2.8769e+00, -3.7398e+00, -5.7654e-01],
        [ 4.6992e-01, -1.4703e+00, -9.9232e-01],
        [ 4.7659e-01, -1.3097e+00,  2.3135e-01],
        [ 1.7607e+00, -4.4873e-01, -4.5190e-02],
        [-9.8769e-01, -1.2195e+00, -2.0330e+00],
        [ 2.4289e-01, -1.4575e+00, -2.1846e-01],
        [-9.8527e-01, -2.5385e+00, -1.4738e+00],
        [ 8.5515e-01,  1.4485e-01, -2.1471e-01],
        [ 4.0687e-01, -1.6485e+00, -1.4501e-01],
        [ 2.8779e-01, -5.2329e-01, -3.0118e-01],
        [ 1.7849e+00, -1.8004e+00, -2.0707e+00],
        [ 2.4335e-01, -2.6328e+00, -6.0297e-02],
        [ 2.1095e+00, -1.3898e+00, -1.5981e-01],
        [ 1.6021e-02, -1.0489e+00,  3.1274e-01],
        [ 8.5741e-01, -2.4808e+00, -1.2997e+00],
        [-4.4456e-01,  4.4688e-01, -3.1704e-01],
        [ 1.0352e+00, -1.5460e+00, -6.1762e-01],
        [ 1.4760e+00, -1.6544e+00, -1.0801e+00],
        [-4.0808e-01, -9.9058e-01, -3.9272e-01],
        [ 7.5613e-01, -1.4402e+00, -3.1174e-01],
        [ 9.8785e-01, -8.1048e-01,  6.0384e-01],
        [ 1.0512e+00, -8.2404e-02, -2.4447e-01],
        [ 8.9907e-01, -1.4647e+00, -3.0882e-01],
        [ 1.1385e+00, -8.7565e-01, -1.5800e+00],
        [ 1.9270e+00, -1.8507e+00, -1.0617e+00],
        [ 1.3855e-01, -1.6300e+00, -2.8776e-01],
        [ 7.1039e-01, -1.0284e+00, -2.3062e+00],
        [ 7.9004e-01, -1.0504e+00, -5.9502e-01],
        [-1.6201e-01, -6.4241e-01, -1.7311e+00],
        [-3.2746e-01, -1.6663e+00, -5.0404e-01],
        [ 2.4016e+00, -3.0358e+00, -1.2555e+00],
        [ 8.0667e-01, -2.1733e-01,  5.8802e-01],
        [ 2.0744e+00, -1.8025e+00, -9.8312e-01],
        [-5.3096e-01, -1.3387e+00,  3.4007e-01],
        [ 1.5610e+00, -1.9483e-01, -5.7604e-01],
        [ 2.3297e+00, -1.4569e+00, -7.1885e-01],
        [ 8.3031e-01, -7.5035e-01, -6.6535e-01],
        [ 1.8977e+00, -2.4824e-01, -5.5588e-02],
        [ 1.1484e+00, -1.0526e+00, -1.0797e+00],
        [ 1.2849e+00, -6.3530e-01, -1.1157e+00],
        [-1.0887e-01, -6.5479e-01, -4.8638e-01],
        [-1.2503e+00, -7.7092e-01, -5.7420e-01],
        [-6.6537e-01, -1.7635e+00, -2.0346e+00],
        [ 1.6786e+00, -1.4864e+00,  7.4711e-01],
        [ 1.1714e+00, -7.4355e-01, -2.8264e+00],
        [ 9.6427e-01, -2.4649e+00, -2.1807e+00],
        [ 1.3324e+00, -1.7194e-01, -1.1895e+00],
        [ 1.0478e+00, -4.5510e-01,  2.1321e-01],
        [ 3.0069e+00, -3.0433e+00, -1.4176e+00],
        [ 3.4544e+00, -2.0591e+00, -2.7181e-01],
        [ 1.4827e+00, -2.8494e+00, -1.0182e+00],
        [ 7.9259e-01, -1.2452e+00, -8.9625e-01],
        [ 2.9511e-01, -1.1966e+00, -1.7508e-01],
        [ 5.7954e-01, -1.1754e+00, -6.6818e-01],
        [ 5.7650e-01, -1.9115e+00, -1.2173e+00],
        [-2.5526e-01, -1.3564e+00, -6.1227e-01],
        [ 5.4015e-01, -7.5788e-01, -1.1283e+00],
        [ 1.2654e+00, -5.4419e-01, -9.9465e-02],
        [ 1.9625e-01, -7.6771e-01, -1.1287e-01],
        [ 4.2351e-01, -1.0029e+00, -1.8527e+00],
        [ 8.3114e-01, -1.8723e+00, -1.5467e+00],
        [ 7.9777e-01, -1.3487e+00, -4.1641e-01],
        [-1.3208e-01, -8.6371e-01, -2.1500e-01],
        [ 6.5631e-01, -1.0950e+00,  1.4890e-01],
        [ 4.5283e-01, -1.5928e+00,  2.2064e-01],
        [ 8.2074e-01, -1.9389e+00, -9.8868e-01],
        [ 6.2115e-01, -2.1593e+00, -1.7738e+00],
        [ 1.3429e+00, -2.3914e+00,  6.0366e-02],
        [-4.5201e-01, -6.8492e-01,  6.7524e-01],
        [-2.5761e-01, -1.8191e+00, -9.6235e-01],
        [ 1.2304e+00, -2.2160e+00, -5.7542e-01],
        [ 1.7002e+00, -1.4859e+00, -1.1297e+00],
        [ 6.0418e-01, -2.2557e+00, -5.0322e-01],
        [ 8.7596e-01, -1.2926e+00,  2.0376e-01],
        [ 5.9932e-01, -1.8750e+00, -1.3218e+00],
        [-2.2087e-01, -1.7703e+00, -4.9496e-01],
        [ 7.1694e-01, -1.2769e+00, -1.5384e+00],
        [ 1.6496e+00, -5.2275e-01,  1.2634e+00],
        [ 1.1525e+00, -1.2389e+00, -9.5781e-01],
        [ 1.3715e+00, -8.3527e-01,  2.2883e-01],
        [ 1.0726e+00, -1.9245e+00,  7.7884e-01],
        [ 5.7558e-01, -1.8319e+00, -4.5633e-01],
        [ 1.7894e+00, -2.1602e+00, -1.2419e+00],
        [ 8.2945e-01, -1.9290e+00, -8.4217e-01],
        [ 1.4374e+00, -3.7468e-01, -2.1470e+00],
        [ 1.4281e+00, -1.6519e+00,  5.5583e-01],
        [-1.0315e-01, -1.8076e+00,  2.2890e-01],
        [ 2.4794e+00, -3.8123e-01, -1.2421e+00],
        [ 3.5876e+00, -2.4225e+00, -8.6127e-01],
        [ 5.4643e-01, -1.3661e+00, -9.3849e-01],
        [ 5.8441e-01, -1.1049e+00, -2.1155e+00],
        [ 1.3287e+00, -1.4573e+00, -6.8465e-01],
        [ 1.0486e+00, -1.8963e+00, -7.9020e-01],
        [-1.6662e-01, -1.5204e+00,  7.9396e-02],
        [ 6.5649e-01, -3.4151e-01, -1.1950e+00],
        [-7.0887e-01, -1.0885e+00, -7.0568e-01],
        [ 8.5170e-01, -1.7875e+00, -1.8952e+00],
        [ 3.4002e+00, -2.0195e+00, -6.6768e-01],
        [ 1.9844e+00, -2.3194e+00, -6.2556e-01],
        [ 3.8729e-01,  1.1132e-01,  1.0764e-03],
        [-1.8004e+00, -1.4075e+00, -6.7437e-01],
        [-6.2217e-01, -8.3035e-01, -1.6045e+00],
        [ 6.4092e-01, -1.1918e+00, -1.0059e+00],
        [ 9.4061e-01, -1.1071e-01, -3.3454e-02],
        [ 1.7967e-01, -1.3505e+00, -4.2179e-02],
        [-4.0233e-01, -1.3784e+00, -3.7925e-01],
        [ 1.0027e+00, -1.0443e+00, -2.5297e+00],
        [ 6.3871e-01,  2.0242e-01, -2.8312e+00],
        [-1.8181e-02, -2.5695e+00, -1.4423e+00],
        [ 1.9028e+00, -3.0946e+00, -1.1780e+00],
        [ 1.7865e+00, -7.6915e-01, -6.5897e-01],
        [ 2.1215e+00, -7.8831e-01, -7.2294e-01],
        [-1.0873e+00, -6.9903e-01, -7.0625e-01],
        [ 1.4929e-01, -1.0209e+00, -1.3098e+00],
        [ 2.0668e-01, -2.1028e+00, -8.4249e-01],
        [-7.2819e-01, -7.5377e-01,  3.7413e-01],
        [ 1.4902e+00, -1.8825e+00, -1.9032e+00],
        [ 1.2926e+00, -2.1891e+00, -7.0235e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 16:34:36 - INFO - Logging started
2025-03-01 16:35:14 - INFO - Full sequence shape : (201, 81)
2025-03-01 16:35:14 - INFO - Decoder sequence length : 188
2025-03-01 16:35:14 - INFO - Slope value shape : (188,)
2025-03-01 16:35:14 - INFO - Slope values : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1
 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2
 2 2 1]
2025-03-01 16:35:14 - INFO - Target bin max : 1
2025-03-01 16:35:14 - INFO - slope classes : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1])
2025-03-01 16:35:14 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-03-01 16:35:14 - INFO - Future price : 0
2025-03-01 16:35:14 - INFO - X shape : torch.Size([200, 51])
2025-03-01 16:35:14 - INFO - y shape : torch.Size([3])
2025-03-01 16:35:14 - INFO - y : tensor([1., 0., 0.])
2025-03-01 16:35:14 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 16:35:14 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 16:35:14 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 16:35:14 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 16:35:14 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 16:35:14 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 16:35:14 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 16:35:15 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 16:35:15 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 16:35:15 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 16:35:16 - INFO - Output s : tensor([[-1.0922,  2.6894,  0.6545],
        [ 1.2410,  0.9353,  4.0610],
        [-1.5896,  1.7203,  2.1085],
        [ 1.7605,  0.1850,  3.5073],
        [-0.7966,  1.9646,  1.8064],
        [ 2.7193,  0.9697,  3.7211],
        [ 0.9613,  1.2324,  2.4698],
        [-1.3692,  2.7332,  1.3710],
        [-1.2306,  0.9046,  1.8975],
        [ 1.3560,  0.6391,  2.9426],
        [-2.3893,  2.1334,  1.0669],
        [ 1.0764, -1.1462,  3.5667],
        [ 0.2106,  0.5218,  3.8010],
        [-0.6194,  0.7420,  1.7466],
        [-1.2593,  1.7291,  1.7384],
        [-1.6419,  0.3592,  1.5623],
        [-2.7040,  2.5099,  2.0036],
        [-2.0332,  1.2273,  0.6432],
        [ 0.8652, -0.8101,  2.6841],
        [-0.9158,  1.9809,  0.7908],
        [ 2.9240,  2.4998,  3.1929],
        [-0.1559, -0.6030,  2.3974],
        [ 1.7033,  2.0213,  2.3582],
        [ 1.2434,  2.2002,  2.2372],
        [ 1.7720,  0.8878,  2.3070],
        [ 0.9238,  0.6214,  3.4273],
        [-3.4008,  1.3581,  2.5566],
        [ 1.2971,  0.9241,  3.1278],
        [ 1.8267,  1.1714,  2.8160],
        [-1.4092,  1.2814,  1.4057],
        [ 1.8667,  1.1531,  2.8501],
        [-2.7935,  2.3032,  0.9012],
        [-2.0226,  1.2992,  3.3387],
        [-1.4081,  1.5249,  1.2935],
        [ 1.3240,  1.2383,  3.4207],
        [-1.7374,  0.4462,  1.2329],
        [ 0.7382, -0.6207,  2.6385],
        [ 1.7679,  0.4148,  2.1448],
        [ 1.4981,  1.4423,  2.6889],
        [-2.8405,  1.5550,  2.3295],
        [ 2.6736,  1.9168,  2.2555],
        [-1.3194,  2.1292,  2.1357],
        [-1.9831,  1.3342,  2.1880],
        [-2.2565,  1.3179,  1.7540],
        [-1.7393,  1.2881,  1.6214],
        [ 0.9494, -1.3298,  3.2272],
        [-1.6650,  2.2770,  2.5549],
        [-0.4852,  1.1536,  2.5597],
        [ 2.0396,  0.4466,  2.0046],
        [-1.9228,  1.9757,  1.7741],
        [-0.8716,  1.2798,  1.9763],
        [ 1.3230, -0.9500,  2.8261],
        [-2.2806,  1.9413,  1.3331],
        [ 0.0241,  0.6393,  2.2412],
        [ 0.5833, -0.7202,  0.9345],
        [-2.5555,  2.1042,  1.7770],
        [ 1.4579,  0.7384,  3.5619],
        [-2.8021,  1.7549,  2.4697],
        [ 0.1141,  2.1446,  1.9545],
        [ 0.0258,  0.1990,  3.6298],
        [-2.6558,  1.9351,  1.9552],
        [ 0.9067,  2.4528,  2.1552],
        [-3.3040,  0.8721,  1.8154],
        [-1.1786,  1.5405,  2.6353],
        [ 2.2101,  1.5417,  2.4601],
        [-1.8603, -0.5181,  1.2340],
        [ 1.3805, -1.4753,  2.4759],
        [-2.8626,  2.0343,  0.8565],
        [-2.3409,  1.5933,  0.6637],
        [-1.4344,  2.4050,  1.0836],
        [ 0.7473, -0.9173,  2.2052],
        [ 2.3527,  1.0634,  3.5080],
        [ 0.6723,  1.0430,  4.0393],
        [-1.8397,  2.0637,  0.9148],
        [-1.8797,  2.1104,  2.5464],
        [ 0.9429,  1.0212,  3.4928],
        [-2.0550,  2.0056,  1.4520],
        [ 2.5535,  1.2643,  3.1090],
        [ 0.7138,  0.4771,  2.7770],
        [-1.0401,  1.9120,  2.2130],
        [-1.3944,  1.8619,  2.3598],
        [ 2.5271,  0.1320,  4.9954],
        [ 0.6213,  1.2268,  2.6793],
        [-1.3878,  1.4431,  3.8895],
        [-2.5162,  2.4571,  0.9672],
        [-1.6922,  1.8611,  2.1568],
        [ 0.3719,  0.2520,  2.8925],
        [-0.6990,  0.8034,  1.4621],
        [ 1.2253,  2.2125,  3.6896],
        [-1.3170,  2.3733,  1.4753],
        [ 0.0622,  1.2720,  1.7571],
        [-2.2178,  2.0683,  2.0859],
        [-1.3263,  1.7973,  0.9359],
        [-1.8284,  2.0930,  1.7986],
        [ 0.5186, -0.1892,  2.2140],
        [ 0.7790,  1.6630,  3.2366],
        [ 0.8957,  1.1842,  2.7267],
        [-1.0986,  1.7606,  0.9624],
        [-1.8235,  1.8937,  0.9823],
        [ 1.2827,  1.9104,  2.6407],
        [ 0.5286, -0.0208,  3.4360],
        [-2.6200,  1.7592,  2.5623],
        [ 0.5990, -1.7146,  2.3289],
        [-1.0457,  1.3911,  2.4727],
        [ 0.7653, -1.0099,  2.6295],
        [ 0.3429,  1.1554,  3.3458],
        [ 1.5782,  1.4264,  1.8144],
        [-0.6869,  1.6670,  1.2499],
        [ 0.5667,  0.1550,  1.8735],
        [ 0.3313,  0.0397,  3.9274],
        [-0.7099,  1.1544,  1.9254],
        [ 1.5509,  0.9965,  3.1487],
        [-2.8794,  1.5396,  2.2120],
        [-2.3337,  1.7648,  1.0589],
        [-3.4654,  2.2162,  2.6420],
        [-2.1299,  1.4397,  2.9050],
        [ 0.7074,  1.6032,  3.0354],
        [-3.0220,  2.4329,  2.4695],
        [-2.9813,  2.2630,  1.0915],
        [ 0.8345, -0.9881,  3.3130],
        [-2.9124,  1.9495,  1.0905],
        [-2.7414,  2.1120,  1.6452],
        [ 0.8395,  1.4869,  1.7460],
        [-1.6134,  1.9948,  2.0424],
        [ 2.0395,  1.7748,  2.7317],
        [-2.0714,  1.7372,  1.3976],
        [-2.8084,  2.6096,  0.7878],
        [ 1.3643, -0.3964,  3.7259],
        [-0.4262,  0.7924,  2.4833],
        [ 2.4948,  1.2443,  2.6671],
        [ 0.5913, -0.2550,  4.1500],
        [-1.8225,  1.8719,  3.3964],
        [-0.0189, -0.6835,  3.2777],
        [ 0.3990, -1.1392,  4.1559],
        [ 0.8444, -0.1286,  3.8007],
        [ 2.1623,  0.8929,  2.0143],
        [-4.8963,  1.1338,  0.8007],
        [-1.1720,  2.1248,  2.0731],
        [-2.3615,  1.1282,  0.8415],
        [-2.1989,  2.2062,  2.1220],
        [ 0.6585, -0.3072,  2.4207],
        [ 2.5744,  1.2450,  1.9214],
        [-1.4369,  0.8460,  1.2510],
        [-1.5504,  1.8755,  2.0500],
        [-1.3542,  0.9259,  3.8140],
        [-2.4110,  1.6258,  0.5770],
        [ 1.4901, -0.0601,  2.9174],
        [-1.4264,  2.3326,  1.5130],
        [-3.0421,  1.3595,  1.8945],
        [-1.8586,  2.0213,  1.3676],
        [ 0.8160, -0.3918,  3.1389],
        [ 0.9656,  1.3113,  3.2902],
        [-1.1334,  2.1130,  1.4981],
        [ 0.4186, -0.7298,  3.8148],
        [-1.5759,  1.9159,  2.3230],
        [ 1.5264,  0.2273,  2.2279],
        [-1.0205,  2.9236,  1.4374],
        [ 1.0872, -0.6612,  2.1411],
        [-0.1728,  0.0911,  3.7821],
        [ 3.2615,  1.4828,  2.1765],
        [-1.7480,  1.7551,  2.0238],
        [ 0.4329, -0.3510,  3.5367],
        [ 1.3220,  1.8953,  2.9607],
        [ 0.2573, -0.9949,  2.4445],
        [ 1.1605,  2.2362,  3.0055],
        [ 1.8169,  0.0377,  2.5160],
        [-1.6802,  1.4827,  1.7165],
        [-1.3300,  2.2249,  2.3887],
        [-1.8348,  1.4583, -0.0639],
        [ 1.0121, -0.1200,  3.4875],
        [-2.1286,  0.6898,  1.8636],
        [ 2.1765,  0.7511,  2.9480],
        [ 0.5388, -0.6454,  4.5133],
        [-0.3864,  2.1118,  0.4673],
        [ 1.5041,  0.0574,  2.5303],
        [-1.8460,  2.6509,  3.7710],
        [-1.5182,  0.3385,  2.1353],
        [ 1.3711, -0.5090,  4.1825],
        [-1.0452,  2.5105,  1.0825],
        [-1.5994,  1.3191,  2.4124],
        [ 0.0700, -0.7727,  3.3450],
        [-1.1361,  0.8735,  0.7119],
        [ 0.3086, -1.2550,  3.3878],
        [-1.5814,  1.6031,  0.7291],
        [-1.1688,  0.5001,  2.0017],
        [-2.0572,  3.0097,  1.3429],
        [-1.2966,  1.8952,  2.1229],
        [-1.5484,  1.9203,  1.9227],
        [-1.3212,  2.5283,  2.2653],
        [ 1.9991, -0.1591,  2.9973],
        [-1.8316,  2.0181,  1.6570],
        [-1.0560,  0.5744,  1.7334],
        [ 0.6548, -1.2432,  3.1855],
        [ 0.3817,  0.7601,  3.3523],
        [ 2.6480,  0.9570,  2.1619],
        [-1.2811,  1.2846,  2.9866],
        [-0.4631, -0.1316,  2.9002],
        [-2.1867,  2.2535,  1.4987],
        [ 0.3593, -0.1657,  3.1162],
        [-0.7867,  2.5384,  2.0484],
        [ 0.4997, -1.6014,  2.9558],
        [-0.2569,  0.1521,  2.9211],
        [-2.9818,  2.9117,  1.5314],
        [-2.9062,  1.1275,  1.5118],
        [ 2.1803, -0.7605,  2.6598],
        [ 0.7095,  1.8490,  3.5087],
        [-2.3631,  1.2078,  1.8524],
        [ 0.2080,  1.7664,  3.1887],
        [-1.2535,  0.3363,  2.6491],
        [ 2.5402,  1.0973,  2.3456],
        [ 0.4099,  1.3944,  2.6781],
        [-1.2437,  2.6846,  1.2461],
        [ 1.5093, -1.0870,  1.5928],
        [ 1.7034,  1.4135,  3.4513],
        [-1.3707,  1.1010,  2.4742],
        [-2.0780,  1.2818, -0.4463],
        [-2.1773,  2.4096,  1.7745],
        [ 2.1792,  1.8953,  2.5899],
        [-1.7781,  2.1230,  3.0804],
        [-2.7397,  1.4382,  2.2854],
        [ 0.2570,  1.3634,  2.0834],
        [-1.9191,  2.3196,  1.5996],
        [-2.1629,  0.5067,  2.3296],
        [ 0.3471,  1.5584,  2.4030],
        [ 2.0925,  2.0455,  2.8692],
        [-2.2855,  2.6333,  1.6220],
        [ 1.7471,  1.8932,  3.5573],
        [ 0.8328,  2.0055,  3.0457],
        [-2.4881,  2.3458,  1.5163],
        [-2.2001,  1.5841,  2.1613],
        [ 0.6827, -0.2119,  3.5203],
        [ 1.0050, -0.3323,  3.0196],
        [ 1.1120,  1.9720,  2.7254],
        [-0.3452,  0.8697,  1.5829],
        [-2.2733,  2.9144,  1.3630],
        [-2.1416,  1.7710,  1.5908],
        [-1.8013,  1.9377, -0.0339],
        [-0.9787,  1.8811,  1.2371],
        [-0.1780,  1.2716,  2.8478],
        [ 2.0942,  1.7119,  3.1512],
        [-2.2136,  1.1599,  1.8752],
        [-1.4401,  1.9071,  0.7565],
        [-1.6618,  2.6213,  2.5185],
        [-2.1048,  1.8308,  0.4783],
        [-3.1928,  0.6865,  2.2601],
        [-0.6824,  0.8285,  2.0512],
        [-1.1933,  0.4743,  2.3156],
        [-0.4489,  1.4512,  2.6002],
        [ 1.6866,  1.2305,  2.6588],
        [-2.7787,  1.5379,  1.2942],
        [ 1.5983,  2.0985,  3.3918],
        [ 0.0891,  0.2014,  2.3958],
        [ 2.0363,  1.1378,  2.5180],
        [-1.9525,  1.8694,  1.5885],
        [ 1.4000,  1.9146,  2.6204],
        [-1.0172,  0.7425,  3.2398]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 16:38:19 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 16:38:19 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 16:38:19 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 16:38:19 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 16:38:19 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 16:38:19 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 16:38:19 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 16:38:19 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 16:38:24 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 16:38:24 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 16:38:24 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 16:38:24 - INFO - Output s : tensor([[ 3.4387, -3.4389,  0.7654],
        [ 2.0126, -2.5273, -0.8001],
        [ 0.0924, -2.0190, -0.0357],
        [ 0.2037, -1.6725, -0.3622],
        [ 0.4244, -2.2954, -1.0289],
        [ 2.2718,  0.1751, -0.4797],
        [ 1.8895, -1.2564, -0.8810],
        [ 0.0597, -1.1475,  0.0902],
        [ 2.0531, -0.2621,  0.3185],
        [ 0.6219, -2.7351, -1.1539],
        [ 0.3475, -2.3310,  0.8139],
        [ 2.1245, -1.8065,  0.6961],
        [ 1.5219, -2.6076,  0.2727],
        [ 0.7615, -1.0636, -0.7117],
        [ 1.1140, -2.1514, -0.5737],
        [ 2.3058, -0.9728, -1.0450],
        [ 1.8792, -2.5193,  0.7743],
        [-1.2989, -2.0288, -0.3464],
        [ 0.7767,  0.4984, -0.0220],
        [ 1.9911, -0.6484, -0.1464],
        [ 1.5856, -2.2689,  0.3215],
        [ 3.5124,  0.3553,  0.3314],
        [ 0.4398, -1.3842,  0.6190],
        [ 2.2688,  0.4194,  0.6760],
        [ 2.1912, -3.7795,  0.2858],
        [-0.2388, -1.9768, -1.0172],
        [ 1.2482, -3.9215,  1.0180],
        [ 1.8612, -2.7794,  0.2257],
        [ 2.6498,  0.7475, -0.5145],
        [ 3.0022, -2.2571,  0.3818],
        [ 0.9789, -1.5393, -1.3917],
        [ 1.2657, -2.2346,  0.3907],
        [ 1.9981, -2.9580, -0.6468],
        [ 0.3960, -1.8975,  0.7078],
        [ 1.1095, -1.8562, -0.8642],
        [ 2.4676, -1.2383,  0.6193],
        [ 1.4182, -1.9122,  1.1225],
        [ 1.1093, -1.6016, -0.6291],
        [ 1.5320, -2.1520, -0.8507],
        [ 0.0155, -2.7465, -1.3007],
        [ 2.5055, -0.5910,  0.0605],
        [ 1.7375, -0.6649,  0.5361],
        [ 2.1344, -3.2978,  0.5733],
        [ 1.5017, -1.7822, -1.2458],
        [-0.1200, -2.4460, -3.1788],
        [ 1.8112,  0.5419,  0.6546],
        [ 1.4966, -2.4215, -0.5165],
        [ 1.0585, -2.2301, -1.3584],
        [ 1.4795, -2.3902,  0.8969],
        [ 1.1452,  0.1459, -1.0567],
        [ 2.2756, -3.2989, -0.4563],
        [ 2.2843, -2.0799, -2.3226],
        [ 1.6455, -1.2644, -1.0552],
        [ 2.4986, -2.5441, -0.1093],
        [ 1.2451, -1.7650,  0.5992],
        [ 1.0510, -2.8559, -2.3254],
        [ 1.9417, -1.4596, -0.2237],
        [ 0.6029, -2.6266,  0.1210],
        [ 0.5007, -1.2884, -1.8343],
        [ 2.3092, -2.0518, -0.5161],
        [ 1.8905, -2.5513, -0.3541],
        [ 2.0623, -1.0534, -0.0824],
        [ 2.7695, -0.7311,  0.7384],
        [ 1.3351,  0.1518, -0.9668],
        [ 2.6825, -1.0887,  0.6086],
        [ 1.5082, -1.8821, -2.9459],
        [ 1.2032, -2.8425, -0.3104],
        [ 1.6058, -0.9956, -0.9760],
        [ 1.2964, -2.7123, -0.5436],
        [ 2.4769, -0.5490, -0.9693],
        [ 0.2791, -2.7156, -1.6064],
        [ 1.2047, -2.1021, -1.0501],
        [ 1.8120, -2.5322, -1.2299],
        [ 2.5205,  0.7561, -0.2381],
        [ 1.5868, -2.0468,  0.3741],
        [ 0.8944, -1.7190,  0.0953],
        [ 0.0908, -2.4497,  1.1053],
        [ 1.2166, -2.4080,  0.1180],
        [ 2.3041, -2.2564,  0.4704],
        [ 1.7362, -2.6892,  0.2876],
        [ 2.2995, -1.5986,  0.0447],
        [ 1.6470, -2.6655, -0.8591],
        [ 2.5893, -0.7651, -1.8770],
        [ 1.4592, -3.1065, -0.8583],
        [ 1.5581, -1.8488, -1.8035],
        [ 2.0309, -1.3980, -0.9734],
        [ 1.1767, -3.0499,  1.4709],
        [ 1.7169, -0.4473,  0.1181],
        [ 2.3956, -3.1612, -1.8177],
        [ 1.1804, -0.7526,  0.0370],
        [ 1.3784, -2.2822,  0.3384],
        [ 1.6747, -3.3405, -0.3336],
        [ 2.0121, -1.5644, -0.2022],
        [ 0.4525, -1.5072, -2.4677],
        [ 1.3343, -3.1566,  0.0294],
        [ 0.7784, -1.1779, -1.7278],
        [ 0.9672, -2.9008, -1.3104],
        [ 2.0018, -0.3835, -1.7873],
        [ 2.7670, -2.9788,  0.0990],
        [ 0.9975, -3.1636, -0.8484],
        [ 2.0885, -3.0030,  0.9602],
        [ 1.6952, -3.4913,  0.5820],
        [ 3.3644, -2.2007,  0.9063],
        [ 1.4378, -1.8855,  0.2682],
        [ 0.3757, -3.8167, -0.1987],
        [ 0.9553, -2.5436, -1.3699],
        [ 1.3733, -2.8869, -2.7285],
        [ 1.5883, -1.3312, -2.0033],
        [ 1.8980, -0.8321, -3.8285],
        [ 2.0529, -1.6580,  1.0257],
        [ 3.2029, -1.3988,  1.1765],
        [ 0.6632,  0.3037,  1.0250],
        [ 1.9953, -0.9133,  0.4922],
        [ 1.9059, -1.9743, -1.1948],
        [ 0.7047, -3.2873, -0.1194],
        [ 2.0918, -1.7016, -0.6108],
        [ 0.9424, -1.3713,  0.4894],
        [ 0.2470, -2.8855, -2.0140],
        [ 2.2902, -1.7165, -1.4443],
        [ 2.2270, -2.7618, -0.1466],
        [ 1.1385, -2.4098, -0.4686],
        [ 1.9376, -3.4654,  1.6419],
        [ 0.8049, -2.6291,  0.2721],
        [ 3.0154, -1.0992, -0.5277],
        [ 3.2336, -1.4373, -0.7151],
        [ 0.8483, -1.2612, -1.9500],
        [ 1.2839, -1.8128, -0.0930],
        [ 2.6905, -0.0335, -1.4954],
        [ 1.5174, -2.5034, -0.0698],
        [ 1.1402, -2.9392, -1.6556],
        [ 2.5568, -4.2808, -0.0222],
        [ 2.2911, -4.5096, -1.0427],
        [ 2.2650, -2.5994,  1.0189],
        [ 2.7101, -1.5237, -0.2909],
        [ 1.2678, -2.4519, -1.5017],
        [-0.2578, -1.3643, -0.6343],
        [ 2.2288, -1.5055, -0.3504],
        [ 2.7339, -0.8440, -0.5522],
        [ 2.8238,  1.0362,  0.0432],
        [ 1.7154, -1.8739,  0.6878],
        [-0.2916, -2.0299, -1.0368],
        [ 1.4069, -1.8544,  0.0506],
        [ 0.1315, -1.5172, -1.6176],
        [ 2.2000, -4.4689, -0.3678],
        [ 1.7571, -3.6849, -2.4903],
        [ 0.4473, -2.3663, -1.6551],
        [ 2.1579, -2.4802,  1.8113],
        [ 1.0663, -1.8076, -1.6596],
        [ 1.3827, -0.4708, -1.0830],
        [ 2.5892, -2.3519,  0.4981],
        [ 2.0616, -3.3230,  1.0929],
        [ 2.1314, -4.4052,  0.0866],
        [ 1.5103, -2.6097, -0.3142],
        [ 1.2293, -1.6568,  0.4447],
        [-0.1063, -3.3697, -0.8877],
        [ 1.2498,  0.5492,  0.1917],
        [ 0.5341, -3.0636, -1.4029],
        [ 0.2149, -2.7619,  0.0987],
        [ 2.0874, -1.0211, -0.3962],
        [ 0.3962, -0.7648, -2.1849],
        [ 1.9137, -2.8834, -1.3828],
        [ 2.4937, -1.7900,  1.2256],
        [ 0.9083, -4.1450,  1.1387],
        [ 1.1514, -1.6041,  0.0090],
        [ 1.9776, -1.7770,  0.6644],
        [ 1.6968, -0.4698, -1.0498],
        [ 0.6372, -2.1686, -1.1426],
        [ 0.5201, -2.4951, -1.0826],
        [ 2.4018, -3.1864, -1.2192],
        [ 1.2069, -1.4529, -0.5785],
        [ 2.2647, -1.4277,  0.2438],
        [ 1.6653, -2.4240,  1.2594],
        [ 0.0799, -2.3444, -0.1781],
        [ 1.5650, -1.7453,  0.7075],
        [ 1.3629, -2.2721,  1.8698],
        [ 2.0800,  0.0861,  0.3855],
        [ 2.5608, -1.0115,  0.3788],
        [-0.4595, -1.2939, -1.8702],
        [ 1.4368, -2.4917, -0.0809],
        [ 2.0692, -3.8448, -0.4260],
        [ 2.1427, -1.3871, -0.2854],
        [ 0.8358, -1.5444, -2.5827],
        [ 1.7741, -1.4992, -0.1885],
        [ 3.0068, -2.8995, -2.2041],
        [ 2.6624, -1.3807,  0.6280],
        [ 0.6986, -2.0392, -1.9962],
        [ 2.7889, -1.2002,  0.2664],
        [ 1.3377, -3.1182,  1.9445],
        [ 1.4002, -2.0565, -0.6937],
        [ 2.7020, -1.5679, -0.3435],
        [ 2.8251, -1.4930, -0.2357],
        [ 1.7098, -1.7612, -1.1372],
        [ 1.6760, -2.9276, -0.3412],
        [ 2.2828, -2.9272,  0.5865],
        [ 2.3696, -2.6060, -0.1376],
        [ 2.6417, -0.4068,  0.4890],
        [ 1.2295, -1.4021, -0.9078],
        [ 2.0224, -2.6349, -1.2847],
        [ 0.7571, -1.9028,  0.2389],
        [ 2.0475, -0.8781, -0.2281],
        [ 3.7888, -2.0237, -0.4922],
        [ 1.8641, -2.3233, -2.1114],
        [ 0.3740, -2.4341,  0.4762],
        [ 1.5144, -0.9542,  0.8451],
        [ 1.5702, -0.5612,  0.1820],
        [ 1.3701, -1.9952, -0.1464],
        [ 1.2223, -3.0160, -0.8963],
        [-0.2696, -3.6012, -1.0866],
        [ 2.1404, -1.0338, -1.2843],
        [-0.3930, -2.0909, -0.9785],
        [ 0.0488, -2.8485, -0.9478],
        [-0.4432, -2.1270, -0.9576],
        [ 1.1606, -3.6892,  0.1278],
        [ 1.4168, -2.8164,  0.8213],
        [ 2.1352, -2.0588, -0.4356],
        [ 0.6065, -2.9394,  0.0363],
        [ 1.4232, -1.1544,  0.3523],
        [ 0.9834, -2.9642, -1.7967],
        [ 1.5253, -0.3354, -1.1346],
        [ 2.4091, -2.5606,  0.8859],
        [ 0.6610, -1.3747, -0.0448],
        [ 0.3449, -2.2791,  0.4408],
        [-0.0302, -0.7878, -0.0966],
        [ 3.0411, -0.7653,  0.8711],
        [ 2.1214, -1.8240, -1.5710],
        [ 2.5915, -0.4294,  0.1750],
        [ 1.9711, -0.5267, -1.5682],
        [ 1.3698, -1.7382,  1.2514],
        [ 0.2797, -1.1982, -0.2491],
        [ 1.5146, -0.3976, -0.0599],
        [ 2.0315, -1.7270, -1.4307],
        [ 2.8519, -2.6010,  0.5669],
        [ 1.5118, -0.7137, -1.2899],
        [ 3.0352, -2.0650,  0.4394],
        [ 1.9946, -1.5621,  0.2704],
        [ 0.1730, -1.2138, -2.5252],
        [ 1.4964, -2.5137, -0.1532],
        [ 0.3249, -1.8898, -2.3908],
        [ 1.0204, -2.8626,  0.8560],
        [ 1.3336, -0.8448,  0.8056],
        [ 1.0514, -2.1270, -0.0304],
        [ 1.5386, -2.9449, -2.1476],
        [ 0.5043, -0.8521,  0.7986],
        [ 0.2398, -2.2546, -0.4592],
        [ 1.2102, -2.0131,  0.2740],
        [ 0.4705, -1.9441,  0.3173],
        [ 2.5523, -2.9266,  1.2330],
        [ 1.9987, -0.0267, -0.5278],
        [ 1.9367, -1.0568, -0.0046],
        [ 2.0592, -0.4219,  0.9297],
        [ 2.6493, -3.0562,  1.2799],
        [ 1.7429, -2.9720,  1.5018],
        [ 2.2808, -1.5422,  0.5874],
        [ 1.5456, -1.9276, -1.8494],
        [ 1.1315, -2.3837, -0.9166],
        [ 4.3271,  0.0140, -0.6488]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 16:38:24 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 16:38:24 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 16:38:24 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 16:38:31 - INFO - Epoch : 1 , Batch [ 0 / 93 ] : Loss = 0.969750, Accuracy = 35.94%, MSE = 1.6367
2025-03-01 16:38:31 - INFO - Classwise accuracy : (0: 97.73% (88)), (1: 0.00% (79)), (2: 6.74% (89))
2025-03-01 16:42:29 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 16:42:29 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 16:42:29 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 16:42:29 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 16:42:29 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 16:42:37 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 16:42:37 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 16:42:37 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 16:42:43 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 16:42:43 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 16:42:43 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 16:42:44 - INFO - Output s : tensor([[-4.2697e-01, -1.7988e+00,  1.0716e+00],
        [ 4.2571e-01, -1.7187e+00,  1.0704e-02],
        [ 2.8984e-01, -2.2495e+00, -7.8387e-01],
        [ 3.5931e-01, -1.9539e+00, -1.9883e+00],
        [ 4.5390e-01, -2.0713e+00, -1.2799e+00],
        [-8.8096e-02,  9.4250e-01, -4.7978e-01],
        [ 2.6523e-02, -8.1992e-01, -2.5221e+00],
        [-5.8916e-01, -4.4485e-01, -4.0824e-01],
        [ 4.1173e-01, -1.6819e+00, -1.7167e+00],
        [ 1.6970e-01, -1.9545e-01, -1.2734e+00],
        [-8.1092e-01, -1.0749e+00, -2.7833e+00],
        [-5.8832e-01, -3.3980e-01, -1.4006e+00],
        [-6.5699e-01, -2.1805e+00, -2.4136e+00],
        [ 5.8699e-02, -7.8787e-01, -1.4332e+00],
        [-3.2233e-01, -2.7432e+00, -2.2136e+00],
        [ 1.3356e+00, -5.6564e-01, -4.9635e-01],
        [ 1.3703e+00, -1.6828e+00,  1.3021e-01],
        [ 9.4427e-02, -1.2269e+00, -3.3952e-01],
        [ 6.1225e-01, -1.0160e+00, -2.6974e+00],
        [ 3.0761e-01, -3.5486e-01, -1.2626e+00],
        [ 3.5371e-01, -8.7536e-01, -6.2763e-01],
        [ 5.6322e-01, -3.9735e-03,  3.8315e-01],
        [-6.0144e-01, -1.8142e+00,  9.4562e-01],
        [-2.0491e-01, -8.2080e-01, -7.2622e-01],
        [-1.1504e-04, -1.3815e+00, -3.4181e-01],
        [-2.6734e-01, -1.4585e+00,  2.1430e-02],
        [ 5.6291e-01, -1.3501e+00, -1.5771e-01],
        [ 8.7266e-01, -4.7468e-01, -4.3736e-01],
        [-2.9912e-01, -1.8642e-01, -1.2371e-01],
        [-7.7029e-01, -1.0627e+00, -5.1690e-01],
        [ 3.0045e-01, -1.9657e-01, -1.0119e+00],
        [ 7.4785e-01, -7.2847e-01, -7.6466e-01],
        [-8.6748e-01,  3.0907e-01, -7.8354e-01],
        [-9.9037e-01, -1.8346e+00, -1.5592e+00],
        [-8.7239e-01, -2.3070e+00, -4.1180e+00],
        [-1.6264e+00,  3.0804e-01,  3.0786e-01],
        [ 3.5810e-01, -3.1220e+00, -5.8024e-01],
        [ 3.7332e-01, -1.7610e+00, -1.6131e+00],
        [ 1.4245e-01, -1.8812e+00, -5.3889e-01],
        [ 2.1808e+00, -1.1529e+00, -1.1372e+00],
        [ 1.1171e+00, -2.2555e+00, -2.2140e+00],
        [ 8.9799e-02, -2.5195e+00, -3.3702e-01],
        [ 1.0953e+00, -8.4100e-01,  6.7112e-02],
        [-3.4683e-01, -1.3293e+00, -7.5098e-01],
        [ 2.0379e+00, -2.4456e+00, -1.1977e+00],
        [ 4.7082e-01, -1.0902e+00, -5.4458e-01],
        [ 3.8135e-01, -1.8384e+00,  6.3154e-01],
        [-1.8977e-01, -6.2827e-01,  6.3767e-01],
        [-1.4605e-01, -1.6639e+00, -8.0767e-01],
        [ 6.9250e-01, -2.2113e+00, -5.2442e-01],
        [-6.2740e-01, -3.2290e+00, -5.1528e-01],
        [ 1.2619e+00, -2.2632e-01, -5.7994e-01],
        [ 1.1744e+00, -2.7286e+00, -5.9685e-01],
        [ 8.2840e-01, -9.5429e-01, -1.3960e+00],
        [-8.3679e-01, -1.4648e-01, -1.6171e+00],
        [ 9.0350e-01, -1.9035e-01, -1.6351e+00],
        [ 9.6393e-01, -1.6405e+00, -6.8838e-01],
        [-7.4982e-01,  2.7595e-01, -1.2364e+00],
        [-9.7511e-01,  1.6518e-01,  5.7533e-01],
        [-2.4481e+00, -5.6337e-01, -1.2285e+00],
        [ 1.9136e-01, -1.2786e+00, -1.2474e+00],
        [-1.1195e+00, -1.9300e+00, -6.7689e-01],
        [-1.5483e-01, -8.8322e-01, -5.4242e-01],
        [-7.8963e-01, -1.0587e+00, -1.0412e+00],
        [-1.5898e-01, -1.1571e+00, -9.8978e-01],
        [-1.5955e-01, -5.3687e-01, -1.1149e+00],
        [-2.8283e+00, -6.5155e-01, -9.1003e-01],
        [-1.2378e+00,  2.8467e-01, -2.0198e+00],
        [-1.0250e+00, -2.1977e+00, -1.2844e+00],
        [-1.1779e+00, -8.4011e-01,  3.4650e-02],
        [-5.2396e-01, -9.3495e-01,  6.7718e-01],
        [ 5.6809e-01,  5.2738e-01, -2.3524e+00],
        [ 4.4575e-01,  3.7742e-01, -1.0063e+00],
        [-3.6946e-01, -1.0993e+00, -1.6878e-01],
        [-2.5794e-01, -1.8703e+00, -6.9011e-01],
        [ 1.0484e+00, -7.3330e-01, -1.3508e+00],
        [-9.4416e-01, -1.2229e+00, -8.8129e-01],
        [ 1.1368e+00, -2.2510e+00, -1.7389e+00],
        [-4.1084e-01, -6.2320e-01, -1.3650e+00],
        [ 1.2206e+00, -2.1165e+00, -1.0079e+00],
        [-5.6831e-01, -3.4095e+00, -1.0913e+00],
        [ 2.8576e-02, -7.3620e-01, -2.4899e-01],
        [-2.7627e-02, -1.3774e+00, -3.3204e-01],
        [-2.8573e-01, -1.5130e+00, -3.6623e-01],
        [-2.7259e-01, -2.0060e+00, -1.0129e+00],
        [-1.2899e+00, -1.4243e+00, -9.0532e-01],
        [-2.1664e-01, -2.1867e+00, -1.2521e+00],
        [-7.6227e-01, -5.8940e-01,  1.9216e-01],
        [-1.7398e+00, -2.2274e-01,  2.0713e-01],
        [-1.5868e+00, -1.5451e+00, -3.7611e-01],
        [ 7.8241e-02, -2.1982e+00,  2.0417e-01],
        [-1.1011e+00, -1.6852e+00, -1.1065e-01],
        [ 7.3380e-01, -2.0841e+00, -6.7959e-01],
        [ 5.4328e-01, -2.0246e+00, -1.5460e+00],
        [-1.5189e+00, -2.2483e+00, -6.8463e-01],
        [-1.8976e+00, -1.3342e+00, -2.9542e+00],
        [-4.1745e-01, -1.6688e+00, -8.1265e-01],
        [-2.7566e-01, -3.4432e-01, -1.6084e+00],
        [-3.0257e-01, -2.2434e+00, -2.4150e+00],
        [-1.4624e+00, -1.0400e+00,  2.2066e-01],
        [ 1.1719e+00, -9.3983e-01, -1.7045e+00],
        [-1.2966e+00,  2.2995e-01, -3.6669e-02],
        [-3.6084e-01, -9.5150e-01, -9.9117e-02],
        [-2.7972e-01, -2.6043e-01, -6.0168e-01],
        [-2.3604e+00, -1.6623e+00, -1.3283e+00],
        [-2.2459e+00, -1.7737e+00,  3.5734e-01],
        [-8.0311e-01, -1.1489e+00, -1.8748e+00],
        [-2.3286e-01, -5.3096e-01,  3.3285e-01],
        [-9.1402e-01, -5.5071e-01, -1.0888e+00],
        [-2.8967e-01, -1.6164e+00, -1.3242e+00],
        [-5.5219e-01, -4.5094e-01, -1.7762e+00],
        [ 4.0033e-01, -1.2542e+00, -2.4282e+00],
        [-1.2131e+00, -1.2651e+00, -1.1032e+00],
        [-1.9559e+00, -1.2465e+00, -1.0528e+00],
        [-1.0717e+00, -2.0776e+00, -3.2295e-01],
        [ 3.4599e-01,  1.7603e-01, -5.6076e-01],
        [ 2.8486e-01, -4.7435e-01,  5.0853e-01],
        [ 6.9583e-01, -4.0769e+00, -9.9226e-01],
        [ 5.8653e-01, -2.4461e+00, -8.7744e-01],
        [-1.1393e+00, -1.4664e+00,  6.8328e-02],
        [ 1.3120e-01, -2.3123e+00,  4.8226e-02],
        [-4.5825e-01, -1.6152e+00, -7.9430e-02],
        [-8.6931e-01, -2.9416e+00,  1.6935e-01],
        [-1.0672e+00, -1.4097e+00,  3.0256e-01],
        [ 1.1767e+00, -1.2452e+00, -9.2175e-01],
        [-4.7148e-01, -2.5639e+00,  1.4972e+00],
        [-3.4330e-03, -4.3399e-01, -1.4840e-01],
        [ 1.6809e+00,  8.3628e-01,  8.7923e-02],
        [ 4.4010e-01, -1.9080e+00, -2.8888e+00],
        [-4.5933e-01, -2.2704e+00, -1.8412e+00],
        [-1.2805e+00, -1.8096e+00,  1.3983e+00],
        [-6.6896e-01, -4.0561e-01, -1.0101e+00],
        [ 4.8543e-01, -3.0838e-01, -2.3818e+00],
        [ 3.9364e-01, -1.8224e+00, -8.5901e-01],
        [ 6.0170e-01, -1.5220e+00, -7.4413e-01],
        [ 9.2512e-02, -2.2571e+00, -1.8548e+00],
        [ 2.9115e-01, -9.5588e-01, -1.2467e+00],
        [-4.0988e-02, -1.6412e+00, -2.3274e+00],
        [ 1.8919e-01, -1.9297e+00, -5.6554e-01],
        [-6.2826e-01, -1.7466e+00, -1.0547e+00],
        [-6.1468e-01, -1.7893e-01, -1.4194e+00],
        [ 6.7960e-02,  2.4552e-01, -1.7213e+00],
        [-1.2582e+00, -2.6456e+00, -2.3228e+00],
        [-7.5744e-01, -9.2666e-01, -2.5873e+00],
        [-5.1180e-01, -2.9598e+00, -1.7630e+00],
        [-6.2153e-01, -3.5125e-01,  2.8619e-01],
        [ 5.3700e-01, -1.1766e+00, -5.4059e-01],
        [-4.9111e-01,  2.1444e-01, -1.7336e+00],
        [ 1.0895e+00, -2.5366e-01, -7.8935e-01],
        [ 7.3831e-01, -1.5452e-01, -2.4971e+00],
        [ 9.1024e-01, -2.3723e+00, -1.1853e+00],
        [ 3.4184e-01, -4.5624e-01,  1.7097e-01],
        [ 5.7045e-01, -2.8260e+00,  1.5503e-01],
        [ 1.5105e+00,  4.7672e-02, -2.5379e+00],
        [ 1.9450e+00, -2.7600e+00, -1.4328e+00],
        [ 3.1533e-01, -1.2149e+00, -1.6204e+00],
        [ 9.6721e-01, -2.8060e+00, -2.4503e-01],
        [ 4.6670e-01, -1.2538e-01, -3.6399e-01],
        [ 8.4200e-01,  5.5716e-01, -1.2038e+00],
        [ 1.9162e+00, -1.5686e-01, -7.9781e-01],
        [ 1.0764e-01, -3.3484e+00, -9.4707e-01],
        [ 4.6289e-01, -4.6371e-01, -8.7661e-01],
        [-1.5470e+00, -1.3858e+00, -9.0919e-01],
        [ 1.5465e-01, -1.0163e+00, -1.1158e+00],
        [ 1.0215e+00, -1.6878e+00, -2.2534e+00],
        [ 3.0855e-01, -1.6388e+00, -1.7292e+00],
        [ 5.7646e-01, -1.6433e+00, -1.0218e+00],
        [ 1.0371e+00, -1.6469e+00,  9.4687e-02],
        [ 1.1300e+00, -1.5527e+00, -1.9202e+00],
        [-2.9753e-01,  3.7359e-02,  9.9688e-01],
        [-5.3388e-01, -1.4541e+00, -2.8786e-01],
        [ 3.4149e-01, -1.9343e-01, -1.0992e+00],
        [-5.1139e-01, -7.7392e-01,  1.0204e+00],
        [ 2.8173e-01, -1.4058e+00, -1.2768e+00],
        [-2.3803e-01, -4.7135e-01,  6.3669e-01],
        [ 4.9837e-01, -9.4597e-01,  1.8729e+00],
        [ 5.9918e-01, -2.5378e+00, -1.9246e+00],
        [ 2.4684e-01,  8.7218e-01,  5.8153e-01],
        [-1.0988e+00, -1.4618e+00, -5.7284e-01],
        [ 1.4959e+00, -3.9180e-01, -2.0407e+00],
        [ 4.3676e-01, -1.9131e+00, -1.5457e+00],
        [-8.4875e-01,  3.3520e-01, -2.7567e-01],
        [ 9.3670e-01, -2.7922e+00, -1.8513e+00],
        [ 6.4560e-01, -9.8085e-01, -1.9119e-01],
        [ 3.4880e-01, -1.3704e+00, -3.2489e+00],
        [ 2.3762e-01, -1.0093e+00, -1.1025e+00],
        [ 9.3308e-01, -2.0015e+00, -2.1835e+00],
        [-6.6261e-01, -3.9993e-01, -7.4186e-01],
        [ 1.5721e-01, -2.1871e+00, -1.3741e-01],
        [-1.0487e-01, -2.2405e+00, -9.3238e-01],
        [ 1.1471e+00, -7.5838e-01, -1.3005e+00],
        [ 4.1819e-01, -7.0042e-01, -1.0375e+00],
        [-3.2179e-01, -3.3072e+00, -1.8379e+00],
        [ 7.8278e-01, -5.7836e-01, -1.1555e+00],
        [ 6.0502e-01, -2.8172e-01, -5.7779e-01],
        [-1.0191e+00, -1.3242e+00,  1.1458e+00],
        [-5.5360e-01, -1.7290e+00,  7.5063e-01],
        [ 2.2789e-01, -2.0806e+00, -3.6290e-01],
        [ 4.1442e-01, -1.1537e+00, -8.0315e-01],
        [-6.4143e-01, -5.4536e-01, -1.6199e+00],
        [ 4.9734e-01, -1.2377e+00, -6.6119e-01],
        [ 8.0438e-01, -8.4542e-01, -1.6979e+00],
        [ 5.3002e-02, -4.7520e-01, -1.4179e+00],
        [-2.4274e-01, -1.8722e+00, -1.6923e+00],
        [-3.9424e-01,  8.9142e-01, -9.7477e-01],
        [-9.1233e-01, -1.5527e+00, -2.8616e-01],
        [-1.9704e+00, -6.3603e-01, -1.4815e+00],
        [-1.6288e+00, -1.9337e-01, -5.5544e-01],
        [-1.9368e-01, -1.7001e+00, -1.6102e+00],
        [ 7.9063e-01,  1.9539e-01, -1.3674e+00],
        [ 1.0458e+00, -1.4273e+00, -1.5713e+00],
        [-3.2962e-01, -3.8121e-01, -7.3679e-01],
        [ 2.1690e-01, -1.4596e+00, -2.0485e-01],
        [ 7.0817e-01, -1.1204e+00, -2.5464e+00],
        [ 1.5970e-01, -2.0929e+00, -1.0562e+00],
        [ 1.3728e+00, -1.6768e+00, -2.7980e+00],
        [-1.5008e-01, -2.4519e+00, -1.1158e+00],
        [-5.1075e-01, -2.8173e-01, -7.5905e-01],
        [-5.4594e-01, -7.7560e-01, -4.0732e-02],
        [-7.2780e-01, -2.0333e+00, -7.1011e-02],
        [-1.1444e+00, -3.0073e-01, -1.2420e+00],
        [-3.8631e-01, -1.9758e+00, -9.4053e-01],
        [ 1.4322e-01, -2.1503e+00, -2.1140e+00],
        [-9.1797e-01, -1.1191e+00, -2.5287e-01],
        [ 4.5305e-01, -1.7052e+00, -2.2382e+00],
        [-1.0629e-01,  8.5485e-02, -1.7950e+00],
        [ 5.3574e-01, -2.8921e-01, -2.0991e+00],
        [-6.2656e-01,  6.1568e-01, -1.1338e+00],
        [ 5.3201e-01, -2.0545e+00, -1.8970e+00],
        [-8.1435e-02,  8.9472e-01, -6.1362e-02],
        [ 1.8937e-01, -8.6876e-01, -9.7096e-01],
        [ 8.9883e-01,  7.6767e-01, -5.0339e-01],
        [ 5.3963e-01, -1.6792e+00, -8.0361e-01],
        [-4.8432e-01,  3.5821e-01, -9.3586e-01],
        [ 1.9944e+00, -2.1295e+00, -1.2508e+00],
        [-2.0348e-01, -4.5462e-01, -1.2710e+00],
        [ 1.6377e+00, -5.1880e-01, -2.0760e+00],
        [ 1.6107e+00,  4.9899e-01, -4.8033e-01],
        [ 1.8144e+00, -1.0311e+00, -2.1020e+00],
        [ 2.5320e+00, -1.7077e-01, -1.3717e+00],
        [ 1.3780e+00, -1.2508e-01, -1.1626e+00],
        [ 8.6885e-01, -1.6585e+00, -9.2096e-01],
        [-4.2018e-01, -2.5317e+00, -1.1976e+00],
        [ 1.0182e+00, -1.1523e+00, -9.5715e-01],
        [ 1.7423e+00, -1.7203e+00, -1.1063e+00],
        [ 4.6482e-02, -9.0998e-02, -1.2792e+00],
        [-7.0291e-01, -3.1712e+00, -7.0714e-01],
        [ 6.0913e-01, -1.1675e+00, -4.4470e-01],
        [ 3.2220e-02, -2.9771e+00,  3.1249e-01],
        [ 1.7995e+00, -5.7880e-01, -2.1291e+00],
        [-6.6210e-02, -2.6707e+00, -3.8324e-01],
        [ 6.9477e-01,  3.4835e-01,  5.7672e-01],
        [-1.4264e+00, -1.8965e+00,  2.3412e-01],
        [ 4.6105e-01, -1.2708e+00,  1.3308e+00],
        [-3.2963e-01, -1.2417e+00,  3.0474e-01],
        [ 1.3474e+00,  5.3927e-02, -2.6981e-01]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 16:42:44 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 16:42:44 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 16:42:44 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 16:42:51 - INFO - Epoch : 1 , Batch [ 0 / 93 ] : Loss = 0.629374, Accuracy = 32.03%, MSE = 1.4766
2025-03-01 16:42:51 - INFO - Classwise accuracy : (0: 67.53% (77)), (1: 10.53% (95)), (2: 23.81% (84))
2025-03-01 16:45:14 - INFO - Epoch : 1 , Batch [ 10 / 93 ] : Loss = 0.936047, Accuracy = 33.42%, MSE = 1.4904
2025-03-01 16:45:14 - INFO - Classwise accuracy : (0: 96.20% (79)), (1: 0.00% (101)), (2: 1.32% (76))
2025-03-01 16:47:32 - INFO - Epoch : 1 , Batch [ 20 / 93 ] : Loss = 0.432324, Accuracy = 33.85%, MSE = 1.5454
2025-03-01 16:47:32 - INFO - Classwise accuracy : (0: 77.89% (95)), (1: 6.41% (78)), (2: 12.05% (83))
2025-03-01 16:49:49 - INFO - Epoch : 1 , Batch [ 30 / 93 ] : Loss = 0.406345, Accuracy = 33.71%, MSE = 1.5433
2025-03-01 16:49:49 - INFO - Classwise accuracy : (0: 9.47% (95)), (1: 1.27% (79)), (2: 85.37% (82))
2025-03-01 16:52:17 - INFO - Epoch : 1 , Batch [ 40 / 93 ] : Loss = 0.334188, Accuracy = 33.46%, MSE = 1.5623
2025-03-01 16:52:17 - INFO - Classwise accuracy : (0: 53.57% (84)), (1: 5.75% (87)), (2: 54.12% (85))
2025-03-01 16:54:25 - INFO - Epoch : 1 , Batch [ 50 / 93 ] : Loss = 0.344851, Accuracy = 33.46%, MSE = 1.5774
2025-03-01 16:54:25 - INFO - Classwise accuracy : (0: 25.29% (87)), (1: 0.00% (86)), (2: 73.49% (83))
2025-03-01 16:56:42 - INFO - Epoch : 1 , Batch [ 60 / 93 ] : Loss = 0.327596, Accuracy = 33.52%, MSE = 1.5860
2025-03-01 16:56:42 - INFO - Classwise accuracy : (0: 78.16% (87)), (1: 8.70% (92)), (2: 18.18% (77))
2025-03-01 16:58:46 - INFO - Epoch : 1 , Batch [ 70 / 93 ] : Loss = 0.319978, Accuracy = 33.46%, MSE = 1.5944
2025-03-01 16:58:46 - INFO - Classwise accuracy : (0: 48.15% (81)), (1: 0.00% (78)), (2: 57.73% (97))
2025-03-01 17:01:02 - INFO - Epoch : 1 , Batch [ 80 / 93 ] : Loss = 0.327592, Accuracy = 33.40%, MSE = 1.6034
2025-03-01 17:01:02 - INFO - Classwise accuracy : (0: 87.23% (94)), (1: 0.00% (82)), (2: 15.00% (80))
2025-03-01 17:03:11 - INFO - Epoch : 1 , Batch [ 90 / 93 ] : Loss = 0.326476, Accuracy = 33.40%, MSE = 1.6061
2025-03-01 17:03:11 - INFO - Classwise accuracy : (0: 59.77% (87)), (1: 0.00% (80)), (2: 49.44% (89))
2025-03-01 17:03:46 - INFO - Epoch 1: Train Loss=0.4872, Train Acc=33.47%, Train MSE=1.6010
2025-03-01 17:03:52 - INFO - Epoch 1: Val Loss=nan, Val Acc=12.28%
2025-03-01 17:03:52 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.12      1.00      0.22       175
        Hold       0.00      0.00      0.00       987
         Buy       0.00      0.00      0.00       263

    accuracy                           0.12      1425
   macro avg       0.04      0.33      0.07      1425
weighted avg       0.02      0.12      0.03      1425

2025-03-01 17:04:12 - INFO - Epoch : 2 , Batch [ 0 / 93 ] : Loss = 0.320316, Accuracy = 35.55%, MSE = 1.6172
2025-03-01 17:04:12 - INFO - Classwise accuracy : (0: 95.40% (87)), (1: 0.00% (82)), (2: 9.20% (87))
2025-03-01 17:07:28 - INFO - Epoch : 2 , Batch [ 10 / 93 ] : Loss = 0.329891, Accuracy = 33.49%, MSE = 1.5920
2025-03-01 17:07:28 - INFO - Classwise accuracy : (0: 45.12% (82)), (1: 10.00% (90)), (2: 51.19% (84))
2025-03-01 17:10:54 - INFO - Epoch : 2 , Batch [ 20 / 93 ] : Loss = 0.342234, Accuracy = 33.91%, MSE = 1.6129
2025-03-01 17:10:54 - INFO - Classwise accuracy : (0: 83.70% (92)), (1: 0.00% (73)), (2: 14.29% (91))
2025-03-01 17:13:58 - INFO - Epoch : 2 , Batch [ 30 / 93 ] : Loss = 0.321955, Accuracy = 33.69%, MSE = 1.6266
2025-03-01 17:13:58 - INFO - Classwise accuracy : (0: 49.32% (73)), (1: 0.00% (100)), (2: 42.17% (83))
2025-03-01 17:16:38 - INFO - Epoch : 2 , Batch [ 40 / 93 ] : Loss = 0.334414, Accuracy = 33.45%, MSE = 1.6299
2025-03-01 17:16:38 - INFO - Classwise accuracy : (0: 98.73% (79)), (1: 0.00% (95)), (2: 0.00% (82))
2025-03-01 17:19:14 - INFO - Epoch : 2 , Batch [ 50 / 93 ] : Loss = 0.327153, Accuracy = 33.63%, MSE = 1.6260
2025-03-01 17:19:14 - INFO - Classwise accuracy : (0: 92.63% (95)), (1: 14.46% (83)), (2: 2.56% (78))
2025-03-01 17:21:13 - INFO - Epoch : 2 , Batch [ 60 / 93 ] : Loss = 0.435954, Accuracy = 33.64%, MSE = 1.6128
2025-03-01 17:21:13 - INFO - Classwise accuracy : (0: 5.95% (84)), (1: 95.00% (80)), (2: 4.35% (92))
2025-03-01 17:23:30 - INFO - Epoch : 2 , Batch [ 70 / 93 ] : Loss = 0.318844, Accuracy = 33.61%, MSE = 1.5989
2025-03-01 17:23:30 - INFO - Classwise accuracy : (0: 51.16% (86)), (1: 0.00% (75)), (2: 64.21% (95))
2025-03-01 17:25:50 - INFO - Epoch : 2 , Batch [ 80 / 93 ] : Loss = 0.317059, Accuracy = 33.56%, MSE = 1.6017
2025-03-01 17:25:50 - INFO - Classwise accuracy : (0: 80.68% (88)), (1: 0.00% (83)), (2: 16.47% (85))
2025-03-01 17:28:29 - INFO - Epoch : 2 , Batch [ 90 / 93 ] : Loss = 0.327923, Accuracy = 33.83%, MSE = 1.5969
2025-03-01 17:28:29 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (83)), (2: 0.00% (86))
2025-03-01 17:29:11 - INFO - Epoch 2: Train Loss=0.3439, Train Acc=33.85%, Train MSE=1.5962
2025-03-01 17:29:17 - INFO - Epoch 2: Val Loss=nan, Val Acc=18.74%
2025-03-01 17:29:17 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      0.09      0.12       175
        Hold       0.00      0.00      0.00       987
         Buy       0.19      0.95      0.32       263

    accuracy                           0.19      1425
   macro avg       0.12      0.35      0.14      1425
weighted avg       0.05      0.19      0.07      1425

2025-03-01 17:29:33 - INFO - Epoch : 3 , Batch [ 0 / 93 ] : Loss = 0.317812, Accuracy = 39.45%, MSE = 1.3906
2025-03-01 17:29:33 - INFO - Classwise accuracy : (0: 51.19% (84)), (1: 11.76% (85)), (2: 55.17% (87))
2025-03-01 17:32:57 - INFO - Epoch : 3 , Batch [ 10 / 93 ] : Loss = 0.354326, Accuracy = 35.48%, MSE = 1.5391
2025-03-01 17:32:57 - INFO - Classwise accuracy : (0: 3.49% (86)), (1: 0.00% (96)), (2: 100.00% (74))
2025-03-01 17:35:59 - INFO - Epoch : 3 , Batch [ 20 / 93 ] : Loss = 0.339697, Accuracy = 35.16%, MSE = 1.5642
2025-03-01 17:35:59 - INFO - Classwise accuracy : (0: 5.81% (86)), (1: 3.61% (83)), (2: 94.25% (87))
2025-03-01 17:38:41 - INFO - Epoch : 3 , Batch [ 30 / 93 ] : Loss = 0.314041, Accuracy = 35.45%, MSE = 1.5475
2025-03-01 17:38:41 - INFO - Classwise accuracy : (0: 68.49% (73)), (1: 0.00% (98)), (2: 48.24% (85))
2025-03-01 17:40:58 - INFO - Epoch : 3 , Batch [ 40 / 93 ] : Loss = 0.315002, Accuracy = 35.19%, MSE = 1.5541
2025-03-01 17:40:58 - INFO - Classwise accuracy : (0: 73.17% (82)), (1: 0.00% (86)), (2: 28.41% (88))
2025-03-01 17:43:13 - INFO - Epoch : 3 , Batch [ 50 / 93 ] : Loss = 0.322255, Accuracy = 35.14%, MSE = 1.5622
2025-03-01 17:43:13 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (79)), (2: 1.04% (96))
2025-03-01 17:45:29 - INFO - Epoch : 3 , Batch [ 60 / 93 ] : Loss = 0.308988, Accuracy = 35.43%, MSE = 1.5544
2025-03-01 17:45:29 - INFO - Classwise accuracy : (0: 54.55% (77)), (1: 4.55% (88)), (2: 62.64% (91))
2025-03-01 17:48:04 - INFO - Epoch : 3 , Batch [ 70 / 93 ] : Loss = 0.314072, Accuracy = 36.09%, MSE = 1.5343
2025-03-01 17:48:04 - INFO - Classwise accuracy : (0: 69.62% (79)), (1: 0.00% (76)), (2: 42.57% (101))
2025-03-01 17:50:29 - INFO - Epoch : 3 , Batch [ 80 / 93 ] : Loss = 0.309766, Accuracy = 36.41%, MSE = 1.5242
2025-03-01 17:50:29 - INFO - Classwise accuracy : (0: 77.65% (85)), (1: 0.00% (86)), (2: 36.47% (85))
2025-03-01 17:53:23 - INFO - Epoch : 3 , Batch [ 90 / 93 ] : Loss = 0.303330, Accuracy = 36.79%, MSE = 1.5179
2025-03-01 17:53:23 - INFO - Classwise accuracy : (0: 65.62% (96)), (1: 3.75% (80)), (2: 50.00% (80))
2025-03-01 17:53:53 - INFO - Epoch 3: Train Loss=0.3156, Train Acc=36.73%, Train MSE=1.5254
2025-03-01 17:53:58 - INFO - Epoch 3: Val Loss=nan, Val Acc=18.74%
2025-03-01 17:53:58 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      0.09      0.12       175
        Hold       0.00      0.00      0.00       987
         Buy       0.19      0.95      0.32       263

    accuracy                           0.19      1425
   macro avg       0.12      0.35      0.14      1425
weighted avg       0.05      0.19      0.07      1425

2025-03-01 17:54:16 - INFO - Epoch : 4 , Batch [ 0 / 93 ] : Loss = 0.307534, Accuracy = 36.72%, MSE = 1.4531
2025-03-01 17:54:16 - INFO - Classwise accuracy : (0: 71.76% (85)), (1: 0.00% (92)), (2: 41.77% (79))
2025-03-01 17:56:25 - INFO - Epoch : 4 , Batch [ 10 / 93 ] : Loss = 0.298790, Accuracy = 39.28%, MSE = 1.4062
2025-03-01 17:56:25 - INFO - Classwise accuracy : (0: 97.44% (78)), (1: 6.25% (80)), (2: 13.27% (98))
2025-03-01 17:58:27 - INFO - Epoch : 4 , Batch [ 20 / 93 ] : Loss = 0.310270, Accuracy = 39.73%, MSE = 1.3884
2025-03-01 17:58:27 - INFO - Classwise accuracy : (0: 93.18% (88)), (1: 1.20% (83)), (2: 14.12% (85))
2025-03-01 18:00:25 - INFO - Epoch : 4 , Batch [ 30 / 93 ] : Loss = 0.291542, Accuracy = 39.74%, MSE = 1.3983
2025-03-01 18:00:25 - INFO - Classwise accuracy : (0: 66.25% (80)), (1: 2.20% (91)), (2: 60.00% (85))
2025-03-01 18:02:35 - INFO - Epoch : 4 , Batch [ 40 / 93 ] : Loss = 0.315032, Accuracy = 39.91%, MSE = 1.3943
2025-03-01 18:02:35 - INFO - Classwise accuracy : (0: 96.94% (98)), (1: 12.70% (63)), (2: 18.95% (95))
2025-03-01 18:05:13 - INFO - Epoch : 4 , Batch [ 50 / 93 ] : Loss = 0.303183, Accuracy = 39.99%, MSE = 1.4000
2025-03-01 18:05:13 - INFO - Classwise accuracy : (0: 28.75% (80)), (1: 1.14% (88)), (2: 88.64% (88))
2025-03-01 18:07:07 - INFO - Epoch : 4 , Batch [ 60 / 93 ] : Loss = 0.301846, Accuracy = 39.85%, MSE = 1.3993
2025-03-01 18:07:07 - INFO - Classwise accuracy : (0: 51.95% (77)), (1: 0.00% (98)), (2: 64.20% (81))
2025-03-01 18:09:12 - INFO - Epoch : 4 , Batch [ 70 / 93 ] : Loss = 0.309169, Accuracy = 39.60%, MSE = 1.4116
2025-03-01 18:09:12 - INFO - Classwise accuracy : (0: 91.40% (93)), (1: 0.00% (68)), (2: 24.21% (95))
2025-03-01 18:11:21 - INFO - Epoch : 4 , Batch [ 80 / 93 ] : Loss = 0.309606, Accuracy = 39.67%, MSE = 1.4054
2025-03-01 18:11:21 - INFO - Classwise accuracy : (0: 20.25% (79)), (1: 7.37% (95)), (2: 89.02% (82))
2025-03-01 18:13:23 - INFO - Epoch : 4 , Batch [ 90 / 93 ] : Loss = 0.309417, Accuracy = 39.58%, MSE = 1.4143
2025-03-01 18:13:23 - INFO - Classwise accuracy : (0: 83.15% (89)), (1: 0.00% (75)), (2: 30.43% (92))
2025-03-01 18:14:03 - INFO - Epoch 4: Train Loss=0.3064, Train Acc=39.62%, Train MSE=1.4103
2025-03-01 18:14:09 - INFO - Epoch 4: Val Loss=nan, Val Acc=18.74%
2025-03-01 18:14:09 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      0.09      0.12       175
        Hold       0.00      0.00      0.00       987
         Buy       0.19      0.95      0.32       263

    accuracy                           0.19      1425
   macro avg       0.12      0.35      0.14      1425
weighted avg       0.05      0.19      0.07      1425

2025-03-01 18:14:28 - INFO - Epoch : 5 , Batch [ 0 / 93 ] : Loss = 0.305375, Accuracy = 36.72%, MSE = 1.4531
2025-03-01 18:14:28 - INFO - Classwise accuracy : (0: 74.39% (82)), (1: 0.00% (92)), (2: 40.24% (82))
2025-03-01 18:17:36 - INFO - Epoch : 5 , Batch [ 10 / 93 ] : Loss = 0.369390, Accuracy = 40.59%, MSE = 1.3910
2025-03-01 18:17:36 - INFO - Classwise accuracy : (0: 5.56% (90)), (1: 1.23% (81)), (2: 100.00% (85))
2025-03-01 18:20:23 - INFO - Epoch : 5 , Batch [ 20 / 93 ] : Loss = 0.317559, Accuracy = 38.78%, MSE = 1.4721
2025-03-01 18:20:23 - INFO - Classwise accuracy : (0: 64.84% (91)), (1: 6.98% (86)), (2: 49.37% (79))
2025-03-01 18:32:16 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 18:35:00 - INFO - Full sequence shape : (201, 81)
2025-03-01 18:35:00 - INFO - Decoder sequence length : 188
2025-03-01 18:35:00 - INFO - Slope value shape : (188,)
2025-03-01 18:35:00 - INFO - Slope values : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 0 0 0 0 0 1 1 2 2 2 2 2 2 2 2 2
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0
 0 0 0]
2025-03-01 18:35:00 - INFO - Target bin max : 1
2025-03-01 18:35:00 - INFO - slope classes : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])
2025-03-01 18:35:00 - INFO - Slope tensor shape : torch.Size([188, 3])
2025-03-01 18:35:00 - INFO - Future price : 0
2025-03-01 18:35:00 - INFO - X shape : torch.Size([200, 51])
2025-03-01 18:35:00 - INFO - y shape : torch.Size([3])
2025-03-01 18:35:00 - INFO - y : tensor([1., 0., 0.])
2025-03-01 18:35:00 - INFO - y batch shape : torch.Size([256, 3])
2025-03-01 18:35:00 - INFO - Encoder Input shape : torch.Size([256, 200, 51])
2025-03-01 18:35:00 - INFO - Decoder Input shape : torch.Size([256, 188, 3])
2025-03-01 18:35:00 - INFO - Encoder input pt 1 : torch.Size([256, 200, 256])
2025-03-01 18:35:00 - INFO - Encoder input pt 2 : torch.Size([256, 200, 256])
2025-03-01 18:35:13 - INFO - Encoder output : torch.Size([256, 200, 256])
2025-03-01 18:35:13 - INFO - Decoder hidden pt 1 : torch.Size([256, 188, 256])
2025-03-01 18:35:13 - INFO - Decoder hidden pt 2 : torch.Size([256, 188, 256])
2025-03-01 18:35:41 - INFO - Decoder hidden pt 3 : torch.Size([256, 188, 256])
2025-03-01 18:35:41 - INFO - Final decoder output shape : torch.Size([256, 256])
2025-03-01 18:35:41 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 18:35:42 - INFO - Output s : tensor([[-2.0858e+00, -3.6324e-01,  3.1681e-01],
        [-8.1443e-01, -2.8449e+00,  2.9638e+00],
        [ 1.7063e+00, -1.5346e-01,  1.5018e+00],
        [-1.7480e+00, -2.5793e+00, -5.5068e-01],
        [-2.6381e-01, -6.5781e-01,  8.8935e-01],
        [ 1.2673e+00, -2.2166e+00,  1.3909e+00],
        [-9.0253e-01, -2.1314e+00,  4.8773e-01],
        [ 6.7896e-01, -7.9520e-02,  1.3357e-01],
        [ 1.8771e+00, -1.7080e+00,  2.3701e+00],
        [-5.2626e-01, -3.0984e+00,  9.3955e-01],
        [ 6.2516e-01,  5.7609e-01,  1.3593e-01],
        [ 1.4697e+00, -1.7891e+00,  4.9977e-01],
        [ 9.7334e-01, -1.0692e+00,  2.1235e+00],
        [-8.4124e-01, -3.1775e+00, -7.4626e-01],
        [ 7.0915e-01, -2.7888e+00,  2.0037e+00],
        [-7.4853e-01, -1.4188e+00,  3.2046e+00],
        [-3.0240e+00, -6.2131e-01, -1.7402e+00],
        [-9.7525e-01, -9.7269e-01,  3.3169e-01],
        [ 3.7498e-01, -1.0143e+00, -4.0740e-01],
        [ 1.4741e-01, -1.5682e+00,  2.5944e+00],
        [ 3.7147e-01, -3.7757e-01,  3.2788e+00],
        [-1.1433e+00, -7.3158e-01,  1.0882e+00],
        [-2.9153e-01, -7.0367e-01,  1.1309e+00],
        [-1.2956e+00, -3.8690e-01,  5.2235e-01],
        [-1.8174e+00, -1.2123e+00, -7.7965e-01],
        [-2.7623e+00, -1.9138e+00,  3.0172e-01],
        [-3.0353e-01, -1.8380e-01,  2.2716e+00],
        [-2.0117e+00, -1.8578e+00, -6.1051e-01],
        [-3.9816e-02, -1.2143e+00,  6.5732e-01],
        [ 1.0537e+00, -1.1851e+00,  2.9172e-01],
        [-7.1623e-01, -1.3173e+00, -1.0587e-01],
        [ 3.9359e-01,  1.6290e+00, -1.2982e+00],
        [ 3.4009e-01,  4.1220e-01, -4.0877e-03],
        [-9.6567e-01, -4.9599e-01, -8.8584e-01],
        [-2.0974e+00, -7.7968e-01, -2.3087e-01],
        [-3.5136e-01, -4.5318e-01,  1.9274e+00],
        [-3.5085e-01, -3.9858e-01,  1.6869e+00],
        [ 7.6289e-01, -4.0115e-02,  8.4546e-01],
        [ 3.4789e-01, -1.5445e+00,  1.9454e+00],
        [-1.4989e+00, -1.7973e+00,  3.2123e-01],
        [-2.5301e-01, -2.1309e+00,  2.7811e+00],
        [ 1.2895e+00, -1.7430e+00,  3.0026e+00],
        [-1.3700e+00, -1.8900e+00,  3.4876e+00],
        [ 2.4728e+00, -4.8793e-01,  2.1150e+00],
        [-1.2518e+00, -4.3076e-01,  1.5170e-01],
        [-1.9873e+00, -4.6393e-01,  1.1593e+00],
        [ 1.9558e-01, -9.2329e-01,  1.7251e+00],
        [ 5.7309e-02, -1.7014e+00,  2.1662e+00],
        [ 4.2880e-02,  6.3657e-01,  2.0678e+00],
        [ 2.2753e+00, -8.0905e-01,  1.4638e+00],
        [ 7.1164e-01, -8.4713e-01,  3.4205e+00],
        [ 9.8111e-01,  7.8533e-01,  2.6496e+00],
        [-1.5699e+00, -2.7344e-01, -7.8451e-01],
        [-1.3564e+00, -2.4584e-01, -1.1592e+00],
        [-1.6605e+00, -1.5025e+00, -7.4535e-01],
        [-1.8506e-01, -1.5567e+00,  1.6832e+00],
        [-2.9541e-01, -5.3840e-01, -1.5978e+00],
        [ 1.5864e-01,  2.2299e-01,  4.2418e-01],
        [-2.2238e-01, -1.8555e+00,  4.0207e+00],
        [-1.0806e+00, -2.8547e-01,  8.4366e-01],
        [-2.4789e+00,  4.5691e-01, -1.2187e+00],
        [-1.9052e-01, -3.3378e-01,  3.1074e+00],
        [ 2.4679e-01, -4.2419e-01,  2.0308e+00],
        [-5.0276e-02, -1.6760e+00,  2.8502e+00],
        [-8.1799e-01, -4.9794e-01,  4.3386e-02],
        [ 2.2689e+00, -2.5445e+00,  1.1662e+00],
        [-5.6477e-01, -2.0392e+00,  1.8141e+00],
        [ 2.0479e-01, -1.6091e+00,  3.9538e-01],
        [-1.3404e+00, -4.5378e-01,  2.7754e+00],
        [ 1.4119e-01, -1.7510e+00,  5.9516e-02],
        [ 1.4254e+00, -7.3883e-01,  1.5859e+00],
        [-9.3967e-01, -1.4650e+00,  9.1571e-01],
        [ 1.1159e+00, -6.9251e-01,  2.0174e+00],
        [-3.7966e+00,  6.5819e-01,  1.4167e-01],
        [-7.5894e-01, -2.6927e-01,  7.5603e-01],
        [-8.6095e-01, -1.8887e+00,  1.9512e+00],
        [-2.2295e+00,  2.2178e-02,  9.9254e-01],
        [-5.8948e-01, -2.1984e+00, -3.5674e-01],
        [-1.4792e+00, -2.9056e+00,  2.7879e+00],
        [ 2.6249e+00, -2.0045e+00,  5.7984e-02],
        [ 1.5604e+00, -2.3645e+00, -1.3807e-01],
        [ 6.9571e-01, -1.1830e+00,  3.1388e+00],
        [ 2.5628e-01, -2.5233e-01,  3.4911e+00],
        [-2.0423e+00,  8.5548e-02, -2.1536e-01],
        [ 9.6102e-01,  9.7847e-02,  1.3584e+00],
        [-4.3601e-02, -1.5238e+00,  6.5085e-01],
        [ 8.6330e-01, -1.6133e+00,  1.1830e+00],
        [ 1.3589e-01, -1.0344e+00,  1.3945e+00],
        [ 5.4350e-01, -4.9651e-01,  3.7767e-01],
        [-3.6334e-01, -3.2917e-01,  1.4906e+00],
        [ 2.2714e+00, -1.2328e+00,  1.9943e-01],
        [-1.4743e-01,  5.9717e-01,  1.1048e+00],
        [-6.1747e-01, -9.3609e-02,  1.0715e+00],
        [ 8.7145e-01,  6.0797e-01,  8.9745e-01],
        [-1.2675e-01, -2.0722e-01,  2.2262e-01],
        [ 2.2775e-01, -2.3013e+00,  2.8122e+00],
        [-1.0733e+00, -1.9710e-01,  2.0175e+00],
        [-1.3106e+00, -8.2551e-01,  1.6403e+00],
        [-1.3957e-01, -4.7681e-01,  3.7668e+00],
        [ 7.0534e-01, -2.0428e+00,  5.4058e-01],
        [ 5.3583e-01, -1.2837e+00,  1.6726e+00],
        [-1.8406e+00, -2.6451e+00,  1.1389e-01],
        [-6.9611e-01, -1.1410e+00,  1.4211e+00],
        [-9.9263e-01, -1.1583e+00, -8.3118e-01],
        [-4.1015e-01, -1.5339e+00,  7.9480e-01],
        [-6.5300e-01,  3.5169e-01,  4.9160e-01],
        [-1.2322e+00, -5.1378e-01,  4.6282e-01],
        [-3.9276e-01, -1.3584e+00,  8.1945e-01],
        [ 5.1020e-01, -1.1443e+00,  4.3621e-01],
        [-2.7038e-01, -1.2985e+00,  3.4031e+00],
        [-1.1811e+00, -2.9279e+00,  3.5412e+00],
        [ 8.3688e-01, -4.0220e-01,  1.5590e+00],
        [-5.0329e-02, -1.8074e+00,  2.0014e+00],
        [-6.9788e-01, -1.6407e+00, -1.2488e+00],
        [ 6.7318e-01, -4.6692e-01,  3.2696e-01],
        [-3.1678e+00, -3.5067e+00, -2.4688e-02],
        [ 4.5786e-01, -2.0855e+00,  6.7875e-01],
        [ 5.9148e-01, -4.4776e-01,  3.8199e+00],
        [-1.8458e+00,  8.1846e-01,  1.1084e+00],
        [ 4.8475e-01, -1.7504e+00,  1.0740e+00],
        [ 7.9108e-01,  6.2897e-01,  1.6686e+00],
        [ 6.0772e-02, -1.9637e+00, -6.7649e-01],
        [ 8.7974e-01, -2.4436e-01,  2.7840e-01],
        [-2.3782e+00, -1.3741e+00,  1.9918e-01],
        [-3.4893e-01, -3.4857e-01,  1.3094e+00],
        [-1.1397e+00, -8.3409e-01, -3.1512e-01],
        [-5.8957e-01, -1.3860e+00,  1.9921e-01],
        [-7.5491e-02, -1.7310e+00, -1.1819e-02],
        [-5.6042e-01, -1.2531e+00,  7.5016e-01],
        [ 1.1172e-01,  4.9520e-01,  5.3745e-01],
        [ 2.4973e-01, -8.3125e-01,  2.0081e+00],
        [ 1.1696e+00, -2.2400e+00,  2.9902e+00],
        [-4.7740e-03, -1.1346e+00,  3.1763e+00],
        [ 9.4526e-01, -2.4256e+00,  1.7419e+00],
        [-1.4598e-01, -2.1953e+00,  9.2926e-01],
        [-4.2097e-02, -2.4461e-01,  2.1081e-01],
        [ 2.9642e-01, -2.9175e-01,  8.1792e-01],
        [-1.9808e-01, -1.5468e+00,  3.4233e+00],
        [ 9.8005e-01, -5.8430e-01, -2.7090e-01],
        [ 1.5923e-01, -2.1090e+00, -8.3150e-01],
        [-1.4163e+00,  1.2573e-01,  3.0481e+00],
        [-6.9788e-01, -1.8318e+00,  3.2249e-02],
        [ 6.4526e-01, -8.3332e-02,  1.9596e+00],
        [-2.2380e-02, -3.3074e-01,  2.3003e+00],
        [-7.3985e-01, -1.6081e+00,  3.6100e+00],
        [-8.9223e-01, -2.5479e+00, -1.7160e+00],
        [ 7.2782e-01, -1.6500e-01,  3.2670e-01],
        [-4.7176e-03, -1.2202e+00,  1.5328e+00],
        [ 1.5431e+00, -1.3937e+00,  2.2963e+00],
        [-3.1589e+00, -1.4292e+00, -8.3516e-01],
        [-6.6810e-01, -4.2447e-01,  5.0421e-01],
        [-1.5272e+00, -7.1329e-01,  9.9940e-02],
        [ 3.7135e-01, -1.3427e+00,  2.9216e+00],
        [-2.2182e+00, -1.0605e+00, -1.2395e+00],
        [ 1.2412e-01, -7.1406e-01,  1.0535e+00],
        [-4.7094e-01, -1.2967e+00,  2.7615e+00],
        [ 2.1725e-01, -5.8374e-01,  2.5486e+00],
        [-1.8268e-01, -9.5362e-01,  2.3860e+00],
        [ 7.4949e-01, -1.1435e+00,  3.4856e+00],
        [-1.3096e-01, -8.3925e-01,  2.5114e-01],
        [-3.1839e-02, -5.4714e-01,  2.2078e+00],
        [ 2.1257e-01, -1.4387e+00,  5.4684e-01],
        [-2.4431e-01, -1.9802e+00,  3.2937e+00],
        [ 5.3279e-01, -1.5739e+00,  3.4276e+00],
        [ 9.6999e-01, -7.3517e-01,  3.6988e-01],
        [-1.6159e+00, -2.4444e+00, -8.0015e-01],
        [ 2.4375e-02, -9.5352e-01,  2.3682e+00],
        [-5.9604e-01,  5.7270e-01,  1.8537e+00],
        [-1.6986e-01, -4.1437e-01,  3.4088e+00],
        [-1.2421e-01, -2.8358e-01,  1.7377e+00],
        [ 9.7649e-01, -8.9641e-01,  8.0684e-02],
        [ 4.5250e-01, -1.4247e+00, -6.9556e-01],
        [-7.7175e-02, -9.5090e-01,  6.2091e-01],
        [-1.3803e-01,  2.8510e-01,  3.5804e+00],
        [-1.2391e+00, -2.8309e+00, -1.0023e+00],
        [-1.1156e-01, -1.0782e+00,  2.8169e+00],
        [ 1.6929e-01, -2.7618e-02,  3.3457e+00],
        [-1.0178e+00, -9.3372e-01, -1.1104e+00],
        [-7.0320e-01, -1.0091e+00, -1.4687e+00],
        [-1.5302e-01, -2.3929e+00, -1.3391e+00],
        [-2.4004e-01, -2.8633e+00,  2.9478e+00],
        [ 1.5295e+00, -2.0608e-01,  1.8218e+00],
        [-9.5563e-01, -4.1926e-01,  1.9654e+00],
        [ 2.4771e-01, -1.6476e+00, -5.3622e-01],
        [-5.1221e-01,  3.8949e-01,  1.2295e-01],
        [-5.9770e-01, -9.5405e-01, -7.7369e-01],
        [ 5.3166e-01, -9.7072e-04,  2.7243e+00],
        [-1.9444e+00, -2.6051e+00, -9.9688e-01],
        [-1.8335e+00,  2.6197e-01, -1.0082e-01],
        [-9.3348e-01, -3.2577e+00, -1.6975e+00],
        [ 9.3767e-01, -9.1065e-01,  1.7819e+00],
        [-1.6887e-01, -3.0745e-02,  1.3951e+00],
        [ 1.7285e+00, -5.1059e-01,  1.4056e+00],
        [-2.9509e+00, -1.1861e+00, -2.4815e+00],
        [-1.5872e+00,  1.6987e+00, -8.6533e-01],
        [-7.4365e-01, -7.2807e-01,  8.8866e-01],
        [ 5.6491e-01, -6.1867e-01,  2.3713e+00],
        [ 1.0463e+00, -1.2666e+00,  8.8235e-01],
        [-1.3795e+00, -1.9226e+00, -7.3690e-01],
        [-1.3473e+00,  1.1746e-01, -7.8626e-02],
        [-8.3321e-01, -1.8061e+00,  1.4904e-02],
        [ 1.6463e+00, -1.6378e+00,  2.0118e+00],
        [-2.2724e+00, -3.2095e-01, -2.6529e-01],
        [-3.7878e-01, -2.6871e+00, -2.0584e+00],
        [-1.5528e+00, -1.9649e+00,  8.1433e-01],
        [-8.4270e-01, -2.5525e+00, -8.4160e-01],
        [-4.4330e-01, -5.1914e-01,  1.2223e+00],
        [ 1.2536e+00,  4.0766e-01, -4.5265e-01],
        [-1.1060e-01, -1.6253e+00,  9.6470e-01],
        [-3.5601e-02, -8.2717e-01,  2.1962e-01],
        [-8.5587e-02, -2.2784e+00,  2.3815e+00],
        [ 1.0453e+00, -3.7891e-01,  6.4351e-01],
        [ 1.1774e+00, -3.4619e-01,  8.2863e-01],
        [ 1.3215e+00, -1.2749e-01,  1.2128e+00],
        [-1.0535e+00, -6.0070e-01, -1.7081e-01],
        [-1.3569e+00, -1.3572e+00, -9.7060e-01],
        [ 1.4124e+00, -1.2419e-01,  1.8895e-01],
        [-7.7749e-01, -7.9806e-01,  6.5885e-01],
        [-2.3447e+00, -3.5689e-01,  3.6966e+00],
        [-5.7504e-01, -5.0559e-01, -1.1648e+00],
        [ 1.4370e-01, -1.6009e+00, -6.0432e-01],
        [-2.1421e-01, -6.7559e-02,  1.2951e+00],
        [-1.1211e+00, -2.3054e-02, -4.4743e-01],
        [ 9.5222e-01, -2.4470e+00,  1.3278e+00],
        [ 1.1789e-01, -2.1017e+00,  1.1633e-01],
        [-1.9959e+00, -7.0694e-01,  1.1141e-01],
        [ 8.8832e-01, -8.0898e-01,  2.9284e+00],
        [ 1.2816e+00, -1.6839e-01,  8.1284e-01],
        [ 8.7657e-01,  6.0334e-01,  1.0738e+00],
        [-2.7708e+00, -9.7598e-02, -5.0299e-01],
        [-7.8816e-01, -1.7787e-01,  2.3804e+00],
        [-3.0608e+00, -9.1930e-01, -1.0503e-01],
        [ 1.7815e+00,  1.5594e-01,  2.2615e+00],
        [-2.6761e+00, -2.1956e+00, -8.5752e-01],
        [ 1.2282e+00, -1.4181e+00,  2.7332e+00],
        [-3.3660e-01, -1.0755e+00, -9.4788e-01],
        [ 7.0605e-01, -1.6397e-01,  8.6863e-01],
        [ 3.8878e-01, -3.4084e-01,  2.0038e+00],
        [ 1.3025e+00, -1.9065e+00,  2.5900e-02],
        [ 4.8107e-01, -1.2348e+00, -8.2266e-01],
        [ 1.7283e-01, -7.6270e-01, -1.1218e+00],
        [-7.9252e-01, -2.9247e+00, -1.1511e+00],
        [ 5.1716e-01,  6.4148e-01,  1.8825e+00],
        [ 1.2799e-01, -7.4292e-01, -3.2907e-01],
        [-4.0670e-01, -1.2846e+00,  3.1949e+00],
        [ 1.5179e+00, -1.2853e+00,  1.8391e+00],
        [ 2.7077e-01,  4.2802e-01,  2.0406e+00],
        [-7.7652e-01, -1.2269e+00, -2.4820e-01],
        [-5.2358e-01, -1.7617e+00,  1.5689e+00],
        [-3.1636e+00,  1.6018e-01,  2.2764e-01],
        [ 4.5643e-01, -6.8442e-01,  3.4591e+00],
        [-1.2719e+00, -3.0887e+00,  3.1424e+00],
        [-3.3012e+00, -1.3167e+00,  7.3098e-01],
        [ 2.5588e-01, -3.5376e-01,  9.8611e-01],
        [-1.3763e+00, -1.8866e+00,  3.3708e+00],
        [-1.4289e+00, -2.3553e+00,  1.1760e+00]], device='mps:0',
       grad_fn=<LinearBackward0>)
2025-03-01 18:35:42 - INFO - Output shape : torch.Size([256, 3])
2025-03-01 18:35:42 - INFO - Output shape after view : torch.Size([256, 3])
2025-03-01 18:35:42 - INFO - y batch shape after view : torch.Size([256, 3])
2025-03-01 18:36:08 - INFO - Epoch : 1 , Batch [ 0 / 93 ] : Loss = 0.775176, Accuracy = 34.38%, MSE = 1.5469
2025-03-01 18:36:08 - INFO - Classwise accuracy : (0: 20.00% (85)), (1: 7.06% (85)), (2: 75.58% (86))
2025-03-01 18:40:36 - INFO - Epoch : 1 , Batch [ 10 / 93 ] : Loss = 72.022644, Accuracy = 33.17%, MSE = 1.2905
2025-03-01 18:40:36 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (92)), (2: 100.00% (87))
2025-03-01 18:44:26 - INFO - Epoch : 1 , Batch [ 20 / 93 ] : Loss = 38.285103, Accuracy = 33.37%, MSE = 1.3683
2025-03-01 18:44:26 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (84)), (2: 0.00% (90))
2025-03-01 18:48:15 - INFO - Epoch : 1 , Batch [ 30 / 93 ] : Loss = 17.240120, Accuracy = 32.59%, MSE = 1.4604
2025-03-01 18:48:15 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (91)), (2: 0.00% (89))
2025-03-01 18:52:03 - INFO - Epoch : 1 , Batch [ 40 / 93 ] : Loss = 4.713831, Accuracy = 32.58%, MSE = 1.4893
2025-03-01 18:52:03 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (93)), (2: 100.00% (80))
2025-03-01 18:55:58 - INFO - Epoch : 1 , Batch [ 50 / 93 ] : Loss = 2.537320, Accuracy = 32.64%, MSE = 1.4641
2025-03-01 18:55:58 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 100.00% (75)), (2: 0.00% (92))
2025-03-01 18:59:44 - INFO - Epoch : 1 , Batch [ 60 / 93 ] : Loss = 1.026225, Accuracy = 32.60%, MSE = 1.4780
2025-03-01 18:59:44 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 0.00% (82)), (2: 100.00% (84))
2025-03-01 19:03:31 - INFO - Epoch : 1 , Batch [ 70 / 93 ] : Loss = 0.737127, Accuracy = 32.78%, MSE = 1.4740
2025-03-01 19:03:31 - INFO - Classwise accuracy : (0: 66.25% (80)), (1: 0.00% (90)), (2: 39.53% (86))
2025-03-01 19:07:17 - INFO - Epoch : 1 , Batch [ 80 / 93 ] : Loss = 0.359594, Accuracy = 32.81%, MSE = 1.4786
2025-03-01 19:07:17 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (86)), (2: 0.00% (89))
2025-03-01 19:11:06 - INFO - Epoch : 1 , Batch [ 90 / 93 ] : Loss = 0.341360, Accuracy = 32.91%, MSE = 1.4933
2025-03-01 19:11:06 - INFO - Classwise accuracy : (0: 98.84% (86)), (1: 0.00% (83)), (2: 0.00% (87))
2025-03-01 19:11:38 - INFO - Epoch 1: Train Loss=15.8552, Train Acc=32.88%, Train MSE=1.4947
2025-03-01 19:11:54 - INFO - Epoch 1: Val Loss=0.2672, Val Acc=15.81%
2025-03-01 19:11:54 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 19:12:00 - INFO - Epoch : 2 , Batch [ 0 / 93 ] : Loss = 0.326019, Accuracy = 31.64%, MSE = 1.4805
2025-03-01 19:12:00 - INFO - Classwise accuracy : (0: 97.59% (83)), (1: 0.00% (101)), (2: 0.00% (72))
2025-03-01 19:13:42 - INFO - Epoch : 2 , Batch [ 10 / 93 ] : Loss = 0.319629, Accuracy = 33.74%, MSE = 1.5927
2025-03-01 19:13:42 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (91)), (2: 0.00% (76))
2025-03-01 19:15:39 - INFO - Epoch : 2 , Batch [ 20 / 93 ] : Loss = 0.323531, Accuracy = 33.61%, MSE = 1.6315
2025-03-01 19:15:39 - INFO - Classwise accuracy : (0: 27.27% (88)), (1: 0.00% (86)), (2: 57.32% (82))
2025-03-01 19:17:38 - INFO - Epoch : 2 , Batch [ 30 / 93 ] : Loss = 0.331715, Accuracy = 33.73%, MSE = 1.6372
2025-03-01 19:17:38 - INFO - Classwise accuracy : (0: 4.17% (96)), (1: 0.00% (87)), (2: 100.00% (73))
2025-03-01 19:19:44 - INFO - Epoch : 2 , Batch [ 40 / 93 ] : Loss = 0.317797, Accuracy = 33.52%, MSE = 1.6546
2025-03-01 19:19:44 - INFO - Classwise accuracy : (0: 98.86% (88)), (1: 0.00% (69)), (2: 2.02% (99))
2025-03-01 19:21:56 - INFO - Epoch : 2 , Batch [ 50 / 93 ] : Loss = 0.317722, Accuracy = 33.27%, MSE = 1.6604
2025-03-01 19:21:56 - INFO - Classwise accuracy : (0: 21.92% (73)), (1: 0.00% (98)), (2: 78.82% (85))
2025-03-01 19:24:07 - INFO - Epoch : 2 , Batch [ 60 / 93 ] : Loss = 0.324020, Accuracy = 33.35%, MSE = 1.6568
2025-03-01 19:24:07 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (81)), (2: 0.00% (94))
2025-03-01 19:26:09 - INFO - Epoch : 2 , Batch [ 70 / 93 ] : Loss = 0.319341, Accuracy = 33.38%, MSE = 1.6554
2025-03-01 19:26:09 - INFO - Classwise accuracy : (0: 96.91% (97)), (1: 0.00% (87)), (2: 5.56% (72))
2025-03-01 19:28:10 - INFO - Epoch : 2 , Batch [ 80 / 93 ] : Loss = 0.316850, Accuracy = 33.28%, MSE = 1.6594
2025-03-01 19:28:10 - INFO - Classwise accuracy : (0: 100.00% (93)), (1: 0.00% (91)), (2: 0.00% (72))
2025-03-01 19:30:22 - INFO - Epoch : 2 , Batch [ 90 / 93 ] : Loss = 0.327766, Accuracy = 33.10%, MSE = 1.6652
2025-03-01 19:30:22 - INFO - Classwise accuracy : (0: 7.37% (95)), (1: 0.00% (88)), (2: 97.26% (73))
2025-03-01 19:30:49 - INFO - Epoch 2: Train Loss=0.3242, Train Acc=33.04%, Train MSE=1.6672
2025-03-01 19:31:05 - INFO - Epoch 2: Val Loss=0.3143, Val Acc=15.81%
2025-03-01 19:31:05 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 19:31:09 - INFO - Epoch : 3 , Batch [ 0 / 93 ] : Loss = 0.322293, Accuracy = 30.08%, MSE = 1.7773
2025-03-01 19:31:09 - INFO - Classwise accuracy : (0: 100.00% (77)), (1: 0.00% (87)), (2: 0.00% (92))
2025-03-01 19:33:06 - INFO - Epoch : 3 , Batch [ 10 / 93 ] : Loss = 0.321041, Accuracy = 32.32%, MSE = 1.6644
2025-03-01 19:33:06 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (85)), (2: 0.00% (81))
2025-03-01 19:35:12 - INFO - Epoch : 3 , Batch [ 20 / 93 ] : Loss = 0.323940, Accuracy = 32.72%, MSE = 1.6566
2025-03-01 19:35:12 - INFO - Classwise accuracy : (0: 98.85% (87)), (1: 0.00% (77)), (2: 1.09% (92))
2025-03-01 19:37:14 - INFO - Epoch : 3 , Batch [ 30 / 93 ] : Loss = 0.321268, Accuracy = 32.46%, MSE = 1.6741
2025-03-01 19:37:14 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (79)), (2: 0.00% (93))
2025-03-01 19:39:18 - INFO - Epoch : 3 , Batch [ 40 / 93 ] : Loss = 0.323796, Accuracy = 32.85%, MSE = 1.6650
2025-03-01 19:39:18 - INFO - Classwise accuracy : (0: 60.20% (98)), (1: 0.00% (79)), (2: 35.44% (79))
2025-03-01 19:41:26 - INFO - Epoch : 3 , Batch [ 50 / 93 ] : Loss = 0.318110, Accuracy = 32.83%, MSE = 1.6710
2025-03-01 19:41:26 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (80)), (2: 0.00% (91))
2025-03-01 19:43:46 - INFO - Epoch : 3 , Batch [ 60 / 93 ] : Loss = 0.318271, Accuracy = 32.79%, MSE = 1.6741
2025-03-01 19:43:46 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (82)), (2: 2.20% (91))
2025-03-01 19:45:50 - INFO - Epoch : 3 , Batch [ 70 / 93 ] : Loss = 0.320894, Accuracy = 32.87%, MSE = 1.6733
2025-03-01 19:45:50 - INFO - Classwise accuracy : (0: 1.14% (88)), (1: 0.00% (86)), (2: 100.00% (82))
2025-03-01 19:47:57 - INFO - Epoch : 3 , Batch [ 80 / 93 ] : Loss = 0.317718, Accuracy = 32.84%, MSE = 1.6781
2025-03-01 19:47:57 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (82)), (2: 1.04% (96))
2025-03-01 19:50:04 - INFO - Epoch : 3 , Batch [ 90 / 93 ] : Loss = 0.324171, Accuracy = 32.92%, MSE = 1.6735
2025-03-01 19:50:04 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (90)), (2: 0.00% (81))
2025-03-01 19:50:26 - INFO - Epoch 3: Train Loss=0.3206, Train Acc=32.93%, Train MSE=1.6726
2025-03-01 19:50:43 - INFO - Epoch 3: Val Loss=0.3721, Val Acc=16.19%
2025-03-01 19:50:43 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 19:50:47 - INFO - Epoch : 4 , Batch [ 0 / 93 ] : Loss = 0.325993, Accuracy = 28.12%, MSE = 1.9141
2025-03-01 19:50:47 - INFO - Classwise accuracy : (0: 8.51% (94)), (1: 0.00% (82)), (2: 80.00% (80))
2025-03-01 19:52:47 - INFO - Epoch : 4 , Batch [ 10 / 93 ] : Loss = 0.325956, Accuracy = 32.10%, MSE = 1.6612
2025-03-01 19:52:47 - INFO - Classwise accuracy : (0: 1.22% (82)), (1: 0.00% (92)), (2: 98.78% (82))
2025-03-01 19:54:45 - INFO - Epoch : 4 , Batch [ 20 / 93 ] : Loss = 0.333431, Accuracy = 32.96%, MSE = 1.6453
2025-03-01 19:54:45 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (93)), (2: 100.00% (87))
2025-03-01 19:56:41 - INFO - Epoch : 4 , Batch [ 30 / 93 ] : Loss = 0.316446, Accuracy = 32.93%, MSE = 1.6510
2025-03-01 19:56:41 - INFO - Classwise accuracy : (0: 84.81% (79)), (1: 0.00% (81)), (2: 20.83% (96))
2025-03-01 19:58:32 - INFO - Epoch : 4 , Batch [ 40 / 93 ] : Loss = 0.428727, Accuracy = 32.81%, MSE = 1.6474
2025-03-01 19:58:33 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 0.00% (86)), (2: 100.00% (80))
2025-03-01 20:00:32 - INFO - Epoch : 4 , Batch [ 50 / 93 ] : Loss = 10.968088, Accuracy = 33.01%, MSE = 1.6177
2025-03-01 20:00:32 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (82)), (2: 0.00% (87))
2025-03-01 20:02:32 - INFO - Epoch : 4 , Batch [ 60 / 93 ] : Loss = 7.899349, Accuracy = 32.85%, MSE = 1.6030
2025-03-01 20:02:32 - INFO - Classwise accuracy : (0: 100.00% (75)), (1: 0.00% (80)), (2: 0.00% (101))
2025-03-01 20:04:35 - INFO - Epoch : 4 , Batch [ 70 / 93 ] : Loss = 26.609634, Accuracy = 32.55%, MSE = 1.5873
2025-03-01 20:04:35 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 100.00% (81)), (2: 0.00% (103))
2025-03-01 20:06:39 - INFO - Epoch : 4 , Batch [ 80 / 93 ] : Loss = 6.350065, Accuracy = 32.65%, MSE = 1.5838
2025-03-01 20:06:39 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (105)), (2: 0.00% (78))
2025-03-01 20:09:02 - INFO - Epoch : 4 , Batch [ 90 / 93 ] : Loss = 16.582724, Accuracy = 32.76%, MSE = 1.5837
2025-03-01 20:09:02 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (67)), (2: 100.00% (100))
2025-03-01 20:09:25 - INFO - Epoch 4: Train Loss=10.9583, Train Acc=32.76%, Train MSE=1.5717
2025-03-01 20:09:41 - INFO - Epoch 4: Val Loss=5.2972, Val Acc=15.81%
2025-03-01 20:09:41 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 20:09:46 - INFO - Epoch : 5 , Batch [ 0 / 93 ] : Loss = 6.553300, Accuracy = 28.52%, MSE = 1.8164
2025-03-01 20:09:46 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (89)), (2: 0.00% (94))
2025-03-01 20:11:35 - INFO - Epoch : 5 , Batch [ 10 / 93 ] : Loss = 5.434361, Accuracy = 33.06%, MSE = 1.5653
2025-03-01 20:11:35 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (84)), (2: 100.00% (89))
2025-03-01 20:13:28 - INFO - Epoch : 5 , Batch [ 20 / 93 ] : Loss = 2.227062, Accuracy = 32.76%, MSE = 1.5658
2025-03-01 20:13:28 - INFO - Classwise accuracy : (0: 100.00% (93)), (1: 0.00% (87)), (2: 0.00% (76))
2025-03-01 20:15:27 - INFO - Epoch : 5 , Batch [ 30 / 93 ] : Loss = 0.782262, Accuracy = 32.71%, MSE = 1.5654
2025-03-01 20:15:27 - INFO - Classwise accuracy : (0: 25.53% (94)), (1: 0.00% (84)), (2: 84.62% (78))
2025-03-01 20:17:33 - INFO - Epoch : 5 , Batch [ 40 / 93 ] : Loss = 0.443760, Accuracy = 32.58%, MSE = 1.5736
2025-03-01 20:17:33 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (82)), (2: 0.00% (91))
2025-03-01 20:19:37 - INFO - Epoch : 5 , Batch [ 50 / 93 ] : Loss = 0.363114, Accuracy = 32.92%, MSE = 1.5628
2025-03-01 20:19:37 - INFO - Classwise accuracy : (0: 97.70% (87)), (1: 8.99% (89)), (2: 0.00% (80))
2025-03-01 20:21:52 - INFO - Epoch : 5 , Batch [ 60 / 93 ] : Loss = 0.335387, Accuracy = 33.21%, MSE = 1.5741
2025-03-01 20:21:52 - INFO - Classwise accuracy : (0: 100.00% (93)), (1: 0.00% (77)), (2: 0.00% (86))
2025-03-01 20:24:05 - INFO - Epoch : 5 , Batch [ 70 / 93 ] : Loss = 0.347362, Accuracy = 33.15%, MSE = 1.5888
2025-03-01 20:24:05 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 0.00% (69)), (2: 100.00% (95))
2025-03-01 20:26:27 - INFO - Epoch : 5 , Batch [ 80 / 93 ] : Loss = 0.316748, Accuracy = 33.07%, MSE = 1.5992
2025-03-01 20:26:27 - INFO - Classwise accuracy : (0: 100.00% (79)), (1: 0.00% (104)), (2: 0.00% (73))
2025-03-01 20:29:00 - INFO - Epoch : 5 , Batch [ 90 / 93 ] : Loss = 0.322471, Accuracy = 33.10%, MSE = 1.6047
2025-03-01 20:29:00 - INFO - Classwise accuracy : (0: 3.19% (94)), (1: 0.00% (75)), (2: 90.80% (87))
2025-03-01 20:29:25 - INFO - Epoch 5: Train Loss=1.3136, Train Acc=33.11%, Train MSE=1.6035
2025-03-01 20:29:41 - INFO - Epoch 5: Val Loss=0.3170, Val Acc=15.81%
2025-03-01 20:29:41 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 20:29:47 - INFO - Epoch : 6 , Batch [ 0 / 93 ] : Loss = 0.324776, Accuracy = 30.47%, MSE = 1.8203
2025-03-01 20:29:47 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (82)), (2: 0.00% (96))
2025-03-01 20:31:42 - INFO - Epoch : 6 , Batch [ 10 / 93 ] : Loss = 0.325266, Accuracy = 34.27%, MSE = 1.6641
2025-03-01 20:31:42 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (79)), (2: 0.00% (90))
2025-03-01 20:34:03 - INFO - Epoch : 6 , Batch [ 20 / 93 ] : Loss = 0.320699, Accuracy = 34.19%, MSE = 1.6559
2025-03-01 20:34:03 - INFO - Classwise accuracy : (0: 100.00% (80)), (1: 0.00% (88)), (2: 0.00% (88))
2025-03-01 20:36:29 - INFO - Epoch : 6 , Batch [ 30 / 93 ] : Loss = 0.318924, Accuracy = 33.74%, MSE = 1.6545
2025-03-01 20:36:29 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (105)), (2: 0.00% (70))
2025-03-01 20:39:03 - INFO - Epoch : 6 , Batch [ 40 / 93 ] : Loss = 0.318512, Accuracy = 33.84%, MSE = 1.6508
2025-03-01 20:39:03 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 0.00% (90)), (2: 0.00% (84))
2025-03-01 20:41:44 - INFO - Epoch : 6 , Batch [ 50 / 93 ] : Loss = 0.317804, Accuracy = 33.88%, MSE = 1.6485
2025-03-01 20:41:44 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (72)), (2: 0.00% (96))
2025-03-01 20:44:19 - INFO - Epoch : 6 , Batch [ 60 / 93 ] : Loss = 0.319581, Accuracy = 33.65%, MSE = 1.6559
2025-03-01 20:44:19 - INFO - Classwise accuracy : (0: 94.05% (84)), (1: 0.00% (86)), (2: 5.81% (86))
2025-03-01 20:46:48 - INFO - Epoch : 6 , Batch [ 70 / 93 ] : Loss = 0.320616, Accuracy = 33.57%, MSE = 1.6562
2025-03-01 20:46:48 - INFO - Classwise accuracy : (0: 100.00% (65)), (1: 0.00% (93)), (2: 0.00% (98))
2025-03-01 20:49:27 - INFO - Epoch : 6 , Batch [ 80 / 93 ] : Loss = 0.317482, Accuracy = 33.25%, MSE = 1.6632
2025-03-01 20:49:27 - INFO - Classwise accuracy : (0: 28.00% (75)), (1: 0.00% (89)), (2: 76.09% (92))
2025-03-01 20:52:02 - INFO - Epoch : 6 , Batch [ 90 / 93 ] : Loss = 0.319520, Accuracy = 33.13%, MSE = 1.6662
2025-03-01 20:52:02 - INFO - Classwise accuracy : (0: 77.91% (86)), (1: 0.00% (84)), (2: 27.91% (86))
2025-03-01 20:52:27 - INFO - Epoch 6: Train Loss=0.3199, Train Acc=33.10%, Train MSE=1.6623
2025-03-01 20:52:44 - INFO - Epoch 6: Val Loss=0.3018, Val Acc=15.81%
2025-03-01 20:52:44 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 20:52:50 - INFO - Epoch : 7 , Batch [ 0 / 93 ] : Loss = 0.327339, Accuracy = 31.25%, MSE = 1.6484
2025-03-01 20:52:50 - INFO - Classwise accuracy : (0: 100.00% (80)), (1: 0.00% (94)), (2: 0.00% (82))
2025-03-01 20:54:45 - INFO - Epoch : 7 , Batch [ 10 / 93 ] : Loss = 0.325947, Accuracy = 34.09%, MSE = 1.6573
2025-03-01 20:54:45 - INFO - Classwise accuracy : (0: 100.00% (76)), (1: 0.00% (81)), (2: 0.00% (99))
2025-03-01 20:57:13 - INFO - Epoch : 7 , Batch [ 20 / 93 ] : Loss = 0.321605, Accuracy = 33.13%, MSE = 1.6693
2025-03-01 20:57:13 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (92)), (2: 100.00% (86))
2025-03-01 20:59:43 - INFO - Epoch : 7 , Batch [ 30 / 93 ] : Loss = 0.314317, Accuracy = 33.15%, MSE = 1.6619
2025-03-01 20:59:43 - INFO - Classwise accuracy : (0: 100.00% (100)), (1: 0.00% (85)), (2: 0.00% (71))
2025-03-01 21:02:19 - INFO - Epoch : 7 , Batch [ 40 / 93 ] : Loss = 0.317892, Accuracy = 33.18%, MSE = 1.6623
2025-03-01 21:02:19 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (84)), (2: 0.00% (87))
2025-03-01 21:04:37 - INFO - Epoch : 7 , Batch [ 50 / 93 ] : Loss = 0.326142, Accuracy = 33.43%, MSE = 1.6498
2025-03-01 21:04:37 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (79)), (2: 100.00% (93))
2025-03-01 21:06:45 - INFO - Epoch : 7 , Batch [ 60 / 93 ] : Loss = 0.320317, Accuracy = 33.34%, MSE = 1.6571
2025-03-01 21:06:45 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 0.00% (76)), (2: 0.00% (98))
2025-03-01 21:09:13 - INFO - Epoch : 7 , Batch [ 70 / 93 ] : Loss = 0.317705, Accuracy = 33.21%, MSE = 1.6638
2025-03-01 21:09:13 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (80)), (2: 0.00% (88))
2025-03-01 21:11:51 - INFO - Epoch : 7 , Batch [ 80 / 93 ] : Loss = 0.319093, Accuracy = 33.09%, MSE = 1.6705
2025-03-01 21:11:51 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (78)), (2: 100.00% (87))
2025-03-01 21:14:47 - INFO - Epoch : 7 , Batch [ 90 / 93 ] : Loss = 0.332731, Accuracy = 32.95%, MSE = 1.6710
2025-03-01 21:14:47 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 0.00% (95)), (2: 100.00% (75))
2025-03-01 21:15:11 - INFO - Epoch 7: Train Loss=0.3215, Train Acc=32.93%, Train MSE=1.6763
2025-03-01 21:15:27 - INFO - Epoch 7: Val Loss=0.3940, Val Acc=16.19%
2025-03-01 21:15:27 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 21:15:32 - INFO - Epoch : 8 , Batch [ 0 / 93 ] : Loss = 0.331901, Accuracy = 32.42%, MSE = 1.6484
2025-03-01 21:15:32 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (90)), (2: 100.00% (83))
2025-03-01 21:17:35 - INFO - Epoch : 8 , Batch [ 10 / 93 ] : Loss = 0.321814, Accuracy = 32.95%, MSE = 1.6772
2025-03-01 21:17:35 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (85)), (2: 100.00% (87))
2025-03-01 21:19:58 - INFO - Epoch : 8 , Batch [ 20 / 93 ] : Loss = 51.972054, Accuracy = 32.78%, MSE = 1.5902
2025-03-01 21:19:58 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 100.00% (84)), (2: 0.00% (87))
2025-03-01 21:22:20 - INFO - Epoch : 8 , Batch [ 30 / 93 ] : Loss = 20.993441, Accuracy = 32.57%, MSE = 1.5494
2025-03-01 21:22:20 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (94)), (2: 100.00% (86))
2025-03-01 21:24:35 - INFO - Epoch : 8 , Batch [ 40 / 93 ] : Loss = 23.720510, Accuracy = 32.50%, MSE = 1.5325
2025-03-01 21:24:35 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (94)), (2: 100.00% (87))
2025-03-01 21:26:58 - INFO - Epoch : 8 , Batch [ 50 / 93 ] : Loss = 10.256409, Accuracy = 32.60%, MSE = 1.5203
2025-03-01 21:26:58 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 100.00% (72)), (2: 0.00% (89))
2025-03-01 21:29:23 - INFO - Epoch : 8 , Batch [ 60 / 93 ] : Loss = 2.863766, Accuracy = 32.63%, MSE = 1.5228
2025-03-01 21:29:23 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (66)), (2: 0.00% (101))
2025-03-01 21:31:51 - INFO - Epoch : 8 , Batch [ 70 / 93 ] : Loss = 1.943510, Accuracy = 32.73%, MSE = 1.5275
2025-03-01 21:31:51 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (86)), (2: 0.00% (87))
2025-03-01 21:34:33 - INFO - Epoch : 8 , Batch [ 80 / 93 ] : Loss = 0.817624, Accuracy = 32.64%, MSE = 1.5335
2025-03-01 21:34:33 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (90)), (2: 0.00% (85))
2025-03-01 21:37:03 - INFO - Epoch : 8 , Batch [ 90 / 93 ] : Loss = 0.651252, Accuracy = 32.65%, MSE = 1.5225
2025-03-01 21:37:03 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (83)), (2: 0.00% (83))
2025-03-01 21:37:29 - INFO - Epoch 8: Train Loss=11.3576, Train Acc=32.65%, Train MSE=1.5261
2025-03-01 21:37:46 - INFO - Epoch 8: Val Loss=1.2942, Val Acc=16.19%
2025-03-01 21:37:46 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 21:37:51 - INFO - Epoch : 9 , Batch [ 0 / 93 ] : Loss = 0.812358, Accuracy = 36.33%, MSE = 1.6562
2025-03-01 21:37:51 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 0.00% (76)), (2: 100.00% (93))
2025-03-01 21:39:44 - INFO - Epoch : 9 , Batch [ 10 / 93 ] : Loss = 0.475653, Accuracy = 34.45%, MSE = 1.5675
2025-03-01 21:39:44 - INFO - Classwise accuracy : (0: 5.56% (90)), (1: 0.00% (95)), (2: 100.00% (71))
2025-03-01 21:41:42 - INFO - Epoch : 9 , Batch [ 20 / 93 ] : Loss = 0.338641, Accuracy = 34.26%, MSE = 1.5558
2025-03-01 21:41:42 - INFO - Classwise accuracy : (0: 78.75% (80)), (1: 0.00% (94)), (2: 24.39% (82))
2025-03-01 21:43:43 - INFO - Epoch : 9 , Batch [ 30 / 93 ] : Loss = 0.326571, Accuracy = 33.49%, MSE = 1.5924
2025-03-01 21:43:43 - INFO - Classwise accuracy : (0: 68.29% (82)), (1: 0.00% (83)), (2: 30.77% (91))
2025-03-01 21:45:47 - INFO - Epoch : 9 , Batch [ 40 / 93 ] : Loss = 0.323695, Accuracy = 33.07%, MSE = 1.6174
2025-03-01 21:45:47 - INFO - Classwise accuracy : (0: 67.02% (94)), (1: 0.00% (80)), (2: 24.39% (82))
2025-03-01 21:47:48 - INFO - Epoch : 9 , Batch [ 50 / 93 ] : Loss = 0.333278, Accuracy = 33.02%, MSE = 1.6328
2025-03-01 21:47:48 - INFO - Classwise accuracy : (0: 2.27% (88)), (1: 0.00% (87)), (2: 97.53% (81))
2025-03-01 21:49:47 - INFO - Epoch : 9 , Batch [ 60 / 93 ] : Loss = 0.314767, Accuracy = 33.09%, MSE = 1.6402
2025-03-01 21:49:47 - INFO - Classwise accuracy : (0: 100.00% (103)), (1: 0.00% (73)), (2: 0.00% (80))
2025-03-01 21:51:40 - INFO - Epoch : 9 , Batch [ 70 / 93 ] : Loss = 0.324021, Accuracy = 32.97%, MSE = 1.6433
2025-03-01 21:51:40 - INFO - Classwise accuracy : (0: 97.18% (71)), (1: 0.00% (95)), (2: 5.56% (90))
2025-03-01 21:53:37 - INFO - Epoch : 9 , Batch [ 80 / 93 ] : Loss = 0.317638, Accuracy = 33.04%, MSE = 1.6412
2025-03-01 21:53:37 - INFO - Classwise accuracy : (0: 31.33% (83)), (1: 0.00% (88)), (2: 80.00% (85))
2025-03-01 21:55:29 - INFO - Epoch : 9 , Batch [ 90 / 93 ] : Loss = 0.322871, Accuracy = 32.87%, MSE = 1.6507
2025-03-01 21:55:29 - INFO - Classwise accuracy : (0: 87.80% (82)), (1: 0.00% (81)), (2: 6.45% (93))
2025-03-01 21:55:50 - INFO - Epoch 9: Train Loss=0.3541, Train Acc=32.89%, Train MSE=1.6535
2025-03-01 21:56:06 - INFO - Epoch 9: Val Loss=0.3642, Val Acc=15.81%
2025-03-01 21:56:06 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 21:56:11 - INFO - Epoch : 10 , Batch [ 0 / 93 ] : Loss = 0.322190, Accuracy = 26.17%, MSE = 1.8984
2025-03-01 21:56:11 - INFO - Classwise accuracy : (0: 71.21% (66)), (1: 0.00% (90)), (2: 20.00% (100))
2025-03-01 21:58:24 - INFO - Epoch : 10 , Batch [ 10 / 93 ] : Loss = 0.333490, Accuracy = 32.03%, MSE = 1.6705
2025-03-01 21:58:24 - INFO - Classwise accuracy : (0: 3.85% (78)), (1: 0.00% (111)), (2: 98.51% (67))
2025-03-01 22:00:27 - INFO - Epoch : 10 , Batch [ 20 / 93 ] : Loss = 0.325029, Accuracy = 32.50%, MSE = 1.6817
2025-03-01 22:00:27 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (76)), (2: 0.00% (93))
2025-03-01 22:02:38 - INFO - Epoch : 10 , Batch [ 30 / 93 ] : Loss = 0.324894, Accuracy = 32.67%, MSE = 1.6735
2025-03-01 22:02:38 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (78)), (2: 0.00% (90))
2025-03-01 22:04:56 - INFO - Epoch : 10 , Batch [ 40 / 93 ] : Loss = 0.321307, Accuracy = 32.74%, MSE = 1.6865
2025-03-01 22:04:56 - INFO - Classwise accuracy : (0: 60.22% (93)), (1: 0.00% (82)), (2: 23.46% (81))
2025-03-01 22:07:14 - INFO - Epoch : 10 , Batch [ 50 / 93 ] : Loss = 0.317763, Accuracy = 32.64%, MSE = 1.6941
2025-03-01 22:07:14 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (75)), (2: 0.00% (93))
2025-03-01 22:09:37 - INFO - Epoch : 10 , Batch [ 60 / 93 ] : Loss = 0.328248, Accuracy = 32.59%, MSE = 1.6836
2025-03-01 22:09:37 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (95)), (2: 1.14% (88))
2025-03-01 22:12:02 - INFO - Epoch : 10 , Batch [ 70 / 93 ] : Loss = 0.314224, Accuracy = 32.88%, MSE = 1.6720
2025-03-01 22:12:02 - INFO - Classwise accuracy : (0: 100.00% (93)), (1: 0.00% (94)), (2: 0.00% (69))
2025-03-01 22:14:27 - INFO - Epoch : 10 , Batch [ 80 / 93 ] : Loss = 0.322692, Accuracy = 33.03%, MSE = 1.6621
2025-03-01 22:14:27 - INFO - Classwise accuracy : (0: 12.00% (75)), (1: 0.00% (93)), (2: 93.18% (88))
2025-03-01 22:16:50 - INFO - Epoch : 10 , Batch [ 90 / 93 ] : Loss = 0.319979, Accuracy = 33.26%, MSE = 1.6590
2025-03-01 22:16:50 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 0.00% (77)), (2: 100.00% (97))
2025-03-01 22:17:15 - INFO - Epoch 10: Train Loss=0.3254, Train Acc=33.26%, Train MSE=1.6582
2025-03-01 22:17:31 - INFO - Epoch 10: Val Loss=0.3561, Val Acc=15.81%
2025-03-01 22:17:31 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-01 22:17:31 - INFO - Updated class weights : tensor([1.0000, 0.4104, 0.8651], device='mps:0')
2025-03-01 22:17:35 - INFO - Epoch : 11 , Batch [ 0 / 93 ] : Loss = 0.362262, Accuracy = 37.89%, MSE = 1.5703
2025-03-01 22:17:35 - INFO - Classwise accuracy : (0: 100.00% (97)), (1: 0.00% (78)), (2: 0.00% (81))
2025-03-01 22:19:43 - INFO - Epoch : 11 , Batch [ 10 / 93 ] : Loss = 0.386823, Accuracy = 33.91%, MSE = 1.6133
2025-03-01 22:19:43 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (82)), (2: 0.00% (101))
2025-03-01 22:22:03 - INFO - Epoch : 11 , Batch [ 20 / 93 ] : Loss = 0.348920, Accuracy = 33.89%, MSE = 1.6148
2025-03-01 22:22:03 - INFO - Classwise accuracy : (0: 1.35% (74)), (1: 0.00% (95)), (2: 97.70% (87))
2025-03-01 22:24:28 - INFO - Epoch : 11 , Batch [ 30 / 93 ] : Loss = 0.345863, Accuracy = 34.02%, MSE = 1.6249
2025-03-01 22:24:28 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (88)), (2: 100.00% (89))
2025-03-01 22:26:58 - INFO - Epoch : 11 , Batch [ 40 / 93 ] : Loss = 0.370093, Accuracy = 33.89%, MSE = 1.6172
2025-03-01 22:26:58 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (81)), (2: 0.00% (94))
2025-03-01 22:29:28 - INFO - Epoch : 11 , Batch [ 50 / 93 ] : Loss = 1.158617, Accuracy = 33.87%, MSE = 1.6227
2025-03-01 22:29:28 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 0.00% (94)), (2: 100.00% (90))
2025-03-01 22:31:58 - INFO - Epoch : 11 , Batch [ 60 / 93 ] : Loss = 28.979136, Accuracy = 33.86%, MSE = 1.5761
2025-03-01 22:31:58 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 0.00% (73)), (2: 100.00% (93))
2025-03-01 22:34:11 - INFO - Epoch : 11 , Batch [ 70 / 93 ] : Loss = 22.440214, Accuracy = 33.87%, MSE = 1.5618
2025-03-01 22:34:11 - INFO - Classwise accuracy : (0: 0.00% (70)), (1: 100.00% (89)), (2: 0.00% (97))
2025-03-01 22:36:36 - INFO - Epoch : 11 , Batch [ 80 / 93 ] : Loss = 14.984685, Accuracy = 33.67%, MSE = 1.5538
2025-03-01 22:36:36 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 0.00% (93)), (2: 100.00% (76))
2025-03-01 22:38:51 - INFO - Epoch : 11 , Batch [ 90 / 93 ] : Loss = 22.854549, Accuracy = 33.58%, MSE = 1.5345
2025-03-01 22:38:51 - INFO - Classwise accuracy : (0: 100.00% (77)), (1: 0.00% (88)), (2: 0.00% (91))
2025-03-01 22:39:14 - INFO - Epoch 11: Train Loss=8.0546, Train Acc=33.59%, Train MSE=1.5385
2025-03-01 22:39:30 - INFO - Epoch 11: Val Loss=12.3820, Val Acc=16.19%
2025-03-01 22:39:30 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 22:39:30 - INFO - Updated class weights : tensor([1.0000, 0.4811, 0.8813], device='mps:0')
2025-03-01 22:39:34 - INFO - Epoch : 12 , Batch [ 0 / 93 ] : Loss = 13.159134, Accuracy = 34.38%, MSE = 1.6172
2025-03-01 22:39:34 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 0.00% (86)), (2: 100.00% (88))
2025-03-01 22:41:30 - INFO - Epoch : 12 , Batch [ 10 / 93 ] : Loss = 10.406895, Accuracy = 31.89%, MSE = 1.4982
2025-03-01 22:41:30 - INFO - Classwise accuracy : (0: 100.00% (75)), (1: 0.00% (110)), (2: 0.00% (71))
2025-03-01 22:43:43 - INFO - Epoch : 12 , Batch [ 20 / 93 ] : Loss = 5.893918, Accuracy = 33.02%, MSE = 1.4142
2025-03-01 22:43:43 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 0.00% (91)), (2: 100.00% (84))
2025-03-01 22:45:51 - INFO - Epoch : 12 , Batch [ 30 / 93 ] : Loss = 5.194229, Accuracy = 33.13%, MSE = 1.4501
2025-03-01 22:45:51 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (91)), (2: 0.00% (87))
2025-03-01 22:48:06 - INFO - Epoch : 12 , Batch [ 40 / 93 ] : Loss = 1.708447, Accuracy = 33.46%, MSE = 1.4463
2025-03-01 22:48:06 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 0.00% (84)), (2: 100.00% (84))
2025-03-01 22:50:09 - INFO - Epoch : 12 , Batch [ 50 / 93 ] : Loss = 0.587252, Accuracy = 33.33%, MSE = 1.4675
2025-03-01 22:50:09 - INFO - Classwise accuracy : (0: 13.46% (104)), (1: 0.00% (78)), (2: 86.49% (74))
2025-03-01 22:52:24 - INFO - Epoch : 12 , Batch [ 60 / 93 ] : Loss = 0.482035, Accuracy = 33.31%, MSE = 1.4654
2025-03-01 22:52:24 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 28.92% (83)), (2: 69.66% (89))
2025-03-01 22:54:34 - INFO - Epoch : 12 , Batch [ 70 / 93 ] : Loss = 0.365376, Accuracy = 33.48%, MSE = 1.4648
2025-03-01 22:54:34 - INFO - Classwise accuracy : (0: 40.62% (64)), (1: 3.26% (92)), (2: 64.00% (100))
2025-03-01 22:56:48 - INFO - Epoch : 12 , Batch [ 80 / 93 ] : Loss = 0.373237, Accuracy = 33.62%, MSE = 1.4867
2025-03-01 22:56:48 - INFO - Classwise accuracy : (0: 46.51% (86)), (1: 0.00% (86)), (2: 57.14% (84))
2025-03-01 22:59:00 - INFO - Epoch : 12 , Batch [ 90 / 93 ] : Loss = 0.365900, Accuracy = 33.71%, MSE = 1.5006
2025-03-01 22:59:00 - INFO - Classwise accuracy : (0: 56.00% (75)), (1: 0.00% (95)), (2: 51.16% (86))
2025-03-01 22:59:26 - INFO - Epoch 12: Train Loss=3.3113, Train Acc=33.67%, Train MSE=1.5040
2025-03-01 22:59:42 - INFO - Epoch 12: Val Loss=0.3672, Val Acc=16.19%
2025-03-01 22:59:42 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 22:59:42 - INFO - Updated class weights : tensor([1.0000, 0.5486, 0.8967], device='mps:0')
2025-03-01 22:59:47 - INFO - Epoch : 13 , Batch [ 0 / 93 ] : Loss = 0.384776, Accuracy = 35.94%, MSE = 1.5898
2025-03-01 22:59:47 - INFO - Classwise accuracy : (0: 19.05% (84)), (1: 0.00% (83)), (2: 85.39% (89))
2025-03-01 23:01:59 - INFO - Epoch : 13 , Batch [ 10 / 93 ] : Loss = 0.393408, Accuracy = 33.63%, MSE = 1.6460
2025-03-01 23:01:59 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (83)), (2: 2.41% (83))
2025-03-01 23:04:13 - INFO - Epoch : 13 , Batch [ 20 / 93 ] : Loss = 0.385949, Accuracy = 33.02%, MSE = 1.6782
2025-03-01 23:04:13 - INFO - Classwise accuracy : (0: 3.70% (81)), (1: 0.00% (86)), (2: 98.88% (89))
2025-03-01 23:06:22 - INFO - Epoch : 13 , Batch [ 30 / 93 ] : Loss = 0.380494, Accuracy = 33.06%, MSE = 1.6745
2025-03-01 23:06:22 - INFO - Classwise accuracy : (0: 56.79% (81)), (1: 0.00% (84)), (2: 56.04% (91))
2025-03-01 23:08:35 - INFO - Epoch : 13 , Batch [ 40 / 93 ] : Loss = 0.386091, Accuracy = 33.12%, MSE = 1.6603
2025-03-01 23:08:35 - INFO - Classwise accuracy : (0: 54.12% (85)), (1: 0.00% (86)), (2: 47.06% (85))
2025-03-01 23:10:53 - INFO - Epoch : 13 , Batch [ 50 / 93 ] : Loss = 0.387781, Accuracy = 33.10%, MSE = 1.6670
2025-03-01 23:10:53 - INFO - Classwise accuracy : (0: 98.65% (74)), (1: 0.00% (94)), (2: 1.14% (88))
2025-03-01 23:13:05 - INFO - Epoch : 13 , Batch [ 60 / 93 ] : Loss = 0.387770, Accuracy = 33.30%, MSE = 1.6595
2025-03-01 23:13:05 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (75)), (2: 1.03% (97))
2025-03-01 23:15:03 - INFO - Epoch : 13 , Batch [ 70 / 93 ] : Loss = 0.385511, Accuracy = 33.56%, MSE = 1.6526
2025-03-01 23:15:03 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (87)), (2: 0.00% (73))
2025-03-01 23:17:16 - INFO - Epoch : 13 , Batch [ 80 / 93 ] : Loss = 0.385820, Accuracy = 33.57%, MSE = 1.6519
2025-03-01 23:17:16 - INFO - Classwise accuracy : (0: 74.12% (85)), (1: 0.00% (80)), (2: 17.58% (91))
2025-03-01 23:19:35 - INFO - Epoch : 13 , Batch [ 90 / 93 ] : Loss = 0.384135, Accuracy = 33.47%, MSE = 1.6502
2025-03-01 23:19:35 - INFO - Classwise accuracy : (0: 88.51% (87)), (1: 0.00% (92)), (2: 18.18% (77))
2025-03-01 23:20:01 - INFO - Epoch 13: Train Loss=0.3876, Train Acc=33.48%, Train MSE=1.6499
2025-03-01 23:20:17 - INFO - Epoch 13: Val Loss=0.3694, Val Acc=16.19%
2025-03-01 23:20:17 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 23:20:17 - INFO - Updated class weights : tensor([1.0000, 0.6118, 0.9112], device='mps:0')
2025-03-01 23:20:23 - INFO - Epoch : 14 , Batch [ 0 / 93 ] : Loss = 0.405102, Accuracy = 34.38%, MSE = 1.6758
2025-03-01 23:20:23 - INFO - Classwise accuracy : (0: 17.71% (96)), (1: 0.00% (81)), (2: 89.87% (79))
2025-03-01 23:22:36 - INFO - Epoch : 14 , Batch [ 10 / 93 ] : Loss = 0.405665, Accuracy = 31.61%, MSE = 1.7205
2025-03-01 23:22:36 - INFO - Classwise accuracy : (0: 55.29% (85)), (1: 0.00% (95)), (2: 46.05% (76))
2025-03-01 23:25:13 - INFO - Epoch : 14 , Batch [ 20 / 93 ] : Loss = 0.402723, Accuracy = 31.62%, MSE = 1.6994
2025-03-01 23:25:13 - INFO - Classwise accuracy : (0: 5.19% (77)), (1: 0.00% (95)), (2: 86.90% (84))
2025-03-01 23:27:35 - INFO - Epoch : 14 , Batch [ 30 / 93 ] : Loss = 0.402588, Accuracy = 32.28%, MSE = 1.6941
2025-03-01 23:27:35 - INFO - Classwise accuracy : (0: 75.00% (92)), (1: 0.00% (82)), (2: 17.07% (82))
2025-03-01 23:29:48 - INFO - Epoch : 14 , Batch [ 40 / 93 ] : Loss = 0.401526, Accuracy = 32.24%, MSE = 1.6983
2025-03-01 23:29:48 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 0.00% (94)), (2: 100.00% (88))
2025-03-01 23:32:13 - INFO - Epoch : 14 , Batch [ 50 / 93 ] : Loss = 0.400725, Accuracy = 32.35%, MSE = 1.6961
2025-03-01 23:32:13 - INFO - Classwise accuracy : (0: 9.33% (75)), (1: 0.00% (104)), (2: 89.61% (77))
2025-03-01 23:34:39 - INFO - Epoch : 14 , Batch [ 60 / 93 ] : Loss = 0.411791, Accuracy = 32.02%, MSE = 1.7091
2025-03-01 23:34:39 - INFO - Classwise accuracy : (0: 0.00% (99)), (1: 0.00% (84)), (2: 100.00% (73))
2025-03-01 23:36:56 - INFO - Epoch : 14 , Batch [ 70 / 93 ] : Loss = 0.424166, Accuracy = 32.27%, MSE = 1.7014
2025-03-01 23:36:56 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (82)), (2: 100.00% (83))
2025-03-01 23:39:22 - INFO - Epoch : 14 , Batch [ 80 / 93 ] : Loss = 0.413493, Accuracy = 32.42%, MSE = 1.6795
2025-03-01 23:39:22 - INFO - Classwise accuracy : (0: 82.43% (74)), (1: 0.00% (103)), (2: 25.32% (79))
2025-03-01 23:41:42 - INFO - Epoch : 14 , Batch [ 90 / 93 ] : Loss = 0.411222, Accuracy = 32.54%, MSE = 1.6763
2025-03-01 23:41:42 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (78)), (2: 0.00% (93))
2025-03-01 23:42:07 - INFO - Epoch 14: Train Loss=0.4054, Train Acc=32.53%, Train MSE=1.6813
2025-03-01 23:42:23 - INFO - Epoch 14: Val Loss=0.5242, Val Acc=16.19%
2025-03-01 23:42:23 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-01 23:42:23 - INFO - Updated class weights : tensor([1.0000, 0.6700, 0.9245], device='mps:0')
2025-03-01 23:42:28 - INFO - Epoch : 15 , Batch [ 0 / 93 ] : Loss = 0.447673, Accuracy = 36.72%, MSE = 1.4414
2025-03-01 23:42:28 - INFO - Classwise accuracy : (0: 1.47% (68)), (1: 0.00% (93)), (2: 97.89% (95))
2025-03-01 23:44:41 - INFO - Epoch : 15 , Batch [ 10 / 93 ] : Loss = 0.552541, Accuracy = 33.24%, MSE = 1.3771
2025-03-01 23:44:41 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (87)), (2: 0.00% (81))
2025-03-01 23:47:19 - INFO - Epoch : 15 , Batch [ 20 / 93 ] : Loss = 11.594147, Accuracy = 33.65%, MSE = 1.4046
2025-03-01 23:47:19 - INFO - Classwise accuracy : (0: 0.00% (99)), (1: 0.00% (67)), (2: 100.00% (90))
2025-03-01 23:49:55 - INFO - Epoch : 15 , Batch [ 30 / 93 ] : Loss = 29.172459, Accuracy = 33.82%, MSE = 1.3921
2025-03-01 23:49:55 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (89)), (2: 0.00% (79))
2025-03-01 23:52:23 - INFO - Epoch : 15 , Batch [ 40 / 93 ] : Loss = 10.526525, Accuracy = 33.74%, MSE = 1.4109
2025-03-01 23:52:23 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (92)), (2: 100.00% (79))
2025-03-01 23:54:55 - INFO - Epoch : 15 , Batch [ 50 / 93 ] : Loss = 17.972115, Accuracy = 33.71%, MSE = 1.3835
2025-03-01 23:54:55 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 100.00% (74)), (2: 0.00% (92))
2025-03-01 23:57:27 - INFO - Epoch : 15 , Batch [ 60 / 93 ] : Loss = 15.729314, Accuracy = 33.52%, MSE = 1.3841
2025-03-01 23:57:27 - INFO - Classwise accuracy : (0: 100.00% (99)), (1: 0.00% (70)), (2: 0.00% (87))
2025-03-01 23:59:58 - INFO - Epoch : 15 , Batch [ 70 / 93 ] : Loss = 16.988468, Accuracy = 33.64%, MSE = 1.3695
2025-03-01 23:59:58 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (98)), (2: 100.00% (83))
2025-03-02 00:02:38 - INFO - Epoch : 15 , Batch [ 80 / 93 ] : Loss = 16.564838, Accuracy = 33.71%, MSE = 1.3845
2025-03-02 00:02:38 - INFO - Classwise accuracy : (0: 100.00% (86)), (1: 0.00% (85)), (2: 0.00% (85))
2025-03-02 00:05:16 - INFO - Epoch : 15 , Batch [ 90 / 93 ] : Loss = 6.684773, Accuracy = 33.67%, MSE = 1.3864
2025-03-02 00:05:16 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (96)), (2: 0.00% (83))
2025-03-02 00:05:38 - INFO - Epoch 15: Train Loss=12.5662, Train Acc=33.63%, Train MSE=1.3819
2025-03-02 00:05:54 - INFO - Epoch 15: Val Loss=8.0547, Val Acc=16.19%
2025-03-02 00:05:54 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 00:05:55 - INFO - Updated class weights : tensor([1.0000, 0.7228, 0.9366], device='mps:0')
2025-03-02 00:06:00 - INFO - Epoch : 16 , Batch [ 0 / 93 ] : Loss = 5.146903, Accuracy = 36.33%, MSE = 1.5977
2025-03-02 00:06:00 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 0.00% (81)), (2: 100.00% (93))
2025-03-02 00:07:59 - INFO - Epoch : 16 , Batch [ 10 / 93 ] : Loss = 7.893996, Accuracy = 33.35%, MSE = 1.4336
2025-03-02 00:07:59 - INFO - Classwise accuracy : (0: 100.00% (76)), (1: 0.00% (99)), (2: 0.00% (81))
2025-03-02 00:10:24 - INFO - Epoch : 16 , Batch [ 20 / 93 ] : Loss = 3.429076, Accuracy = 33.17%, MSE = 1.3558
2025-03-02 00:10:24 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (95)), (2: 100.00% (81))
2025-03-02 00:12:52 - INFO - Epoch : 16 , Batch [ 30 / 93 ] : Loss = 0.466280, Accuracy = 32.75%, MSE = 1.3957
2025-03-02 00:12:52 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (87)), (2: 0.00% (79))
2025-03-02 00:15:37 - INFO - Epoch : 16 , Batch [ 40 / 93 ] : Loss = 0.561579, Accuracy = 32.84%, MSE = 1.3942
2025-03-02 00:15:37 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (75)), (2: 98.91% (92))
2025-03-02 00:18:08 - INFO - Epoch : 16 , Batch [ 50 / 93 ] : Loss = 0.531968, Accuracy = 32.99%, MSE = 1.4137
2025-03-02 00:18:08 - INFO - Classwise accuracy : (0: 100.00% (72)), (1: 0.00% (95)), (2: 0.00% (89))
2025-03-02 00:20:52 - INFO - Epoch : 16 , Batch [ 60 / 93 ] : Loss = 0.437321, Accuracy = 33.10%, MSE = 1.4284
2025-03-02 00:20:52 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (91)), (2: 0.00% (77))
2025-03-02 00:23:33 - INFO - Epoch : 16 , Batch [ 70 / 93 ] : Loss = 0.451479, Accuracy = 33.15%, MSE = 1.4172
2025-03-02 00:23:33 - INFO - Classwise accuracy : (0: 100.00% (97)), (1: 1.32% (76)), (2: 0.00% (83))
2025-03-02 00:26:30 - INFO - Epoch : 16 , Batch [ 80 / 93 ] : Loss = 0.429061, Accuracy = 33.30%, MSE = 1.4082
2025-03-02 00:26:30 - INFO - Classwise accuracy : (0: 4.23% (71)), (1: 0.00% (93)), (2: 88.04% (92))
2025-03-02 00:29:16 - INFO - Epoch : 16 , Batch [ 90 / 93 ] : Loss = 0.440640, Accuracy = 33.28%, MSE = 1.4156
2025-03-02 00:29:16 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (84)), (2: 100.00% (81))
2025-03-02 00:29:46 - INFO - Epoch 16: Train Loss=1.7894, Train Acc=33.27%, Train MSE=1.4216
2025-03-02 00:30:03 - INFO - Epoch 16: Val Loss=0.3647, Val Acc=68.00%
2025-03-02 00:30:03 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 00:30:03 - INFO - Updated class weights : tensor([1.0000, 0.7699, 0.9474], device='mps:0')
2025-03-02 00:30:08 - INFO - Epoch : 17 , Batch [ 0 / 93 ] : Loss = 0.446804, Accuracy = 38.67%, MSE = 0.8359
2025-03-02 00:30:08 - INFO - Classwise accuracy : (0: 28.89% (90)), (1: 80.22% (91)), (2: 0.00% (75))
2025-03-02 00:32:18 - INFO - Epoch : 17 , Batch [ 10 / 93 ] : Loss = 0.445238, Accuracy = 34.73%, MSE = 1.3335
2025-03-02 00:32:18 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 47.78% (90)), (2: 50.00% (78))
2025-03-02 00:34:48 - INFO - Epoch : 17 , Batch [ 20 / 93 ] : Loss = 0.452047, Accuracy = 34.67%, MSE = 1.3837
2025-03-02 00:34:48 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 100.00% (79)), (2: 0.00% (97))
2025-03-02 00:37:04 - INFO - Epoch : 17 , Batch [ 30 / 93 ] : Loss = 0.446280, Accuracy = 33.85%, MSE = 1.4546
2025-03-02 00:37:04 - INFO - Classwise accuracy : (0: 79.78% (89)), (1: 12.99% (77)), (2: 0.00% (90))
2025-03-02 00:39:28 - INFO - Epoch : 17 , Batch [ 40 / 93 ] : Loss = 0.441127, Accuracy = 34.12%, MSE = 1.5029
2025-03-02 00:39:28 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (73)), (2: 0.00% (87))
2025-03-02 00:42:25 - INFO - Epoch : 17 , Batch [ 50 / 93 ] : Loss = 0.438946, Accuracy = 33.93%, MSE = 1.5283
2025-03-02 00:42:25 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (88)), (2: 0.00% (79))
2025-03-02 00:45:29 - INFO - Epoch : 17 , Batch [ 60 / 93 ] : Loss = 0.440187, Accuracy = 33.57%, MSE = 1.5561
2025-03-02 00:45:29 - INFO - Classwise accuracy : (0: 85.00% (80)), (1: 0.00% (83)), (2: 9.68% (93))
2025-03-02 00:48:17 - INFO - Epoch : 17 , Batch [ 70 / 93 ] : Loss = 0.442487, Accuracy = 33.31%, MSE = 1.5792
2025-03-02 00:48:17 - INFO - Classwise accuracy : (0: 86.75% (83)), (1: 0.00% (92)), (2: 16.05% (81))
2025-03-02 00:51:08 - INFO - Epoch : 17 , Batch [ 80 / 93 ] : Loss = 0.439927, Accuracy = 33.29%, MSE = 1.5914
2025-03-02 00:51:08 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (78)), (2: 100.00% (89))
2025-03-02 00:53:29 - INFO - Epoch : 17 , Batch [ 90 / 93 ] : Loss = 0.445084, Accuracy = 33.27%, MSE = 1.5976
2025-03-02 00:53:29 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 0.00% (94)), (2: 100.00% (72))
2025-03-02 00:53:55 - INFO - Epoch 17: Train Loss=0.4430, Train Acc=33.25%, Train MSE=1.6045
2025-03-02 00:54:11 - INFO - Epoch 17: Val Loss=0.4427, Val Acc=16.19%
2025-03-02 00:54:11 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 00:54:11 - INFO - Updated class weights : tensor([1.0000, 0.8113, 0.9568], device='mps:0')
2025-03-02 00:54:16 - INFO - Epoch : 18 , Batch [ 0 / 93 ] : Loss = 0.450128, Accuracy = 32.81%, MSE = 1.5625
2025-03-02 00:54:16 - INFO - Classwise accuracy : (0: 4.05% (74)), (1: 0.00% (96)), (2: 94.19% (86))
2025-03-02 00:56:19 - INFO - Epoch : 18 , Batch [ 10 / 93 ] : Loss = 0.457089, Accuracy = 33.38%, MSE = 1.3949
2025-03-02 00:56:19 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 100.00% (87)), (2: 0.00% (85))
2025-03-02 00:58:38 - INFO - Epoch : 18 , Batch [ 20 / 93 ] : Loss = 0.448345, Accuracy = 33.65%, MSE = 1.4481
2025-03-02 00:58:38 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 10.71% (84)), (2: 92.31% (91))
2025-03-02 01:01:05 - INFO - Epoch : 18 , Batch [ 30 / 93 ] : Loss = 0.458624, Accuracy = 33.38%, MSE = 1.4740
2025-03-02 01:01:05 - INFO - Classwise accuracy : (0: 100.00% (79)), (1: 0.00% (85)), (2: 0.00% (92))
2025-03-02 01:03:45 - INFO - Epoch : 18 , Batch [ 40 / 93 ] : Loss = 0.461258, Accuracy = 33.22%, MSE = 1.4952
2025-03-02 01:03:45 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (84)), (2: 100.00% (81))
2025-03-02 01:06:26 - INFO - Epoch : 18 , Batch [ 50 / 93 ] : Loss = 0.446328, Accuracy = 33.11%, MSE = 1.5037
2025-03-02 01:06:26 - INFO - Classwise accuracy : (0: 98.92% (93)), (1: 0.00% (90)), (2: 0.00% (73))
2025-03-02 01:09:14 - INFO - Epoch : 18 , Batch [ 60 / 93 ] : Loss = 0.448843, Accuracy = 33.36%, MSE = 1.4962
2025-03-02 01:09:14 - INFO - Classwise accuracy : (0: 100.00% (92)), (1: 0.00% (81)), (2: 0.00% (83))
2025-03-02 01:11:58 - INFO - Epoch : 18 , Batch [ 70 / 93 ] : Loss = 0.449447, Accuracy = 33.36%, MSE = 1.5194
2025-03-02 01:11:58 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (91)), (2: 100.00% (88))
2025-03-02 01:14:40 - INFO - Epoch : 18 , Batch [ 80 / 93 ] : Loss = 0.866551, Accuracy = 33.32%, MSE = 1.5279
2025-03-02 01:14:40 - INFO - Classwise accuracy : (0: 0.00% (94)), (1: 18.07% (83)), (2: 83.54% (79))
2025-03-02 01:17:21 - INFO - Epoch : 18 , Batch [ 90 / 93 ] : Loss = 14.572321, Accuracy = 33.39%, MSE = 1.5065
2025-03-02 01:17:21 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (81)), (2: 0.00% (96))
2025-03-02 01:17:48 - INFO - Epoch 18: Train Loss=1.6892, Train Acc=33.37%, Train MSE=1.5084
2025-03-02 01:18:05 - INFO - Epoch 18: Val Loss=4.4532, Val Acc=68.00%
2025-03-02 01:18:05 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 01:18:05 - INFO - Updated class weights : tensor([1.0000, 0.8472, 0.9650], device='mps:0')
2025-03-02 01:18:10 - INFO - Epoch : 19 , Batch [ 0 / 93 ] : Loss = 9.500113, Accuracy = 30.47%, MSE = 0.6953
2025-03-02 01:18:10 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (78)), (2: 0.00% (90))
2025-03-02 01:19:57 - INFO - Epoch : 19 , Batch [ 10 / 93 ] : Loss = 25.177935, Accuracy = 32.56%, MSE = 1.2326
2025-03-02 01:19:57 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (86)), (2: 0.00% (91))
2025-03-02 01:21:53 - INFO - Epoch : 19 , Batch [ 20 / 93 ] : Loss = 13.422352, Accuracy = 32.83%, MSE = 1.2883
2025-03-02 01:21:53 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (83)), (2: 0.00% (86))
2025-03-02 01:24:04 - INFO - Epoch : 19 , Batch [ 30 / 93 ] : Loss = 5.891271, Accuracy = 32.55%, MSE = 1.3323
2025-03-02 01:24:04 - INFO - Classwise accuracy : (0: 0.00% (96)), (1: 0.00% (78)), (2: 100.00% (82))
2025-03-02 01:26:30 - INFO - Epoch : 19 , Batch [ 40 / 93 ] : Loss = 1.015077, Accuracy = 32.98%, MSE = 1.3347
2025-03-02 01:26:30 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (96)), (2: 0.00% (81))
2025-03-02 01:28:46 - INFO - Epoch : 19 , Batch [ 50 / 93 ] : Loss = 1.334857, Accuracy = 32.90%, MSE = 1.3397
2025-03-02 01:28:46 - INFO - Classwise accuracy : (0: 0.00% (60)), (1: 0.00% (100)), (2: 100.00% (96))
2025-03-02 01:30:54 - INFO - Epoch : 19 , Batch [ 60 / 93 ] : Loss = 0.578733, Accuracy = 32.95%, MSE = 1.3397
2025-03-02 01:30:54 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (86)), (2: 0.00% (83))
2025-03-02 01:33:00 - INFO - Epoch : 19 , Batch [ 70 / 93 ] : Loss = 0.495308, Accuracy = 32.80%, MSE = 1.3550
2025-03-02 01:33:00 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (78)), (2: 100.00% (85))
2025-03-02 01:35:13 - INFO - Epoch : 19 , Batch [ 80 / 93 ] : Loss = 0.477816, Accuracy = 32.93%, MSE = 1.3509
2025-03-02 01:35:13 - INFO - Classwise accuracy : (0: 96.25% (80)), (1: 0.00% (92)), (2: 10.71% (84))
2025-03-02 01:37:18 - INFO - Epoch : 19 , Batch [ 90 / 93 ] : Loss = 0.487479, Accuracy = 33.02%, MSE = 1.3656
2025-03-02 01:37:18 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 8.00% (75)), (2: 94.85% (97))
2025-03-02 01:37:45 - INFO - Epoch 19: Train Loss=4.6392, Train Acc=32.98%, Train MSE=1.3742
2025-03-02 01:38:01 - INFO - Epoch 19: Val Loss=0.5283, Val Acc=15.81%
2025-03-02 01:38:01 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 01:38:01 - INFO - Updated class weights : tensor([1.0000, 0.8777, 0.9720], device='mps:0')
2025-03-02 01:38:06 - INFO - Epoch : 20 , Batch [ 0 / 93 ] : Loss = 0.485941, Accuracy = 26.56%, MSE = 1.8242
2025-03-02 01:38:06 - INFO - Classwise accuracy : (0: 100.00% (68)), (1: 0.00% (95)), (2: 0.00% (93))
2025-03-02 01:39:59 - INFO - Epoch : 20 , Batch [ 10 / 93 ] : Loss = 0.470320, Accuracy = 31.07%, MSE = 1.4585
2025-03-02 01:39:59 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (83)), (2: 0.00% (90))
2025-03-02 01:42:11 - INFO - Epoch : 20 , Batch [ 20 / 93 ] : Loss = 0.463517, Accuracy = 31.92%, MSE = 1.3895
2025-03-02 01:42:11 - INFO - Classwise accuracy : (0: 51.90% (79)), (1: 0.00% (89)), (2: 47.73% (88))
2025-03-02 01:44:22 - INFO - Epoch : 20 , Batch [ 30 / 93 ] : Loss = 0.463047, Accuracy = 32.36%, MSE = 1.3909
2025-03-02 01:44:22 - INFO - Classwise accuracy : (0: 5.32% (94)), (1: 0.00% (78)), (2: 95.24% (84))
2025-03-02 01:46:31 - INFO - Epoch : 20 , Batch [ 40 / 93 ] : Loss = 0.464053, Accuracy = 32.64%, MSE = 1.4110
2025-03-02 01:46:31 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 92.47% (93)), (2: 8.64% (81))
2025-03-02 01:48:40 - INFO - Epoch : 20 , Batch [ 50 / 93 ] : Loss = 0.463816, Accuracy = 32.52%, MSE = 1.4480
2025-03-02 01:48:40 - INFO - Classwise accuracy : (0: 51.90% (79)), (1: 34.09% (88)), (2: 12.36% (89))
2025-03-02 01:50:56 - INFO - Epoch : 20 , Batch [ 60 / 93 ] : Loss = 0.461897, Accuracy = 32.55%, MSE = 1.4606
2025-03-02 01:50:56 - INFO - Classwise accuracy : (0: 26.67% (75)), (1: 3.26% (92)), (2: 71.91% (89))
2025-03-02 01:53:07 - INFO - Epoch : 20 , Batch [ 70 / 93 ] : Loss = 0.464146, Accuracy = 32.52%, MSE = 1.4773
2025-03-02 01:53:07 - INFO - Classwise accuracy : (0: 1.23% (81)), (1: 0.00% (96)), (2: 98.73% (79))
2025-03-02 01:55:22 - INFO - Epoch : 20 , Batch [ 80 / 93 ] : Loss = 0.460986, Accuracy = 32.74%, MSE = 1.4370
2025-03-02 01:55:22 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 0.00% (82)), (2: 100.00% (102))
2025-03-02 01:57:35 - INFO - Epoch : 20 , Batch [ 90 / 93 ] : Loss = 0.462840, Accuracy = 32.97%, MSE = 1.4423
2025-03-02 01:57:35 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (82)), (2: 0.00% (78))
2025-03-02 01:57:57 - INFO - Epoch 20: Train Loss=0.4676, Train Acc=32.95%, Train MSE=1.4489
2025-03-02 01:58:12 - INFO - Epoch 20: Val Loss=0.4172, Val Acc=68.00%
2025-03-02 01:58:12 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 01:58:12 - INFO - Updated class weights : tensor([1.0000, 0.9034, 0.9779], device='mps:0')
2025-03-02 01:58:16 - INFO - Epoch : 21 , Batch [ 0 / 93 ] : Loss = 0.473402, Accuracy = 32.81%, MSE = 0.6719
2025-03-02 01:58:16 - INFO - Classwise accuracy : (0: 1.09% (92)), (1: 100.00% (83)), (2: 0.00% (81))
2025-03-02 01:59:57 - INFO - Epoch : 21 , Batch [ 10 / 93 ] : Loss = 0.466849, Accuracy = 33.95%, MSE = 1.4691
2025-03-02 01:59:57 - INFO - Classwise accuracy : (0: 82.22% (90)), (1: 3.49% (86)), (2: 21.25% (80))
2025-03-02 02:01:49 - INFO - Epoch : 21 , Batch [ 20 / 93 ] : Loss = 0.471635, Accuracy = 33.85%, MSE = 1.4483
2025-03-02 02:01:49 - INFO - Classwise accuracy : (0: 94.32% (88)), (1: 1.32% (76)), (2: 3.26% (92))
2025-03-02 02:03:49 - INFO - Epoch : 21 , Batch [ 30 / 93 ] : Loss = 0.469972, Accuracy = 32.86%, MSE = 1.5019
2025-03-02 02:03:49 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 63.10% (84)), (2: 26.37% (91))
2025-03-02 02:06:11 - INFO - Epoch : 21 , Batch [ 40 / 93 ] : Loss = 0.468767, Accuracy = 33.30%, MSE = 1.5062
2025-03-02 02:06:11 - INFO - Classwise accuracy : (0: 9.52% (84)), (1: 0.00% (82)), (2: 86.67% (90))
2025-03-02 02:08:56 - INFO - Epoch : 21 , Batch [ 50 / 93 ] : Loss = 0.474648, Accuracy = 33.00%, MSE = 1.4616
2025-03-02 02:08:56 - INFO - Classwise accuracy : (0: 62.50% (80)), (1: 40.54% (74)), (2: 0.00% (102))
2025-03-02 02:11:30 - INFO - Epoch : 21 , Batch [ 60 / 93 ] : Loss = 0.477299, Accuracy = 33.36%, MSE = 1.4324
2025-03-02 02:11:30 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 100.00% (81)), (2: 0.00% (90))
2025-03-02 02:13:38 - INFO - Epoch : 21 , Batch [ 70 / 93 ] : Loss = 0.466240, Accuracy = 33.44%, MSE = 1.4252
2025-03-02 02:13:38 - INFO - Classwise accuracy : (0: 1.18% (85)), (1: 97.96% (98)), (2: 0.00% (73))
2025-03-02 02:15:40 - INFO - Epoch : 21 , Batch [ 80 / 93 ] : Loss = 0.473661, Accuracy = 33.49%, MSE = 1.4376
2025-03-02 02:15:40 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 0.00% (93)), (2: 100.00% (77))
2025-03-02 02:17:38 - INFO - Epoch : 21 , Batch [ 90 / 93 ] : Loss = 0.468138, Accuracy = 33.50%, MSE = 1.4376
2025-03-02 02:17:38 - INFO - Classwise accuracy : (0: 13.75% (80)), (1: 0.00% (78)), (2: 82.65% (98))
2025-03-02 02:18:01 - INFO - Epoch 21: Train Loss=0.4741, Train Acc=33.57%, Train MSE=1.4381
2025-03-02 02:18:17 - INFO - Epoch 21: Val Loss=0.4126, Val Acc=68.00%
2025-03-02 02:18:17 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 02:18:17 - INFO - Updated class weights : tensor([1.0000, 0.9247, 0.9828], device='mps:0')
2025-03-02 02:18:22 - INFO - Epoch : 22 , Batch [ 0 / 93 ] : Loss = 0.474751, Accuracy = 39.06%, MSE = 0.6094
2025-03-02 02:18:22 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (100)), (2: 0.00% (79))
2025-03-02 02:20:11 - INFO - Epoch : 22 , Batch [ 10 / 93 ] : Loss = 20.119177, Accuracy = 32.67%, MSE = 1.1410
2025-03-02 02:20:11 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 100.00% (77)), (2: 0.00% (99))
2025-03-02 02:22:05 - INFO - Epoch : 22 , Batch [ 20 / 93 ] : Loss = 62.953468, Accuracy = 32.48%, MSE = 1.2098
2025-03-02 02:22:05 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (72)), (2: 0.00% (106))
2025-03-02 02:23:57 - INFO - Epoch : 22 , Batch [ 30 / 93 ] : Loss = 41.380070, Accuracy = 32.72%, MSE = 1.3037
2025-03-02 02:23:57 - INFO - Classwise accuracy : (0: 100.00% (77)), (1: 0.00% (90)), (2: 0.00% (89))
2025-03-02 02:25:54 - INFO - Epoch : 22 , Batch [ 40 / 93 ] : Loss = 11.436387, Accuracy = 32.88%, MSE = 1.2920
2025-03-02 02:25:54 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (90)), (2: 0.00% (90))
2025-03-02 02:27:37 - INFO - Epoch : 22 , Batch [ 50 / 93 ] : Loss = 3.931268, Accuracy = 32.90%, MSE = 1.3258
2025-03-02 02:27:37 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (90)), (2: 100.00% (88))
2025-03-02 02:29:23 - INFO - Epoch : 22 , Batch [ 60 / 93 ] : Loss = 0.695352, Accuracy = 32.87%, MSE = 1.3245
2025-03-02 02:29:23 - INFO - Classwise accuracy : (0: 1.12% (89)), (1: 0.00% (99)), (2: 100.00% (68))
2025-03-02 02:31:10 - INFO - Epoch : 22 , Batch [ 70 / 93 ] : Loss = 0.681037, Accuracy = 32.76%, MSE = 1.3204
2025-03-02 02:31:10 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (74)), (2: 0.00% (99))
2025-03-02 02:32:51 - INFO - Epoch : 22 , Batch [ 80 / 93 ] : Loss = 0.475351, Accuracy = 33.01%, MSE = 1.3321
2025-03-02 02:32:51 - INFO - Classwise accuracy : (0: 47.13% (87)), (1: 10.39% (77)), (2: 33.70% (92))
2025-03-02 02:34:35 - INFO - Epoch : 22 , Batch [ 90 / 93 ] : Loss = 0.521809, Accuracy = 32.89%, MSE = 1.3361
2025-03-02 02:34:35 - INFO - Classwise accuracy : (0: 0.00% (96)), (1: 0.00% (81)), (2: 100.00% (79))
2025-03-02 02:34:56 - INFO - Epoch 22: Train Loss=8.6852, Train Acc=32.90%, Train MSE=1.3338
2025-03-02 02:35:12 - INFO - Epoch 22: Val Loss=0.5012, Val Acc=15.81%
2025-03-02 02:35:12 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 02:35:12 - INFO - Updated class weights : tensor([1.0000, 0.9420, 0.9867], device='mps:0')
2025-03-02 02:35:16 - INFO - Epoch : 23 , Batch [ 0 / 93 ] : Loss = 0.493345, Accuracy = 37.50%, MSE = 1.5039
2025-03-02 02:35:16 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (85)), (2: 0.00% (75))
2025-03-02 02:37:07 - INFO - Epoch : 23 , Batch [ 10 / 93 ] : Loss = 0.500552, Accuracy = 35.80%, MSE = 1.2472
2025-03-02 02:37:07 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (92)), (2: 0.00% (78))
2025-03-02 02:39:07 - INFO - Epoch : 23 , Batch [ 20 / 93 ] : Loss = 0.474576, Accuracy = 34.69%, MSE = 1.3211
2025-03-02 02:39:07 - INFO - Classwise accuracy : (0: 91.21% (91)), (1: 0.00% (79)), (2: 10.47% (86))
2025-03-02 02:41:09 - INFO - Epoch : 23 , Batch [ 30 / 93 ] : Loss = 0.492437, Accuracy = 33.71%, MSE = 1.3925
2025-03-02 02:41:09 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (89)), (2: 0.00% (94))
2025-03-02 02:43:01 - INFO - Epoch : 23 , Batch [ 40 / 93 ] : Loss = 0.479611, Accuracy = 33.34%, MSE = 1.4149
2025-03-02 02:43:01 - INFO - Classwise accuracy : (0: 62.82% (78)), (1: 37.63% (93)), (2: 0.00% (85))
2025-03-02 02:44:56 - INFO - Epoch : 23 , Batch [ 50 / 93 ] : Loss = 0.475966, Accuracy = 33.35%, MSE = 1.4340
2025-03-02 02:44:56 - INFO - Classwise accuracy : (0: 6.25% (80)), (1: 11.76% (85)), (2: 95.60% (91))
2025-03-02 02:46:57 - INFO - Epoch : 23 , Batch [ 60 / 93 ] : Loss = 0.478296, Accuracy = 33.24%, MSE = 1.4276
2025-03-02 02:46:57 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 96.51% (86)), (2: 2.30% (87))
2025-03-02 02:49:17 - INFO - Epoch : 23 , Batch [ 70 / 93 ] : Loss = 0.483814, Accuracy = 33.35%, MSE = 1.4166
2025-03-02 02:49:17 - INFO - Classwise accuracy : (0: 87.18% (78)), (1: 0.00% (82)), (2: 15.62% (96))
2025-03-02 02:51:24 - INFO - Epoch : 23 , Batch [ 80 / 93 ] : Loss = 0.486948, Accuracy = 33.20%, MSE = 1.4113
2025-03-02 02:51:24 - INFO - Classwise accuracy : (0: 2.27% (88)), (1: 100.00% (76)), (2: 0.00% (92))
2025-03-02 02:53:30 - INFO - Epoch : 23 , Batch [ 90 / 93 ] : Loss = 0.495493, Accuracy = 33.35%, MSE = 1.3983
2025-03-02 02:53:30 - INFO - Classwise accuracy : (0: 60.71% (84)), (1: 0.00% (83)), (2: 32.58% (89))
2025-03-02 02:53:53 - INFO - Epoch 23: Train Loss=0.4865, Train Acc=33.33%, Train MSE=1.4035
2025-03-02 02:54:09 - INFO - Epoch 23: Val Loss=0.4100, Val Acc=68.00%
2025-03-02 02:54:09 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 02:54:09 - INFO - Updated class weights : tensor([1.0000, 0.9559, 0.9899], device='mps:0')
2025-03-02 02:54:13 - INFO - Epoch : 24 , Batch [ 0 / 93 ] : Loss = 0.487778, Accuracy = 34.38%, MSE = 0.6562
2025-03-02 02:54:13 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (88)), (2: 0.00% (81))
2025-03-02 02:56:01 - INFO - Epoch : 24 , Batch [ 10 / 93 ] : Loss = 0.489797, Accuracy = 32.67%, MSE = 1.1346
2025-03-02 02:56:01 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (79)), (2: 0.00% (85))
2025-03-02 02:58:18 - INFO - Epoch : 24 , Batch [ 20 / 93 ] : Loss = 0.496257, Accuracy = 32.92%, MSE = 1.2031
2025-03-02 02:58:18 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (90)), (2: 100.00% (83))
2025-03-02 03:00:32 - INFO - Epoch : 24 , Batch [ 30 / 93 ] : Loss = 0.479267, Accuracy = 33.04%, MSE = 1.2877
2025-03-02 03:00:32 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (91)), (2: 100.00% (88))
2025-03-02 03:02:42 - INFO - Epoch : 24 , Batch [ 40 / 93 ] : Loss = 0.473825, Accuracy = 33.74%, MSE = 1.2777
2025-03-02 03:02:42 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (80)), (2: 100.00% (101))
2025-03-02 03:05:04 - INFO - Epoch : 24 , Batch [ 50 / 93 ] : Loss = 0.480411, Accuracy = 33.73%, MSE = 1.3189
2025-03-02 03:05:04 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 2.30% (87)), (2: 98.86% (88))
2025-03-02 03:07:14 - INFO - Epoch : 24 , Batch [ 60 / 93 ] : Loss = 0.484568, Accuracy = 33.53%, MSE = 1.3094
2025-03-02 03:07:14 - INFO - Classwise accuracy : (0: 92.50% (80)), (1: 6.52% (92)), (2: 0.00% (84))
2025-03-02 03:09:23 - INFO - Epoch : 24 , Batch [ 70 / 93 ] : Loss = 0.489561, Accuracy = 33.64%, MSE = 1.3238
2025-03-02 03:09:23 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (90)), (2: 100.00% (88))
2025-03-02 03:11:35 - INFO - Epoch : 24 , Batch [ 80 / 93 ] : Loss = 0.483765, Accuracy = 33.66%, MSE = 1.3153
2025-03-02 03:11:35 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (84)), (2: 0.00% (85))
2025-03-02 03:13:43 - INFO - Epoch : 24 , Batch [ 90 / 93 ] : Loss = 0.488562, Accuracy = 33.71%, MSE = 1.3201
2025-03-02 03:13:43 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (83)), (2: 0.00% (86))
2025-03-02 03:14:08 - INFO - Epoch 24: Train Loss=0.4868, Train Acc=33.70%, Train MSE=1.3292
2025-03-02 03:14:24 - INFO - Epoch 24: Val Loss=0.5059, Val Acc=16.19%
2025-03-02 03:14:24 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 03:14:24 - INFO - Updated class weights : tensor([1.0000, 0.9669, 0.9924], device='mps:0')
2025-03-02 03:14:28 - INFO - Epoch : 25 , Batch [ 0 / 93 ] : Loss = 0.480875, Accuracy = 38.28%, MSE = 1.5430
2025-03-02 03:14:28 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (79)), (2: 100.00% (98))
2025-03-02 03:16:21 - INFO - Epoch : 25 , Batch [ 10 / 93 ] : Loss = 0.478108, Accuracy = 33.06%, MSE = 1.5653
2025-03-02 03:16:21 - INFO - Classwise accuracy : (0: 0.00% (70)), (1: 1.08% (93)), (2: 97.85% (93))
2025-03-02 03:18:16 - INFO - Epoch : 25 , Batch [ 20 / 93 ] : Loss = 0.527695, Accuracy = 33.35%, MSE = 1.5521
2025-03-02 03:18:16 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (87)), (2: 100.00% (85))
2025-03-02 03:20:19 - INFO - Epoch : 25 , Batch [ 30 / 93 ] : Loss = 0.709106, Accuracy = 33.30%, MSE = 1.5273
2025-03-02 03:20:19 - INFO - Classwise accuracy : (0: 69.77% (86)), (1: 35.96% (89)), (2: 0.00% (81))
2025-03-02 03:22:00 - INFO - Epoch : 25 , Batch [ 40 / 93 ] : Loss = 36.152222, Accuracy = 33.29%, MSE = 1.4606
2025-03-02 03:22:00 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (86)), (2: 0.00% (82))
2025-03-02 03:24:08 - INFO - Epoch : 25 , Batch [ 50 / 93 ] : Loss = 10.669650, Accuracy = 33.50%, MSE = 1.4589
2025-03-02 03:24:08 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (73)), (2: 100.00% (90))
2025-03-02 03:26:18 - INFO - Epoch : 25 , Batch [ 60 / 93 ] : Loss = 12.570820, Accuracy = 33.29%, MSE = 1.4284
2025-03-02 03:26:18 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 100.00% (76)), (2: 0.00% (90))
2025-03-02 03:28:23 - INFO - Epoch : 25 , Batch [ 70 / 93 ] : Loss = 7.038889, Accuracy = 33.45%, MSE = 1.4154
2025-03-02 03:28:23 - INFO - Classwise accuracy : (0: 0.00% (94)), (1: 0.00% (79)), (2: 100.00% (83))
2025-03-02 03:30:30 - INFO - Epoch : 25 , Batch [ 80 / 93 ] : Loss = 1.692371, Accuracy = 33.25%, MSE = 1.3991
2025-03-02 03:30:30 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (74)), (2: 0.00% (100))
2025-03-02 03:32:42 - INFO - Epoch : 25 , Batch [ 90 / 93 ] : Loss = 3.998311, Accuracy = 33.15%, MSE = 1.4020
2025-03-02 03:32:42 - INFO - Classwise accuracy : (0: 100.00% (67)), (1: 0.00% (98)), (2: 0.00% (91))
2025-03-02 03:33:06 - INFO - Epoch 25: Train Loss=4.7263, Train Acc=33.18%, Train MSE=1.3960
2025-03-02 03:33:22 - INFO - Epoch 25: Val Loss=1.2288, Val Acc=16.19%
2025-03-02 03:33:22 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 03:33:22 - INFO - Updated class weights : tensor([1.0000, 0.9755, 0.9944], device='mps:0')
2025-03-02 03:33:26 - INFO - Epoch : 26 , Batch [ 0 / 93 ] : Loss = 1.918504, Accuracy = 30.86%, MSE = 1.7695
2025-03-02 03:33:26 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 0.00% (85)), (2: 100.00% (79))
2025-03-02 03:35:15 - INFO - Epoch : 26 , Batch [ 10 / 93 ] : Loss = 4.110247, Accuracy = 31.85%, MSE = 1.3409
2025-03-02 03:35:15 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (94)), (2: 100.00% (71))
2025-03-02 03:37:24 - INFO - Epoch : 26 , Batch [ 20 / 93 ] : Loss = 1.290203, Accuracy = 33.15%, MSE = 1.3097
2025-03-02 03:37:24 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 0.00% (71)), (2: 100.00% (95))
2025-03-02 03:39:47 - INFO - Epoch : 26 , Batch [ 30 / 93 ] : Loss = 1.455586, Accuracy = 32.95%, MSE = 1.3362
2025-03-02 03:39:47 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 100.00% (89)), (2: 0.00% (95))
2025-03-02 03:41:52 - INFO - Epoch : 26 , Batch [ 40 / 93 ] : Loss = 0.729502, Accuracy = 33.18%, MSE = 1.3393
2025-03-02 03:41:52 - INFO - Classwise accuracy : (0: 25.88% (85)), (1: 0.00% (79)), (2: 78.26% (92))
2025-03-02 03:43:56 - INFO - Epoch : 26 , Batch [ 50 / 93 ] : Loss = 0.630546, Accuracy = 33.14%, MSE = 1.3179
2025-03-02 03:43:56 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 100.00% (85)), (2: 0.00% (82))
2025-03-02 03:45:59 - INFO - Epoch : 26 , Batch [ 60 / 93 ] : Loss = 0.511819, Accuracy = 33.39%, MSE = 1.3051
2025-03-02 03:45:59 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (88)), (2: 100.00% (90))
2025-03-02 03:48:12 - INFO - Epoch : 26 , Batch [ 70 / 93 ] : Loss = 0.493301, Accuracy = 33.36%, MSE = 1.3119
2025-03-02 03:48:12 - INFO - Classwise accuracy : (0: 0.00% (97)), (1: 0.00% (74)), (2: 100.00% (85))
2025-03-02 03:50:31 - INFO - Epoch : 26 , Batch [ 80 / 93 ] : Loss = 0.500249, Accuracy = 33.21%, MSE = 1.3657
2025-03-02 03:50:31 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 2.47% (81)), (2: 97.67% (86))
2025-03-02 03:52:36 - INFO - Epoch : 26 , Batch [ 90 / 93 ] : Loss = 0.485953, Accuracy = 33.24%, MSE = 1.3776
2025-03-02 03:52:36 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 9.88% (81)), (2: 88.24% (85))
2025-03-02 03:53:00 - INFO - Epoch 26: Train Loss=1.1848, Train Acc=33.21%, Train MSE=1.3935
2025-03-02 03:53:16 - INFO - Epoch 26: Val Loss=0.5200, Val Acc=15.81%
2025-03-02 03:53:16 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 03:53:16 - INFO - Updated class weights : tensor([1.0000, 0.9821, 0.9959], device='mps:0')
2025-03-02 03:53:21 - INFO - Epoch : 27 , Batch [ 0 / 93 ] : Loss = 0.483529, Accuracy = 37.50%, MSE = 1.5625
2025-03-02 03:53:21 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (80)), (2: 0.00% (80))
2025-03-02 03:55:17 - INFO - Epoch : 27 , Batch [ 10 / 93 ] : Loss = 0.494936, Accuracy = 34.09%, MSE = 1.2717
2025-03-02 03:55:17 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (82)), (2: 100.00% (89))
2025-03-02 03:57:08 - INFO - Epoch : 27 , Batch [ 20 / 93 ] : Loss = 0.486716, Accuracy = 33.87%, MSE = 1.3962
2025-03-02 03:57:08 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (89)), (2: 100.00% (87))
2025-03-02 03:59:11 - INFO - Epoch : 27 , Batch [ 30 / 93 ] : Loss = 0.499384, Accuracy = 33.71%, MSE = 1.4008
2025-03-02 03:59:11 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (100)), (2: 100.00% (72))
2025-03-02 04:01:07 - INFO - Epoch : 27 , Batch [ 40 / 93 ] : Loss = 0.485291, Accuracy = 33.51%, MSE = 1.3895
2025-03-02 04:01:07 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (81)), (2: 0.00% (86))
2025-03-02 04:03:18 - INFO - Epoch : 27 , Batch [ 50 / 93 ] : Loss = 0.488141, Accuracy = 33.47%, MSE = 1.4215
2025-03-02 04:03:18 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 0.00% (97)), (2: 0.00% (77))
2025-03-02 04:05:40 - INFO - Epoch : 27 , Batch [ 60 / 93 ] : Loss = 0.484365, Accuracy = 33.73%, MSE = 1.3589
2025-03-02 04:05:40 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 100.00% (95)), (2: 0.00% (90))
2025-03-02 04:08:04 - INFO - Epoch : 27 , Batch [ 70 / 93 ] : Loss = 0.486790, Accuracy = 33.67%, MSE = 1.3899
2025-03-02 04:08:04 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (85)), (2: 0.00% (86))
2025-03-02 04:10:17 - INFO - Epoch : 27 , Batch [ 80 / 93 ] : Loss = 0.484607, Accuracy = 33.87%, MSE = 1.3356
2025-03-02 04:10:17 - INFO - Classwise accuracy : (0: 5.41% (74)), (1: 94.44% (90)), (2: 0.00% (92))
2025-03-02 04:12:30 - INFO - Epoch : 27 , Batch [ 90 / 93 ] : Loss = 0.491305, Accuracy = 33.88%, MSE = 1.3328
2025-03-02 04:12:30 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (78)), (2: 0.00% (87))
2025-03-02 04:12:52 - INFO - Epoch 27: Train Loss=0.4881, Train Acc=33.87%, Train MSE=1.3439
2025-03-02 04:13:08 - INFO - Epoch 27: Val Loss=0.4952, Val Acc=16.19%
2025-03-02 04:13:08 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 04:13:08 - INFO - Updated class weights : tensor([1.0000, 0.9871, 0.9971], device='mps:0')
2025-03-02 04:13:12 - INFO - Epoch : 28 , Batch [ 0 / 93 ] : Loss = 0.486562, Accuracy = 30.47%, MSE = 1.8555
2025-03-02 04:13:12 - INFO - Classwise accuracy : (0: 0.00% (99)), (1: 0.00% (79)), (2: 100.00% (78))
2025-03-02 04:15:17 - INFO - Epoch : 28 , Batch [ 10 / 93 ] : Loss = 0.509524, Accuracy = 33.31%, MSE = 1.3349
2025-03-02 04:15:17 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (95)), (2: 100.00% (78))
2025-03-02 04:17:19 - INFO - Epoch : 28 , Batch [ 20 / 93 ] : Loss = 0.498449, Accuracy = 32.87%, MSE = 1.3984
2025-03-02 04:17:19 - INFO - Classwise accuracy : (0: 97.18% (71)), (1: 0.00% (99)), (2: 0.00% (86))
2025-03-02 04:19:25 - INFO - Epoch : 28 , Batch [ 30 / 93 ] : Loss = 0.481571, Accuracy = 32.69%, MSE = 1.3143
2025-03-02 04:19:25 - INFO - Classwise accuracy : (0: 0.00% (68)), (1: 0.00% (84)), (2: 100.00% (104))
2025-03-02 04:21:57 - INFO - Epoch : 28 , Batch [ 40 / 93 ] : Loss = 0.482334, Accuracy = 33.24%, MSE = 1.2695
2025-03-02 04:21:57 - INFO - Classwise accuracy : (0: 75.26% (97)), (1: 0.00% (72)), (2: 33.33% (87))
2025-03-02 04:24:22 - INFO - Epoch : 28 , Batch [ 50 / 93 ] : Loss = 0.550114, Accuracy = 33.28%, MSE = 1.3048
2025-03-02 04:24:22 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (90)), (2: 0.00% (87))
2025-03-02 04:26:51 - INFO - Epoch : 28 , Batch [ 60 / 93 ] : Loss = 12.829082, Accuracy = 33.49%, MSE = 1.3267
2025-03-02 04:26:51 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (93)), (2: 100.00% (70))
2025-03-02 04:29:17 - INFO - Epoch : 28 , Batch [ 70 / 93 ] : Loss = 1.749233, Accuracy = 33.46%, MSE = 1.3296
2025-03-02 04:29:17 - INFO - Classwise accuracy : (0: 0.00% (94)), (1: 50.00% (74)), (2: 57.95% (88))
2025-03-02 04:31:18 - INFO - Epoch : 28 , Batch [ 80 / 93 ] : Loss = 7.898062, Accuracy = 33.31%, MSE = 1.3233
2025-03-02 04:31:18 - INFO - Classwise accuracy : (0: 100.00% (91)), (1: 0.00% (82)), (2: 0.00% (83))
2025-03-02 04:33:34 - INFO - Epoch : 28 , Batch [ 90 / 93 ] : Loss = 5.743993, Accuracy = 33.19%, MSE = 1.3273
2025-03-02 04:33:34 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (86)), (2: 0.00% (85))
2025-03-02 04:33:59 - INFO - Epoch 28: Train Loss=3.2922, Train Acc=33.21%, Train MSE=1.3358
2025-03-02 04:34:15 - INFO - Epoch 28: Val Loss=4.7520, Val Acc=68.00%
2025-03-02 04:34:15 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 04:34:15 - INFO - Updated class weights : tensor([1.0000, 0.9909, 0.9979], device='mps:0')
2025-03-02 04:34:19 - INFO - Epoch : 29 , Batch [ 0 / 93 ] : Loss = 10.414110, Accuracy = 32.42%, MSE = 0.6758
2025-03-02 04:34:19 - INFO - Classwise accuracy : (0: 0.00% (96)), (1: 100.00% (83)), (2: 0.00% (77))
2025-03-02 04:36:06 - INFO - Epoch : 29 , Batch [ 10 / 93 ] : Loss = 8.257250, Accuracy = 32.53%, MSE = 1.2702
2025-03-02 04:36:06 - INFO - Classwise accuracy : (0: 100.00% (74)), (1: 0.00% (91)), (2: 0.00% (91))
2025-03-02 04:37:54 - INFO - Epoch : 29 , Batch [ 20 / 93 ] : Loss = 8.412100, Accuracy = 32.51%, MSE = 1.2725
2025-03-02 04:37:54 - INFO - Classwise accuracy : (0: 100.00% (72)), (1: 0.00% (102)), (2: 0.00% (82))
2025-03-02 04:39:33 - INFO - Epoch : 29 , Batch [ 30 / 93 ] : Loss = 1.301333, Accuracy = 32.60%, MSE = 1.2936
2025-03-02 04:39:33 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 7.78% (90)), (2: 89.53% (86))
2025-03-02 04:41:27 - INFO - Epoch : 29 , Batch [ 40 / 93 ] : Loss = 0.496306, Accuracy = 32.73%, MSE = 1.3138
2025-03-02 04:41:27 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 90.32% (93)), (2: 10.23% (88))
2025-03-02 04:43:31 - INFO - Epoch : 29 , Batch [ 50 / 93 ] : Loss = 0.507234, Accuracy = 32.54%, MSE = 1.3235
2025-03-02 04:43:31 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 85.88% (85)), (2: 8.25% (97))
2025-03-02 04:45:32 - INFO - Epoch : 29 , Batch [ 60 / 93 ] : Loss = 0.532870, Accuracy = 32.88%, MSE = 1.3288
2025-03-02 04:45:32 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (95)), (2: 0.00% (79))
2025-03-02 04:47:36 - INFO - Epoch : 29 , Batch [ 70 / 93 ] : Loss = 0.498775, Accuracy = 33.08%, MSE = 1.3320
2025-03-02 04:47:36 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (81)), (2: 0.00% (88))
2025-03-02 04:49:51 - INFO - Epoch : 29 , Batch [ 80 / 93 ] : Loss = 0.489355, Accuracy = 33.08%, MSE = 1.3347
2025-03-02 04:49:51 - INFO - Classwise accuracy : (0: 96.77% (93)), (1: 7.59% (79)), (2: 0.00% (84))
2025-03-02 04:51:52 - INFO - Epoch : 29 , Batch [ 90 / 93 ] : Loss = 0.490210, Accuracy = 33.04%, MSE = 1.3623
2025-03-02 04:51:52 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (83)), (2: 0.00% (81))
2025-03-02 04:52:14 - INFO - Epoch 29: Train Loss=2.1666, Train Acc=33.05%, Train MSE=1.3474
2025-03-02 04:52:30 - INFO - Epoch 29: Val Loss=0.4676, Val Acc=68.00%
2025-03-02 04:52:30 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 04:52:30 - INFO - Updated class weights : tensor([1.0000, 0.9936, 0.9985], device='mps:0')
2025-03-02 04:52:34 - INFO - Epoch : 30 , Batch [ 0 / 93 ] : Loss = 0.488092, Accuracy = 32.42%, MSE = 0.6875
2025-03-02 04:52:34 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (83)), (2: 0.00% (85))
2025-03-02 04:54:29 - INFO - Epoch : 30 , Batch [ 10 / 93 ] : Loss = 0.484251, Accuracy = 34.27%, MSE = 1.2369
2025-03-02 04:54:29 - INFO - Classwise accuracy : (0: 15.56% (90)), (1: 90.53% (95)), (2: 0.00% (71))
2025-03-02 04:56:26 - INFO - Epoch : 30 , Batch [ 20 / 93 ] : Loss = 0.487935, Accuracy = 33.54%, MSE = 1.3287
2025-03-02 04:56:26 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 0.00% (84)), (2: 100.00% (90))
2025-03-02 04:58:35 - INFO - Epoch : 30 , Batch [ 30 / 93 ] : Loss = 0.487180, Accuracy = 33.49%, MSE = 1.2794
2025-03-02 04:58:35 - INFO - Classwise accuracy : (0: 1.14% (88)), (1: 1.37% (73)), (2: 98.95% (95))
2025-03-02 05:00:42 - INFO - Epoch : 30 , Batch [ 40 / 93 ] : Loss = 0.489131, Accuracy = 33.64%, MSE = 1.2895
2025-03-02 05:00:42 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 0.00% (90)), (2: 100.00% (85))
2025-03-02 05:02:50 - INFO - Epoch : 30 , Batch [ 50 / 93 ] : Loss = 0.487478, Accuracy = 33.43%, MSE = 1.3107
2025-03-02 05:02:50 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 100.00% (85)), (2: 1.11% (90))
2025-03-02 05:05:01 - INFO - Epoch : 30 , Batch [ 60 / 93 ] : Loss = 0.489909, Accuracy = 33.43%, MSE = 1.3200
2025-03-02 05:05:01 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 76.74% (86)), (2: 28.41% (88))
2025-03-02 05:06:49 - INFO - Epoch : 30 , Batch [ 70 / 93 ] : Loss = 0.486210, Accuracy = 33.35%, MSE = 1.3465
2025-03-02 05:06:49 - INFO - Classwise accuracy : (0: 93.33% (90)), (1: 0.00% (79)), (2: 6.90% (87))
2025-03-02 05:08:32 - INFO - Epoch : 30 , Batch [ 80 / 93 ] : Loss = 0.489447, Accuracy = 33.22%, MSE = 1.3315
2025-03-02 05:08:32 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (82)), (2: 0.00% (90))
2025-03-02 05:10:21 - INFO - Epoch : 30 , Batch [ 90 / 93 ] : Loss = 0.485219, Accuracy = 33.20%, MSE = 1.3530
2025-03-02 05:10:21 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 83.84% (99)), (2: 14.81% (81))
2025-03-02 05:10:45 - INFO - Epoch 30: Train Loss=0.4899, Train Acc=33.17%, Train MSE=1.3388
2025-03-02 05:11:00 - INFO - Epoch 30: Val Loss=0.4810, Val Acc=16.19%
2025-03-02 05:11:00 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 05:11:00 - INFO - Updated class weights : tensor([1.0000, 0.9956, 0.9990], device='mps:0')
2025-03-02 05:11:04 - INFO - Epoch : 31 , Batch [ 0 / 93 ] : Loss = 0.489198, Accuracy = 26.95%, MSE = 1.6914
2025-03-02 05:11:04 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 1.94% (103)), (2: 97.10% (69))
2025-03-02 05:12:50 - INFO - Epoch : 31 , Batch [ 10 / 93 ] : Loss = 0.488633, Accuracy = 32.74%, MSE = 1.4961
2025-03-02 05:12:50 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (86)), (2: 0.00% (81))
2025-03-02 05:14:58 - INFO - Epoch : 31 , Batch [ 20 / 93 ] : Loss = 0.488954, Accuracy = 33.80%, MSE = 1.4639
2025-03-02 05:14:58 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 94.12% (85)), (2: 20.45% (88))
2025-03-02 05:17:18 - INFO - Epoch : 31 , Batch [ 30 / 93 ] : Loss = 0.487450, Accuracy = 33.57%, MSE = 1.3829
2025-03-02 05:17:18 - INFO - Classwise accuracy : (0: 15.56% (90)), (1: 89.74% (78)), (2: 1.14% (88))
2025-03-02 05:19:32 - INFO - Epoch : 31 , Batch [ 40 / 93 ] : Loss = 0.488764, Accuracy = 33.97%, MSE = 1.3282
2025-03-02 05:19:32 - INFO - Classwise accuracy : (0: 30.86% (81)), (1: 10.00% (90)), (2: 68.24% (85))
2025-03-02 05:21:33 - INFO - Epoch : 31 , Batch [ 50 / 93 ] : Loss = 0.485276, Accuracy = 33.91%, MSE = 1.3983
2025-03-02 05:21:33 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (77)), (2: 100.00% (99))
2025-03-02 05:23:35 - INFO - Epoch : 31 , Batch [ 60 / 93 ] : Loss = 0.490031, Accuracy = 33.98%, MSE = 1.3868
2025-03-02 05:23:35 - INFO - Classwise accuracy : (0: 100.00% (86)), (1: 0.00% (83)), (2: 0.00% (87))
2025-03-02 05:25:26 - INFO - Epoch : 31 , Batch [ 70 / 93 ] : Loss = 0.488035, Accuracy = 33.84%, MSE = 1.3726
2025-03-02 05:25:26 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (87)), (2: 0.00% (73))
2025-03-02 05:27:36 - INFO - Epoch : 31 , Batch [ 80 / 93 ] : Loss = 0.490308, Accuracy = 33.69%, MSE = 1.3373
2025-03-02 05:27:36 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 0.00% (84)), (2: 0.00% (90))
2025-03-02 05:29:53 - INFO - Epoch : 31 , Batch [ 90 / 93 ] : Loss = 0.491551, Accuracy = 33.57%, MSE = 1.3681
2025-03-02 05:29:53 - INFO - Classwise accuracy : (0: 100.00% (75)), (1: 0.00% (93)), (2: 0.00% (88))
2025-03-02 05:30:18 - INFO - Epoch 31: Train Loss=0.4902, Train Acc=33.59%, Train MSE=1.3515
2025-03-02 05:30:34 - INFO - Epoch 31: Val Loss=0.3727, Val Acc=68.00%
2025-03-02 05:30:34 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 05:30:34 - INFO - Updated class weights : tensor([1.0000, 0.9970, 0.9993], device='mps:0')
2025-03-02 05:30:39 - INFO - Epoch : 32 , Batch [ 0 / 93 ] : Loss = 0.555662, Accuracy = 38.67%, MSE = 0.6133
2025-03-02 05:30:39 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (99)), (2: 0.00% (78))
2025-03-02 05:32:52 - INFO - Epoch : 32 , Batch [ 10 / 93 ] : Loss = 8.634987, Accuracy = 34.13%, MSE = 1.2692
2025-03-02 05:32:52 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (90)), (2: 0.00% (81))
2025-03-02 05:35:08 - INFO - Epoch : 32 , Batch [ 20 / 93 ] : Loss = 1.068743, Accuracy = 33.48%, MSE = 1.2640
2025-03-02 05:35:08 - INFO - Classwise accuracy : (0: 1.19% (84)), (1: 0.00% (89)), (2: 98.80% (83))
2025-03-02 05:37:17 - INFO - Epoch : 32 , Batch [ 30 / 93 ] : Loss = 3.429011, Accuracy = 33.57%, MSE = 1.2865
2025-03-02 05:37:17 - INFO - Classwise accuracy : (0: 100.00% (91)), (1: 0.00% (85)), (2: 0.00% (80))
2025-03-02 05:39:43 - INFO - Epoch : 32 , Batch [ 40 / 93 ] : Loss = 3.265159, Accuracy = 33.65%, MSE = 1.2780
2025-03-02 05:39:43 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 0.00% (86)), (2: 100.00% (82))
2025-03-02 05:42:04 - INFO - Epoch : 32 , Batch [ 50 / 93 ] : Loss = 4.934155, Accuracy = 33.66%, MSE = 1.2815
2025-03-02 05:42:04 - INFO - Classwise accuracy : (0: 100.00% (93)), (1: 0.00% (92)), (2: 0.00% (71))
2025-03-02 05:44:29 - INFO - Epoch : 32 , Batch [ 60 / 93 ] : Loss = 11.248384, Accuracy = 33.68%, MSE = 1.2776
2025-03-02 05:44:29 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (92)), (2: 0.00% (86))
2025-03-02 05:46:49 - INFO - Epoch : 32 , Batch [ 70 / 93 ] : Loss = 5.023332, Accuracy = 33.60%, MSE = 1.3067
2025-03-02 05:46:49 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (86)), (2: 100.00% (85))
2025-03-02 05:49:16 - INFO - Epoch : 32 , Batch [ 80 / 93 ] : Loss = 2.624705, Accuracy = 33.62%, MSE = 1.2982
2025-03-02 05:49:16 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 0.00% (89)), (2: 100.00% (81))
2025-03-02 05:51:45 - INFO - Epoch : 32 , Batch [ 90 / 93 ] : Loss = 2.035570, Accuracy = 33.49%, MSE = 1.3058
2025-03-02 05:51:45 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (90)), (2: 0.00% (90))
2025-03-02 05:52:12 - INFO - Epoch 32: Train Loss=6.1167, Train Acc=33.52%, Train MSE=1.3032
2025-03-02 05:52:28 - INFO - Epoch 32: Val Loss=4.1155, Val Acc=15.81%
2025-03-02 05:52:28 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 05:52:28 - INFO - Updated class weights : tensor([1.0000, 0.9980, 0.9995], device='mps:0')
2025-03-02 05:52:32 - INFO - Epoch : 33 , Batch [ 0 / 93 ] : Loss = 3.667190, Accuracy = 32.42%, MSE = 1.6953
2025-03-02 05:52:32 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (86)), (2: 0.00% (87))
2025-03-02 05:54:36 - INFO - Epoch : 33 , Batch [ 10 / 93 ] : Loss = 3.621427, Accuracy = 31.96%, MSE = 1.4187
2025-03-02 05:54:36 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (90)), (2: 0.00% (83))
2025-03-02 05:56:45 - INFO - Epoch : 33 , Batch [ 20 / 93 ] : Loss = 1.983313, Accuracy = 32.63%, MSE = 1.2926
2025-03-02 05:56:45 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (84)), (2: 0.00% (89))
2025-03-02 05:58:58 - INFO - Epoch : 33 , Batch [ 30 / 93 ] : Loss = 1.747037, Accuracy = 33.05%, MSE = 1.2981
2025-03-02 05:58:58 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 27.16% (81)), (2: 90.62% (96))
2025-03-02 06:01:07 - INFO - Epoch : 33 , Batch [ 40 / 93 ] : Loss = 1.011864, Accuracy = 33.00%, MSE = 1.3176
2025-03-02 06:01:07 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (75)), (2: 0.00% (90))
2025-03-02 06:02:53 - INFO - Epoch : 33 , Batch [ 50 / 93 ] : Loss = 0.672592, Accuracy = 33.01%, MSE = 1.3057
2025-03-02 06:02:53 - INFO - Classwise accuracy : (0: 98.68% (76)), (1: 0.00% (89)), (2: 1.10% (91))
2025-03-02 06:04:43 - INFO - Epoch : 33 , Batch [ 60 / 93 ] : Loss = 0.631225, Accuracy = 33.38%, MSE = 1.3080
2025-03-02 06:04:43 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 0.00% (75)), (2: 100.00% (89))
2025-03-02 06:06:37 - INFO - Epoch : 33 , Batch [ 70 / 93 ] : Loss = 0.501738, Accuracy = 33.36%, MSE = 1.3380
2025-03-02 06:06:37 - INFO - Classwise accuracy : (0: 100.00% (76)), (1: 0.00% (101)), (2: 0.00% (79))
2025-03-02 06:08:30 - INFO - Epoch : 33 , Batch [ 80 / 93 ] : Loss = 0.505297, Accuracy = 33.15%, MSE = 1.3441
2025-03-02 06:08:30 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (87)), (2: 0.00% (96))
2025-03-02 06:10:42 - INFO - Epoch : 33 , Batch [ 90 / 93 ] : Loss = 0.501617, Accuracy = 33.16%, MSE = 1.3209
2025-03-02 06:10:42 - INFO - Classwise accuracy : (0: 0.00% (98)), (1: 0.00% (86)), (2: 100.00% (72))
2025-03-02 06:11:07 - INFO - Epoch 33: Train Loss=1.3427, Train Acc=33.10%, Train MSE=1.3274
2025-03-02 06:11:22 - INFO - Epoch 33: Val Loss=0.5168, Val Acc=15.81%
2025-03-02 06:11:22 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 06:11:22 - INFO - Updated class weights : tensor([1.0000, 0.9987, 0.9997], device='mps:0')
2025-03-02 06:11:27 - INFO - Epoch : 34 , Batch [ 0 / 93 ] : Loss = 0.489693, Accuracy = 35.16%, MSE = 1.4688
2025-03-02 06:11:27 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (96)), (2: 0.00% (70))
2025-03-02 06:13:20 - INFO - Epoch : 34 , Batch [ 10 / 93 ] : Loss = 0.497822, Accuracy = 32.78%, MSE = 1.1570
2025-03-02 06:13:20 - INFO - Classwise accuracy : (0: 1.14% (88)), (1: 0.00% (83)), (2: 95.29% (85))
2025-03-02 06:15:19 - INFO - Epoch : 34 , Batch [ 20 / 93 ] : Loss = 0.487949, Accuracy = 32.68%, MSE = 1.1815
2025-03-02 06:15:19 - INFO - Classwise accuracy : (0: 98.90% (91)), (1: 0.00% (86)), (2: 1.27% (79))
2025-03-02 06:17:32 - INFO - Epoch : 34 , Batch [ 30 / 93 ] : Loss = 0.489637, Accuracy = 32.96%, MSE = 1.2941
2025-03-02 06:17:32 - INFO - Classwise accuracy : (0: 1.15% (87)), (1: 85.71% (70)), (2: 8.08% (99))
2025-03-02 06:19:45 - INFO - Epoch : 34 , Batch [ 40 / 93 ] : Loss = 0.487143, Accuracy = 33.03%, MSE = 1.3079
2025-03-02 06:19:45 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (94)), (2: 0.00% (79))
2025-03-02 06:21:53 - INFO - Epoch : 34 , Batch [ 50 / 93 ] : Loss = 0.495045, Accuracy = 33.00%, MSE = 1.2753
2025-03-02 06:21:53 - INFO - Classwise accuracy : (0: 0.00% (97)), (1: 0.00% (88)), (2: 100.00% (71))
2025-03-02 06:24:03 - INFO - Epoch : 34 , Batch [ 60 / 93 ] : Loss = 0.486698, Accuracy = 32.94%, MSE = 1.2406
2025-03-02 06:24:03 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 87.21% (86)), (2: 11.11% (99))
2025-03-02 06:26:07 - INFO - Epoch : 34 , Batch [ 70 / 93 ] : Loss = 0.488132, Accuracy = 33.02%, MSE = 1.2625
2025-03-02 06:26:07 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (89)), (2: 0.00% (74))
2025-03-02 06:28:05 - INFO - Epoch : 34 , Batch [ 80 / 93 ] : Loss = 0.490187, Accuracy = 33.22%, MSE = 1.2724
2025-03-02 06:28:05 - INFO - Classwise accuracy : (0: 85.88% (85)), (1: 0.00% (93)), (2: 16.67% (78))
2025-03-02 06:30:09 - INFO - Epoch : 34 , Batch [ 90 / 93 ] : Loss = 0.484291, Accuracy = 33.40%, MSE = 1.2736
2025-03-02 06:30:09 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (82)), (2: 100.00% (99))
2025-03-02 06:30:32 - INFO - Epoch 34: Train Loss=0.4906, Train Acc=33.36%, Train MSE=1.2874
2025-03-02 06:30:48 - INFO - Epoch 34: Val Loss=0.4955, Val Acc=16.19%
2025-03-02 06:30:48 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 06:30:48 - INFO - Updated class weights : tensor([1.0000, 0.9991, 0.9998], device='mps:0')
2025-03-02 06:30:52 - INFO - Epoch : 35 , Batch [ 0 / 93 ] : Loss = 0.487916, Accuracy = 34.38%, MSE = 1.5586
2025-03-02 06:30:52 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (91)), (2: 100.00% (88))
2025-03-02 06:32:42 - INFO - Epoch : 35 , Batch [ 10 / 93 ] : Loss = 0.494367, Accuracy = 34.20%, MSE = 1.5625
2025-03-02 06:32:42 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (78)), (2: 100.00% (94))
2025-03-02 06:34:33 - INFO - Epoch : 35 , Batch [ 20 / 93 ] : Loss = 0.491906, Accuracy = 33.85%, MSE = 1.4795
2025-03-02 06:34:33 - INFO - Classwise accuracy : (0: 4.05% (74)), (1: 0.00% (91)), (2: 93.41% (91))
2025-03-02 06:36:31 - INFO - Epoch : 35 , Batch [ 30 / 93 ] : Loss = 0.501034, Accuracy = 33.10%, MSE = 1.3865
2025-03-02 06:36:31 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (82)), (2: 0.00% (81))
2025-03-02 06:38:37 - INFO - Epoch : 35 , Batch [ 40 / 93 ] : Loss = 0.503165, Accuracy = 33.40%, MSE = 1.3748
2025-03-02 06:38:37 - INFO - Classwise accuracy : (0: 100.00% (86)), (1: 0.00% (90)), (2: 0.00% (80))
2025-03-02 06:40:56 - INFO - Epoch : 35 , Batch [ 50 / 93 ] : Loss = 0.493205, Accuracy = 33.14%, MSE = 1.3747
2025-03-02 06:40:56 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 1.11% (90)), (2: 100.00% (84))
2025-03-02 06:43:01 - INFO - Epoch : 35 , Batch [ 60 / 93 ] : Loss = 0.522187, Accuracy = 33.28%, MSE = 1.3375
2025-03-02 06:43:01 - INFO - Classwise accuracy : (0: 2.67% (75)), (1: 96.70% (91)), (2: 0.00% (90))
2025-03-02 06:44:57 - INFO - Epoch : 35 , Batch [ 70 / 93 ] : Loss = 0.522507, Accuracy = 33.07%, MSE = 1.3432
2025-03-02 06:44:57 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 0.00% (75)), (2: 100.00% (86))
2025-03-02 06:46:58 - INFO - Epoch : 35 , Batch [ 80 / 93 ] : Loss = 0.630664, Accuracy = 32.94%, MSE = 1.3332
2025-03-02 06:46:58 - INFO - Classwise accuracy : (0: 98.88% (89)), (1: 0.00% (83)), (2: 0.00% (84))
2025-03-02 06:49:03 - INFO - Epoch : 35 , Batch [ 90 / 93 ] : Loss = 10.765697, Accuracy = 33.01%, MSE = 1.3337
2025-03-02 06:49:03 - INFO - Classwise accuracy : (0: 0.00% (68)), (1: 0.00% (85)), (2: 100.00% (103))
2025-03-02 06:49:24 - INFO - Epoch 35: Train Loss=1.2987, Train Acc=33.02%, Train MSE=1.3302
2025-03-02 06:49:40 - INFO - Epoch 35: Val Loss=10.0066, Val Acc=16.19%
2025-03-02 06:49:40 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 06:49:40 - INFO - Updated class weights : tensor([1.0000, 0.9994, 0.9999], device='mps:0')
2025-03-02 06:49:44 - INFO - Epoch : 36 , Batch [ 0 / 93 ] : Loss = 7.159361, Accuracy = 32.42%, MSE = 1.5898
2025-03-02 06:49:44 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (95)), (2: 100.00% (83))
2025-03-02 06:51:42 - INFO - Epoch : 36 , Batch [ 10 / 93 ] : Loss = 0.927081, Accuracy = 32.21%, MSE = 1.4460
2025-03-02 06:51:42 - INFO - Classwise accuracy : (0: 23.29% (73)), (1: 0.00% (96)), (2: 80.46% (87))
2025-03-02 06:53:54 - INFO - Epoch : 36 , Batch [ 20 / 93 ] : Loss = 13.484270, Accuracy = 32.96%, MSE = 1.3545
2025-03-02 06:53:54 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (86)), (2: 0.00% (85))
2025-03-02 06:55:57 - INFO - Epoch : 36 , Batch [ 30 / 93 ] : Loss = 7.456178, Accuracy = 33.08%, MSE = 1.3270
2025-03-02 06:55:57 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (76)), (2: 0.00% (90))
2025-03-02 06:58:01 - INFO - Epoch : 36 , Batch [ 40 / 93 ] : Loss = 2.591934, Accuracy = 33.43%, MSE = 1.2951
2025-03-02 06:58:01 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (77)), (2: 0.00% (86))
2025-03-02 07:00:10 - INFO - Epoch : 36 , Batch [ 50 / 93 ] : Loss = 1.020991, Accuracy = 33.64%, MSE = 1.3295
2025-03-02 07:00:10 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (84)), (2: 100.00% (92))
2025-03-02 07:02:27 - INFO - Epoch : 36 , Batch [ 60 / 93 ] : Loss = 0.508911, Accuracy = 33.64%, MSE = 1.2879
2025-03-02 07:02:27 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (87)), (2: 1.22% (82))
2025-03-02 07:04:44 - INFO - Epoch : 36 , Batch [ 70 / 93 ] : Loss = 0.540556, Accuracy = 33.84%, MSE = 1.2806
2025-03-02 07:04:44 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (79)), (2: 100.00% (97))
2025-03-02 07:06:56 - INFO - Epoch : 36 , Batch [ 80 / 93 ] : Loss = 0.524968, Accuracy = 33.87%, MSE = 1.2756
2025-03-02 07:06:56 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (84)), (2: 100.00% (87))
2025-03-02 07:08:56 - INFO - Epoch : 36 , Batch [ 90 / 93 ] : Loss = 0.490345, Accuracy = 33.92%, MSE = 1.2515
2025-03-02 07:08:56 - INFO - Classwise accuracy : (0: 3.30% (91)), (1: 7.89% (76)), (2: 82.02% (89))
2025-03-02 07:09:19 - INFO - Epoch 36: Train Loss=2.9988, Train Acc=33.95%, Train MSE=1.2603
2025-03-02 07:09:35 - INFO - Epoch 36: Val Loss=0.6480, Val Acc=15.81%
2025-03-02 07:09:35 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 07:09:35 - INFO - Updated class weights : tensor([1.0000, 0.9997, 0.9999], device='mps:0')
2025-03-02 07:09:39 - INFO - Epoch : 37 , Batch [ 0 / 93 ] : Loss = 0.515037, Accuracy = 34.77%, MSE = 1.6602
2025-03-02 07:09:39 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (81)), (2: 0.00% (86))
2025-03-02 07:11:29 - INFO - Epoch : 37 , Batch [ 10 / 93 ] : Loss = 0.512157, Accuracy = 32.42%, MSE = 1.1967
2025-03-02 07:11:29 - INFO - Classwise accuracy : (0: 100.00% (73)), (1: 0.00% (81)), (2: 0.00% (102))
2025-03-02 07:13:44 - INFO - Epoch : 37 , Batch [ 20 / 93 ] : Loss = 0.487489, Accuracy = 33.37%, MSE = 1.1975
2025-03-02 07:13:44 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (95)), (2: 0.00% (85))
2025-03-02 07:15:55 - INFO - Epoch : 37 , Batch [ 30 / 93 ] : Loss = 0.488698, Accuracy = 33.18%, MSE = 1.2235
2025-03-02 07:15:55 - INFO - Classwise accuracy : (0: 3.53% (85)), (1: 85.71% (84)), (2: 8.05% (87))
2025-03-02 07:17:57 - INFO - Epoch : 37 , Batch [ 40 / 93 ] : Loss = 0.494081, Accuracy = 33.15%, MSE = 1.2147
2025-03-02 07:17:57 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 1.27% (79)), (2: 0.00% (87))
2025-03-02 07:20:00 - INFO - Epoch : 37 , Batch [ 50 / 93 ] : Loss = 0.491712, Accuracy = 32.86%, MSE = 1.2622
2025-03-02 07:20:00 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (86)), (2: 100.00% (87))
2025-03-02 07:22:08 - INFO - Epoch : 37 , Batch [ 60 / 93 ] : Loss = 0.489254, Accuracy = 32.95%, MSE = 1.3150
2025-03-02 07:22:08 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (93)), (2: 98.80% (83))
2025-03-02 07:24:24 - INFO - Epoch : 37 , Batch [ 70 / 93 ] : Loss = 0.494464, Accuracy = 33.09%, MSE = 1.2920
2025-03-02 07:24:24 - INFO - Classwise accuracy : (0: 0.00% (97)), (1: 0.00% (81)), (2: 100.00% (78))
2025-03-02 07:26:35 - INFO - Epoch : 37 , Batch [ 80 / 93 ] : Loss = 0.494295, Accuracy = 33.00%, MSE = 1.2976
2025-03-02 07:26:35 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (95)), (2: 100.00% (83))
2025-03-02 07:28:42 - INFO - Epoch : 37 , Batch [ 90 / 93 ] : Loss = 0.490870, Accuracy = 33.05%, MSE = 1.2854
2025-03-02 07:28:42 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 9.78% (92)), (2: 97.37% (76))
2025-03-02 07:29:07 - INFO - Epoch 37: Train Loss=0.4936, Train Acc=33.01%, Train MSE=1.2981
2025-03-02 07:29:23 - INFO - Epoch 37: Val Loss=0.5049, Val Acc=15.81%
2025-03-02 07:29:23 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 07:29:23 - INFO - Updated class weights : tensor([1.0000, 0.9998, 1.0000], device='mps:0')
2025-03-02 07:29:28 - INFO - Epoch : 38 , Batch [ 0 / 93 ] : Loss = 0.489014, Accuracy = 35.55%, MSE = 1.4766
2025-03-02 07:29:28 - INFO - Classwise accuracy : (0: 80.68% (88)), (1: 0.00% (93)), (2: 26.67% (75))
2025-03-02 07:31:20 - INFO - Epoch : 38 , Batch [ 10 / 93 ] : Loss = 0.489051, Accuracy = 34.02%, MSE = 1.1776
2025-03-02 07:31:21 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 3.85% (78)), (2: 87.64% (89))
2025-03-02 07:33:44 - INFO - Epoch : 38 , Batch [ 20 / 93 ] : Loss = 0.490925, Accuracy = 33.67%, MSE = 1.2269
2025-03-02 07:33:44 - INFO - Classwise accuracy : (0: 6.02% (83)), (1: 96.34% (82)), (2: 0.00% (91))
2025-03-02 07:36:05 - INFO - Epoch : 38 , Batch [ 30 / 93 ] : Loss = 0.486858, Accuracy = 33.64%, MSE = 1.3198
2025-03-02 07:36:05 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 98.88% (89)), (2: 0.00% (96))
2025-03-02 07:38:23 - INFO - Epoch : 38 , Batch [ 40 / 93 ] : Loss = 0.489378, Accuracy = 33.82%, MSE = 1.2869
2025-03-02 07:38:23 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (92)), (2: 0.00% (88))
2025-03-02 07:40:56 - INFO - Epoch : 38 , Batch [ 50 / 93 ] : Loss = 0.491559, Accuracy = 33.83%, MSE = 1.2495
2025-03-02 07:40:56 - INFO - Classwise accuracy : (0: 0.00% (73)), (1: 0.00% (96)), (2: 100.00% (87))
2025-03-02 07:43:20 - INFO - Epoch : 38 , Batch [ 60 / 93 ] : Loss = 0.488390, Accuracy = 33.91%, MSE = 1.2730
2025-03-02 07:43:20 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (79)), (2: 100.00% (94))
2025-03-02 07:45:44 - INFO - Epoch : 38 , Batch [ 70 / 93 ] : Loss = 0.497400, Accuracy = 33.87%, MSE = 1.2721
2025-03-02 07:45:44 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (87)), (2: 100.00% (92))
2025-03-02 07:48:22 - INFO - Epoch : 38 , Batch [ 80 / 93 ] : Loss = 0.499437, Accuracy = 33.82%, MSE = 1.2722
2025-03-02 07:48:22 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (72)), (2: 100.00% (91))
2025-03-02 07:50:51 - INFO - Epoch : 38 , Batch [ 90 / 93 ] : Loss = 0.496717, Accuracy = 33.63%, MSE = 1.2983
2025-03-02 07:50:51 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 92.68% (82)), (2: 1.12% (89))
2025-03-02 07:51:16 - INFO - Epoch 38: Train Loss=0.4920, Train Acc=33.67%, Train MSE=1.3008
2025-03-02 07:51:32 - INFO - Epoch 38: Val Loss=0.5599, Val Acc=16.19%
2025-03-02 07:51:32 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 07:51:32 - INFO - Updated class weights : tensor([1.0000, 0.9999, 1.0000], device='mps:0')
2025-03-02 07:51:36 - INFO - Epoch : 39 , Batch [ 0 / 93 ] : Loss = 0.506377, Accuracy = 28.52%, MSE = 1.7344
2025-03-02 07:51:36 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 0.00% (95)), (2: 98.65% (74))
2025-03-02 07:53:29 - INFO - Epoch : 39 , Batch [ 10 / 93 ] : Loss = 1.263291, Accuracy = 33.56%, MSE = 1.3121
2025-03-02 07:53:29 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (84)), (2: 0.00% (90))
2025-03-02 07:55:48 - INFO - Epoch : 39 , Batch [ 20 / 93 ] : Loss = 4.497347, Accuracy = 33.69%, MSE = 1.3238
2025-03-02 07:55:48 - INFO - Classwise accuracy : (0: 100.00% (74)), (1: 0.00% (85)), (2: 0.00% (97))
2025-03-02 07:57:47 - INFO - Epoch : 39 , Batch [ 30 / 93 ] : Loss = 0.854116, Accuracy = 34.05%, MSE = 1.3335
2025-03-02 07:57:47 - INFO - Classwise accuracy : (0: 1.18% (85)), (1: 0.00% (78)), (2: 90.32% (93))
2025-03-02 08:00:07 - INFO - Epoch : 39 , Batch [ 40 / 93 ] : Loss = 0.688765, Accuracy = 33.70%, MSE = 1.3338
2025-03-02 08:00:07 - INFO - Classwise accuracy : (0: 100.00% (92)), (1: 0.00% (81)), (2: 0.00% (83))
2025-03-02 08:02:36 - INFO - Epoch : 39 , Batch [ 50 / 93 ] : Loss = 0.571489, Accuracy = 33.61%, MSE = 1.3413
2025-03-02 08:02:36 - INFO - Classwise accuracy : (0: 1.25% (80)), (1: 0.00% (92)), (2: 98.81% (84))
2025-03-02 08:04:52 - INFO - Epoch : 39 , Batch [ 60 / 93 ] : Loss = 0.524549, Accuracy = 33.60%, MSE = 1.3356
2025-03-02 08:04:52 - INFO - Classwise accuracy : (0: 25.32% (79)), (1: 77.01% (87)), (2: 0.00% (90))
2025-03-02 08:06:58 - INFO - Epoch : 39 , Batch [ 70 / 93 ] : Loss = 0.501743, Accuracy = 33.62%, MSE = 1.3432
2025-03-02 08:06:58 - INFO - Classwise accuracy : (0: 6.41% (78)), (1: 0.00% (89)), (2: 93.26% (89))
2025-03-02 08:09:05 - INFO - Epoch : 39 , Batch [ 80 / 93 ] : Loss = 0.512432, Accuracy = 33.42%, MSE = 1.3194
2025-03-02 08:09:05 - INFO - Classwise accuracy : (0: 1.06% (94)), (1: 100.00% (75)), (2: 1.15% (87))
2025-03-02 08:11:14 - INFO - Epoch : 39 , Batch [ 90 / 93 ] : Loss = 0.509753, Accuracy = 33.21%, MSE = 1.3279
2025-03-02 08:11:14 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 93.33% (75)), (2: 3.30% (91))
2025-03-02 08:11:38 - INFO - Epoch 39: Train Loss=1.5198, Train Acc=33.15%, Train MSE=1.3334
2025-03-02 08:11:53 - INFO - Epoch 39: Val Loss=0.4440, Val Acc=68.00%
2025-03-02 08:11:53 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 08:11:53 - INFO - Updated class weights : tensor([1.0000, 0.9999, 1.0000], device='mps:0')
2025-03-02 08:11:58 - INFO - Epoch : 40 , Batch [ 0 / 93 ] : Loss = 0.495129, Accuracy = 32.03%, MSE = 0.9023
2025-03-02 08:11:58 - INFO - Classwise accuracy : (0: 8.43% (83)), (1: 88.24% (85)), (2: 0.00% (88))
2025-03-02 08:13:54 - INFO - Epoch : 40 , Batch [ 10 / 93 ] : Loss = 0.531481, Accuracy = 32.42%, MSE = 1.2724
2025-03-02 08:13:55 - INFO - Classwise accuracy : (0: 100.00% (86)), (1: 0.00% (82)), (2: 0.00% (88))
2025-03-02 08:16:00 - INFO - Epoch : 40 , Batch [ 20 / 93 ] : Loss = 0.505122, Accuracy = 33.52%, MSE = 1.2719
2025-03-02 08:16:00 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 95.79% (95)), (2: 1.37% (73))
2025-03-02 08:17:59 - INFO - Epoch : 40 , Batch [ 30 / 93 ] : Loss = 0.490970, Accuracy = 33.30%, MSE = 1.2922
2025-03-02 08:17:59 - INFO - Classwise accuracy : (0: 97.73% (88)), (1: 1.02% (98)), (2: 2.86% (70))
2025-03-02 08:20:04 - INFO - Epoch : 40 , Batch [ 40 / 93 ] : Loss = 0.488337, Accuracy = 33.50%, MSE = 1.3610
2025-03-02 08:20:04 - INFO - Classwise accuracy : (0: 48.00% (75)), (1: 55.06% (89)), (2: 5.43% (92))
2025-03-02 08:22:10 - INFO - Epoch : 40 , Batch [ 50 / 93 ] : Loss = 0.491680, Accuracy = 33.48%, MSE = 1.3566
2025-03-02 08:22:10 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 62.63% (99)), (2: 43.42% (76))
2025-03-02 08:24:01 - INFO - Epoch : 40 , Batch [ 60 / 93 ] : Loss = 0.495507, Accuracy = 33.50%, MSE = 1.3228
2025-03-02 08:24:01 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (83)), (2: 100.00% (84))
2025-03-02 08:26:07 - INFO - Epoch : 40 , Batch [ 70 / 93 ] : Loss = 0.494247, Accuracy = 33.28%, MSE = 1.3246
2025-03-02 08:26:07 - INFO - Classwise accuracy : (0: 2.60% (77)), (1: 100.00% (82)), (2: 0.00% (97))
2025-03-02 08:28:08 - INFO - Epoch : 40 , Batch [ 80 / 93 ] : Loss = 0.508655, Accuracy = 33.13%, MSE = 1.3093
2025-03-02 08:28:08 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (95)), (2: 100.00% (82))
2025-03-02 08:30:19 - INFO - Epoch : 40 , Batch [ 90 / 93 ] : Loss = 0.491600, Accuracy = 33.22%, MSE = 1.3130
2025-03-02 08:30:19 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (73)), (2: 0.00% (87))
2025-03-02 08:30:42 - INFO - Epoch 40: Train Loss=0.4985, Train Acc=33.23%, Train MSE=1.3176
2025-03-02 08:30:57 - INFO - Epoch 40: Val Loss=0.3839, Val Acc=68.00%
2025-03-02 08:30:57 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 08:30:57 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 08:31:02 - INFO - Epoch : 41 , Batch [ 0 / 93 ] : Loss = 0.537635, Accuracy = 31.64%, MSE = 0.6836
2025-03-02 08:31:02 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (81)), (2: 0.00% (97))
2025-03-02 08:33:00 - INFO - Epoch : 41 , Batch [ 10 / 93 ] : Loss = 0.594600, Accuracy = 33.81%, MSE = 1.2095
2025-03-02 08:33:00 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 100.00% (94)), (2: 0.00% (88))
2025-03-02 08:34:56 - INFO - Epoch : 41 , Batch [ 20 / 93 ] : Loss = 0.562459, Accuracy = 33.43%, MSE = 1.2852
2025-03-02 08:34:56 - INFO - Classwise accuracy : (0: 100.00% (80)), (1: 0.00% (78)), (2: 0.00% (98))
2025-03-02 08:37:12 - INFO - Epoch : 41 , Batch [ 30 / 93 ] : Loss = 10.128313, Accuracy = 33.01%, MSE = 1.2603
2025-03-02 08:37:12 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 100.00% (84)), (2: 0.00% (91))
2025-03-02 08:39:28 - INFO - Epoch : 41 , Batch [ 40 / 93 ] : Loss = 5.076947, Accuracy = 32.99%, MSE = 1.2843
2025-03-02 08:39:28 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (90)), (2: 100.00% (81))
2025-03-02 08:41:49 - INFO - Epoch : 41 , Batch [ 50 / 93 ] : Loss = 0.750921, Accuracy = 33.18%, MSE = 1.3081
2025-03-02 08:41:49 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 11.39% (79)), (2: 95.35% (86))
2025-03-02 08:43:47 - INFO - Epoch : 41 , Batch [ 60 / 93 ] : Loss = 0.862369, Accuracy = 33.10%, MSE = 1.3204
2025-03-02 08:43:47 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 100.00% (87)), (2: 0.00% (84))
2025-03-02 08:46:13 - INFO - Epoch : 41 , Batch [ 70 / 93 ] : Loss = 1.020258, Accuracy = 33.25%, MSE = 1.3089
2025-03-02 08:46:13 - INFO - Classwise accuracy : (0: 100.00% (91)), (1: 0.00% (85)), (2: 0.00% (80))
2025-03-02 08:48:17 - INFO - Epoch : 41 , Batch [ 80 / 93 ] : Loss = 0.621004, Accuracy = 33.30%, MSE = 1.3190
2025-03-02 08:48:17 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (75)), (2: 0.00% (98))
2025-03-02 08:50:25 - INFO - Epoch : 41 , Batch [ 90 / 93 ] : Loss = 0.541691, Accuracy = 33.21%, MSE = 1.3074
2025-03-02 08:50:25 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (87)), (2: 0.00% (92))
2025-03-02 08:50:50 - INFO - Epoch 41: Train Loss=1.4805, Train Acc=33.21%, Train MSE=1.3045
2025-03-02 08:51:06 - INFO - Epoch 41: Val Loss=0.6190, Val Acc=15.81%
2025-03-02 08:51:06 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 08:51:06 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 08:51:10 - INFO - Epoch : 42 , Batch [ 0 / 93 ] : Loss = 0.512139, Accuracy = 32.42%, MSE = 1.6719
2025-03-02 08:51:10 - INFO - Classwise accuracy : (0: 98.78% (82)), (1: 0.00% (88)), (2: 2.33% (86))
2025-03-02 08:52:55 - INFO - Epoch : 42 , Batch [ 10 / 93 ] : Loss = 0.531883, Accuracy = 32.81%, MSE = 1.4134
2025-03-02 08:52:55 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (91)), (2: 0.00% (82))
2025-03-02 08:54:54 - INFO - Epoch : 42 , Batch [ 20 / 93 ] : Loss = 0.499041, Accuracy = 32.35%, MSE = 1.3830
2025-03-02 08:54:54 - INFO - Classwise accuracy : (0: 8.51% (94)), (1: 0.00% (83)), (2: 88.61% (79))
2025-03-02 08:57:18 - INFO - Epoch : 42 , Batch [ 30 / 93 ] : Loss = 0.492588, Accuracy = 32.59%, MSE = 1.3542
2025-03-02 08:57:18 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (80)), (2: 0.00% (87))
2025-03-02 08:59:32 - INFO - Epoch : 42 , Batch [ 40 / 93 ] : Loss = 0.488861, Accuracy = 32.84%, MSE = 1.3670
2025-03-02 08:59:32 - INFO - Classwise accuracy : (0: 65.43% (81)), (1: 38.46% (91)), (2: 2.38% (84))
2025-03-02 09:01:55 - INFO - Epoch : 42 , Batch [ 50 / 93 ] : Loss = 0.491919, Accuracy = 32.95%, MSE = 1.3736
2025-03-02 09:01:55 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (83)), (2: 0.00% (82))
2025-03-02 09:04:06 - INFO - Epoch : 42 , Batch [ 60 / 93 ] : Loss = 0.489517, Accuracy = 33.38%, MSE = 1.3452
2025-03-02 09:04:06 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 0.00% (86)), (2: 100.00% (89))
2025-03-02 09:06:07 - INFO - Epoch : 42 , Batch [ 70 / 93 ] : Loss = 0.492283, Accuracy = 33.54%, MSE = 1.3051
2025-03-02 09:06:07 - INFO - Classwise accuracy : (0: 0.00% (69)), (1: 0.00% (93)), (2: 100.00% (94))
2025-03-02 09:08:15 - INFO - Epoch : 42 , Batch [ 80 / 93 ] : Loss = 0.499208, Accuracy = 33.52%, MSE = 1.3034
2025-03-02 09:08:15 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (95)), (2: 100.00% (70))
2025-03-02 09:10:21 - INFO - Epoch : 42 , Batch [ 90 / 93 ] : Loss = 0.497000, Accuracy = 33.66%, MSE = 1.3007
2025-03-02 09:10:21 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (87)), (2: 100.00% (86))
2025-03-02 09:10:44 - INFO - Epoch 42: Train Loss=0.4958, Train Acc=33.72%, Train MSE=1.2861
2025-03-02 09:11:00 - INFO - Epoch 42: Val Loss=0.4422, Val Acc=68.00%
2025-03-02 09:11:00 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 09:11:00 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 09:11:05 - INFO - Epoch : 43 , Batch [ 0 / 93 ] : Loss = 0.498422, Accuracy = 32.81%, MSE = 0.7656
2025-03-02 09:11:05 - INFO - Classwise accuracy : (0: 6.98% (86)), (1: 84.78% (92)), (2: 0.00% (78))
2025-03-02 09:12:56 - INFO - Epoch : 43 , Batch [ 10 / 93 ] : Loss = 0.493270, Accuracy = 32.78%, MSE = 1.3413
2025-03-02 09:12:56 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (86)), (2: 100.00% (92))
2025-03-02 09:14:57 - INFO - Epoch : 43 , Batch [ 20 / 93 ] : Loss = 0.492227, Accuracy = 32.85%, MSE = 1.3439
2025-03-02 09:14:57 - INFO - Classwise accuracy : (0: 4.11% (73)), (1: 89.66% (87)), (2: 0.00% (96))
2025-03-02 09:16:59 - INFO - Epoch : 43 , Batch [ 30 / 93 ] : Loss = 0.505700, Accuracy = 32.67%, MSE = 1.2785
2025-03-02 09:16:59 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 100.00% (78)), (2: 0.00% (89))
2025-03-02 09:18:47 - INFO - Epoch : 43 , Batch [ 40 / 93 ] : Loss = 0.499814, Accuracy = 32.90%, MSE = 1.2778
2025-03-02 09:18:47 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 0.00% (81)), (2: 100.00% (87))
2025-03-02 09:20:51 - INFO - Epoch : 43 , Batch [ 50 / 93 ] : Loss = 0.517851, Accuracy = 32.87%, MSE = 1.2982
2025-03-02 09:20:51 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (78)), (2: 0.00% (95))
2025-03-02 09:22:50 - INFO - Epoch : 43 , Batch [ 60 / 93 ] : Loss = 0.484036, Accuracy = 32.95%, MSE = 1.3015
2025-03-02 09:22:50 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 100.00% (102)), (2: 0.00% (74))
2025-03-02 09:24:59 - INFO - Epoch : 43 , Batch [ 70 / 93 ] : Loss = 0.491919, Accuracy = 33.30%, MSE = 1.2748
2025-03-02 09:24:59 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 100.00% (91)), (2: 0.00% (90))
2025-03-02 09:26:55 - INFO - Epoch : 43 , Batch [ 80 / 93 ] : Loss = 0.500009, Accuracy = 33.04%, MSE = 1.2794
2025-03-02 09:26:55 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (76)), (2: 0.00% (89))
2025-03-02 09:29:03 - INFO - Epoch : 43 , Batch [ 90 / 93 ] : Loss = 0.501556, Accuracy = 33.01%, MSE = 1.2767
2025-03-02 09:29:03 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (85)), (2: 0.00% (82))
2025-03-02 09:29:27 - INFO - Epoch 43: Train Loss=0.4954, Train Acc=33.04%, Train MSE=1.2828
2025-03-02 09:29:43 - INFO - Epoch 43: Val Loss=0.4168, Val Acc=68.00%
2025-03-02 09:29:43 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 09:29:43 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 09:29:47 - INFO - Epoch : 44 , Batch [ 0 / 93 ] : Loss = 0.506558, Accuracy = 32.03%, MSE = 0.6797
2025-03-02 09:29:47 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (82)), (2: 0.00% (91))
2025-03-02 09:31:37 - INFO - Epoch : 44 , Batch [ 10 / 93 ] : Loss = 0.534895, Accuracy = 32.67%, MSE = 1.2358
2025-03-02 09:31:37 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 1.16% (86)), (2: 98.77% (81))
2025-03-02 09:33:41 - INFO - Epoch : 44 , Batch [ 20 / 93 ] : Loss = 15.890054, Accuracy = 32.48%, MSE = 1.2684
2025-03-02 09:33:41 - INFO - Classwise accuracy : (0: 100.00% (94)), (1: 0.00% (82)), (2: 0.00% (80))
2025-03-02 09:36:02 - INFO - Epoch : 44 , Batch [ 30 / 93 ] : Loss = 19.959892, Accuracy = 32.64%, MSE = 1.2532
2025-03-02 09:36:02 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (99)), (2: 0.00% (68))
2025-03-02 09:38:13 - INFO - Epoch : 44 , Batch [ 40 / 93 ] : Loss = 12.890736, Accuracy = 33.00%, MSE = 1.2779
2025-03-02 09:38:13 - INFO - Classwise accuracy : (0: 98.53% (68)), (1: 0.00% (101)), (2: 0.00% (87))
2025-03-02 09:40:24 - INFO - Epoch : 44 , Batch [ 50 / 93 ] : Loss = 1.058651, Accuracy = 33.29%, MSE = 1.2721
2025-03-02 09:40:24 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 20.83% (72)), (2: 64.13% (92))
2025-03-02 09:42:32 - INFO - Epoch : 44 , Batch [ 60 / 93 ] : Loss = 1.689994, Accuracy = 33.26%, MSE = 1.2768
2025-03-02 09:42:32 - INFO - Classwise accuracy : (0: 0.00% (69)), (1: 100.00% (92)), (2: 0.00% (95))
2025-03-02 09:44:59 - INFO - Epoch : 44 , Batch [ 70 / 93 ] : Loss = 1.104320, Accuracy = 33.46%, MSE = 1.2935
2025-03-02 09:44:59 - INFO - Classwise accuracy : (0: 1.43% (70)), (1: 0.00% (97)), (2: 100.00% (89))
2025-03-02 09:47:30 - INFO - Epoch : 44 , Batch [ 80 / 93 ] : Loss = 0.522960, Accuracy = 33.71%, MSE = 1.2848
2025-03-02 09:47:30 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (82)), (2: 100.00% (85))
2025-03-02 09:50:08 - INFO - Epoch : 44 , Batch [ 90 / 93 ] : Loss = 0.491792, Accuracy = 33.62%, MSE = 1.2812
2025-03-02 09:50:08 - INFO - Classwise accuracy : (0: 72.73% (88)), (1: 0.00% (75)), (2: 36.56% (93))
2025-03-02 09:50:34 - INFO - Epoch 44: Train Loss=6.2893, Train Acc=33.66%, Train MSE=1.2842
2025-03-02 09:50:49 - INFO - Epoch 44: Val Loss=0.4158, Val Acc=68.00%
2025-03-02 09:50:49 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 09:50:49 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 09:50:54 - INFO - Epoch : 45 , Batch [ 0 / 93 ] : Loss = 0.523727, Accuracy = 30.86%, MSE = 0.6914
2025-03-02 09:50:54 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 100.00% (79)), (2: 0.00% (96))
2025-03-02 09:52:36 - INFO - Epoch : 45 , Batch [ 10 / 93 ] : Loss = 0.487194, Accuracy = 33.38%, MSE = 1.3352
2025-03-02 09:52:36 - INFO - Classwise accuracy : (0: 73.96% (96)), (1: 15.19% (79)), (2: 2.47% (81))
2025-03-02 09:54:35 - INFO - Epoch : 45 , Batch [ 20 / 93 ] : Loss = 0.500162, Accuracy = 33.00%, MSE = 1.4066
2025-03-02 09:54:35 - INFO - Classwise accuracy : (0: 98.70% (77)), (1: 0.00% (96)), (2: 0.00% (83))
2025-03-02 09:56:50 - INFO - Epoch : 45 , Batch [ 30 / 93 ] : Loss = 0.484571, Accuracy = 33.42%, MSE = 1.3780
2025-03-02 09:56:50 - INFO - Classwise accuracy : (0: 76.24% (101)), (1: 5.41% (74)), (2: 16.05% (81))
2025-03-02 09:58:51 - INFO - Epoch : 45 , Batch [ 40 / 93 ] : Loss = 0.491552, Accuracy = 33.39%, MSE = 1.3861
2025-03-02 09:58:51 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 100.00% (87)), (2: 0.00% (94))
2025-03-02 10:00:44 - INFO - Epoch : 45 , Batch [ 50 / 93 ] : Loss = 0.491547, Accuracy = 33.71%, MSE = 1.3996
2025-03-02 10:00:44 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (90)), (2: 0.00% (77))
2025-03-02 10:02:48 - INFO - Epoch : 45 , Batch [ 60 / 93 ] : Loss = 0.489186, Accuracy = 33.82%, MSE = 1.3499
2025-03-02 10:02:48 - INFO - Classwise accuracy : (0: 1.12% (89)), (1: 2.60% (77)), (2: 91.11% (90))
2025-03-02 10:04:42 - INFO - Epoch : 45 , Batch [ 70 / 93 ] : Loss = 0.497417, Accuracy = 33.71%, MSE = 1.3189
2025-03-02 10:04:42 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (88)), (2: 100.00% (83))
2025-03-02 10:06:32 - INFO - Epoch : 45 , Batch [ 80 / 93 ] : Loss = 0.493609, Accuracy = 33.53%, MSE = 1.3269
2025-03-02 10:06:32 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 22.22% (81)), (2: 77.50% (80))
2025-03-02 10:08:29 - INFO - Epoch : 45 , Batch [ 90 / 93 ] : Loss = 0.488312, Accuracy = 33.46%, MSE = 1.3326
2025-03-02 10:08:29 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (92)), (2: 0.00% (87))
2025-03-02 10:08:51 - INFO - Epoch 45: Train Loss=0.4937, Train Acc=33.50%, Train MSE=1.3176
2025-03-02 10:09:07 - INFO - Epoch 45: Val Loss=0.4561, Val Acc=68.00%
2025-03-02 10:09:07 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 10:09:07 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 10:09:11 - INFO - Epoch : 46 , Batch [ 0 / 93 ] : Loss = 0.484440, Accuracy = 39.06%, MSE = 0.6094
2025-03-02 10:09:11 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 100.00% (100)), (2: 0.00% (84))
2025-03-02 10:11:01 - INFO - Epoch : 46 , Batch [ 10 / 93 ] : Loss = 0.489020, Accuracy = 32.95%, MSE = 1.3789
2025-03-02 10:11:01 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 96.63% (89)), (2: 4.71% (85))
2025-03-02 10:12:59 - INFO - Epoch : 46 , Batch [ 20 / 93 ] : Loss = 0.488210, Accuracy = 33.09%, MSE = 1.3627
2025-03-02 10:12:59 - INFO - Classwise accuracy : (0: 33.72% (86)), (1: 32.10% (81)), (2: 32.58% (89))
2025-03-02 10:14:58 - INFO - Epoch : 46 , Batch [ 30 / 93 ] : Loss = 0.487523, Accuracy = 32.98%, MSE = 1.3771
2025-03-02 10:14:58 - INFO - Classwise accuracy : (0: 3.49% (86)), (1: 9.41% (85)), (2: 92.94% (85))
2025-03-02 10:17:01 - INFO - Epoch : 46 , Batch [ 40 / 93 ] : Loss = 0.492594, Accuracy = 32.70%, MSE = 1.3904
2025-03-02 10:17:01 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 87.34% (79)), (2: 13.98% (93))
2025-03-02 10:19:11 - INFO - Epoch : 46 , Batch [ 50 / 93 ] : Loss = 0.488310, Accuracy = 32.86%, MSE = 1.4000
2025-03-02 10:19:11 - INFO - Classwise accuracy : (0: 1.23% (81)), (1: 10.59% (85)), (2: 93.33% (90))
2025-03-02 10:21:14 - INFO - Epoch : 46 , Batch [ 60 / 93 ] : Loss = 0.488962, Accuracy = 32.94%, MSE = 1.4131
2025-03-02 10:21:14 - INFO - Classwise accuracy : (0: 86.52% (89)), (1: 0.00% (90)), (2: 18.18% (77))
2025-03-02 10:23:36 - INFO - Epoch : 46 , Batch [ 70 / 93 ] : Loss = 0.488676, Accuracy = 32.94%, MSE = 1.3861
2025-03-02 10:23:36 - INFO - Classwise accuracy : (0: 5.32% (94)), (1: 97.53% (81)), (2: 2.47% (81))
2025-03-02 10:25:51 - INFO - Epoch : 46 , Batch [ 80 / 93 ] : Loss = 0.486971, Accuracy = 33.04%, MSE = 1.3401
2025-03-02 10:25:51 - INFO - Classwise accuracy : (0: 0.00% (73)), (1: 59.30% (86)), (2: 47.42% (97))
2025-03-02 10:28:08 - INFO - Epoch : 46 , Batch [ 90 / 93 ] : Loss = 0.488078, Accuracy = 32.89%, MSE = 1.3197
2025-03-02 10:28:08 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 69.23% (91)), (2: 21.84% (87))
2025-03-02 10:28:33 - INFO - Epoch 46: Train Loss=0.4910, Train Acc=32.92%, Train MSE=1.3225
2025-03-02 10:28:49 - INFO - Epoch 46: Val Loss=0.5308, Val Acc=15.81%
2025-03-02 10:28:49 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 10:28:49 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 10:28:52 - INFO - Epoch : 47 , Batch [ 0 / 93 ] : Loss = 0.492220, Accuracy = 35.16%, MSE = 1.5625
2025-03-02 10:28:52 - INFO - Classwise accuracy : (0: 81.72% (93)), (1: 0.00% (88)), (2: 18.67% (75))
2025-03-02 10:30:57 - INFO - Epoch : 47 , Batch [ 10 / 93 ] : Loss = 0.491989, Accuracy = 32.88%, MSE = 1.3732
2025-03-02 10:30:57 - INFO - Classwise accuracy : (0: 100.00% (74)), (1: 0.00% (95)), (2: 1.15% (87))
2025-03-02 10:33:18 - INFO - Epoch : 47 , Batch [ 20 / 93 ] : Loss = 0.489633, Accuracy = 33.15%, MSE = 1.2662
2025-03-02 10:33:18 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 67.03% (91)), (2: 16.28% (86))
2025-03-02 10:35:52 - INFO - Epoch : 47 , Batch [ 30 / 93 ] : Loss = 0.497677, Accuracy = 32.80%, MSE = 1.3335
2025-03-02 10:35:52 - INFO - Classwise accuracy : (0: 100.00% (75)), (1: 0.00% (94)), (2: 0.00% (87))
2025-03-02 10:38:14 - INFO - Epoch : 47 , Batch [ 40 / 93 ] : Loss = 0.510936, Accuracy = 32.96%, MSE = 1.3196
2025-03-02 10:38:14 - INFO - Classwise accuracy : (0: 0.00% (101)), (1: 98.65% (74)), (2: 1.23% (81))
2025-03-02 10:40:50 - INFO - Epoch : 47 , Batch [ 50 / 93 ] : Loss = 0.490348, Accuracy = 33.16%, MSE = 1.3454
2025-03-02 10:40:50 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (89)), (2: 0.00% (80))
2025-03-02 10:43:11 - INFO - Epoch : 47 , Batch [ 60 / 93 ] : Loss = 0.493114, Accuracy = 32.99%, MSE = 1.3500
2025-03-02 10:43:11 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (91)), (2: 0.00% (82))
2025-03-02 10:45:40 - INFO - Epoch : 47 , Batch [ 70 / 93 ] : Loss = 0.486830, Accuracy = 33.07%, MSE = 1.3429
2025-03-02 10:45:40 - INFO - Classwise accuracy : (0: 12.00% (100)), (1: 81.71% (82)), (2: 0.00% (74))
2025-03-02 10:47:53 - INFO - Epoch : 47 , Batch [ 80 / 93 ] : Loss = 0.596974, Accuracy = 32.99%, MSE = 1.3654
2025-03-02 10:47:53 - INFO - Classwise accuracy : (0: 12.50% (80)), (1: 85.37% (82)), (2: 0.00% (94))
2025-03-02 10:50:08 - INFO - Epoch : 47 , Batch [ 90 / 93 ] : Loss = 7.622292, Accuracy = 33.11%, MSE = 1.3632
2025-03-02 10:50:08 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (91)), (2: 0.00% (79))
2025-03-02 10:50:31 - INFO - Epoch 47: Train Loss=1.0437, Train Acc=33.15%, Train MSE=1.3653
2025-03-02 10:50:46 - INFO - Epoch 47: Val Loss=3.6301, Val Acc=15.81%
2025-03-02 10:50:46 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 10:50:46 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 10:50:51 - INFO - Epoch : 48 , Batch [ 0 / 93 ] : Loss = 3.457620, Accuracy = 36.33%, MSE = 1.6328
2025-03-02 10:50:51 - INFO - Classwise accuracy : (0: 100.00% (93)), (1: 0.00% (78)), (2: 0.00% (85))
2025-03-02 10:52:42 - INFO - Epoch : 48 , Batch [ 10 / 93 ] : Loss = 2.202685, Accuracy = 32.74%, MSE = 1.3810
2025-03-02 10:52:42 - INFO - Classwise accuracy : (0: 0.00% (73)), (1: 0.00% (96)), (2: 100.00% (87))
2025-03-02 10:54:40 - INFO - Epoch : 48 , Batch [ 20 / 93 ] : Loss = 2.421666, Accuracy = 33.46%, MSE = 1.3149
2025-03-02 10:54:40 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 0.00% (78)), (2: 100.00% (97))
2025-03-02 10:56:58 - INFO - Epoch : 48 , Batch [ 30 / 93 ] : Loss = 1.237518, Accuracy = 33.23%, MSE = 1.2926
2025-03-02 10:56:58 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (90)), (2: 0.00% (78))
2025-03-02 10:59:39 - INFO - Epoch : 48 , Batch [ 40 / 93 ] : Loss = 1.537696, Accuracy = 33.07%, MSE = 1.3038
2025-03-02 10:59:39 - INFO - Classwise accuracy : (0: 100.00% (80)), (1: 0.00% (91)), (2: 0.00% (85))
2025-03-02 11:00:19 - INFO - Logging started
2025-03-02 11:02:09 - INFO - Epoch : 48 , Batch [ 50 / 93 ] : Loss = 0.751192, Accuracy = 33.11%, MSE = 1.2964
2025-03-02 11:02:09 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 100.00% (90)), (2: 0.00% (82))
2025-03-02 11:04:19 - INFO - Epoch : 48 , Batch [ 60 / 93 ] : Loss = 0.505848, Accuracy = 32.91%, MSE = 1.2775
2025-03-02 11:04:19 - INFO - Classwise accuracy : (0: 35.63% (87)), (1: 0.00% (94)), (2: 68.00% (75))
2025-03-02 11:06:17 - INFO - Epoch : 48 , Batch [ 70 / 93 ] : Loss = 0.491842, Accuracy = 33.01%, MSE = 1.2749
2025-03-02 11:06:17 - INFO - Classwise accuracy : (0: 93.75% (96)), (1: 0.00% (84)), (2: 2.63% (76))
2025-03-02 11:08:09 - INFO - Epoch : 48 , Batch [ 80 / 93 ] : Loss = 0.495376, Accuracy = 33.13%, MSE = 1.2846
2025-03-02 11:08:09 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (94)), (2: 100.00% (87))
2025-03-02 11:10:13 - INFO - Epoch : 48 , Batch [ 90 / 93 ] : Loss = 0.490056, Accuracy = 33.11%, MSE = 1.3058
2025-03-02 11:10:13 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 74.16% (89)), (2: 22.22% (81))
2025-03-02 11:10:37 - INFO - Epoch 48: Train Loss=1.6510, Train Acc=33.15%, Train MSE=1.3138
2025-03-02 11:10:53 - INFO - Epoch 48: Val Loss=0.5388, Val Acc=15.81%
2025-03-02 11:10:53 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 11:10:53 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 11:10:58 - INFO - Epoch : 49 , Batch [ 0 / 93 ] : Loss = 0.508229, Accuracy = 30.47%, MSE = 1.8320
2025-03-02 11:10:58 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (81)), (2: 0.00% (97))
2025-03-02 11:13:08 - INFO - Epoch : 49 , Batch [ 10 / 93 ] : Loss = 0.494457, Accuracy = 32.74%, MSE = 1.6069
2025-03-02 11:13:08 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (94)), (2: 0.00% (81))
2025-03-02 11:15:22 - INFO - Epoch : 49 , Batch [ 20 / 93 ] : Loss = 0.499141, Accuracy = 33.11%, MSE = 1.5071
2025-03-02 11:15:22 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (92)), (2: 0.00% (86))
2025-03-02 11:17:28 - INFO - Epoch : 49 , Batch [ 30 / 93 ] : Loss = 0.492980, Accuracy = 33.17%, MSE = 1.3560
2025-03-02 11:17:28 - INFO - Classwise accuracy : (0: 100.00% (76)), (1: 0.00% (87)), (2: 0.00% (93))
2025-03-02 11:19:25 - INFO - Epoch : 49 , Batch [ 40 / 93 ] : Loss = 0.487710, Accuracy = 33.33%, MSE = 1.4016
2025-03-02 11:19:25 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 95.70% (93)), (2: 7.06% (85))
2025-03-02 11:21:45 - INFO - Epoch : 49 , Batch [ 50 / 93 ] : Loss = 0.489716, Accuracy = 32.94%, MSE = 1.3852
2025-03-02 11:21:45 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 12.50% (88)), (2: 89.33% (75))
2025-03-02 11:23:59 - INFO - Epoch : 49 , Batch [ 60 / 93 ] : Loss = 0.493669, Accuracy = 32.92%, MSE = 1.3332
2025-03-02 11:23:59 - INFO - Classwise accuracy : (0: 100.00% (66)), (1: 0.00% (93)), (2: 0.00% (97))
2025-03-02 11:26:11 - INFO - Epoch : 49 , Batch [ 70 / 93 ] : Loss = 0.488197, Accuracy = 32.95%, MSE = 1.3063
2025-03-02 11:26:11 - INFO - Classwise accuracy : (0: 37.66% (77)), (1: 47.50% (80)), (2: 16.16% (99))
2025-03-02 11:28:22 - INFO - Epoch : 49 , Batch [ 80 / 93 ] : Loss = 0.495430, Accuracy = 32.90%, MSE = 1.3258
2025-03-02 11:28:22 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 100.00% (77)), (2: 0.00% (94))
2025-03-02 11:30:38 - INFO - Epoch : 49 , Batch [ 90 / 93 ] : Loss = 0.489262, Accuracy = 32.96%, MSE = 1.3398
2025-03-02 11:30:38 - INFO - Classwise accuracy : (0: 96.59% (88)), (1: 0.00% (79)), (2: 0.00% (89))
2025-03-02 11:31:04 - INFO - Epoch 49: Train Loss=0.4917, Train Acc=33.01%, Train MSE=1.3242
2025-03-02 11:31:20 - INFO - Epoch 49: Val Loss=0.4432, Val Acc=68.00%
2025-03-02 11:31:20 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 11:31:20 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 11:31:24 - INFO - Epoch : 50 , Batch [ 0 / 93 ] : Loss = 0.495067, Accuracy = 31.64%, MSE = 0.6836
2025-03-02 11:31:24 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (81)), (2: 0.00% (88))
2025-03-02 11:33:13 - INFO - Epoch : 50 , Batch [ 10 / 93 ] : Loss = 0.483621, Accuracy = 33.06%, MSE = 1.1797
2025-03-02 11:33:13 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 100.00% (97)), (2: 0.00% (87))
2025-03-02 11:35:27 - INFO - Epoch : 50 , Batch [ 20 / 93 ] : Loss = 0.488321, Accuracy = 33.44%, MSE = 1.3017
2025-03-02 11:35:27 - INFO - Classwise accuracy : (0: 3.45% (87)), (1: 91.95% (87)), (2: 0.00% (82))
2025-03-02 11:38:13 - INFO - Epoch : 50 , Batch [ 30 / 93 ] : Loss = 0.491261, Accuracy = 33.68%, MSE = 1.2155
2025-03-02 11:38:13 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 100.00% (79)), (2: 0.00% (82))
2025-03-02 11:40:45 - INFO - Epoch : 50 , Batch [ 40 / 93 ] : Loss = 0.490206, Accuracy = 33.49%, MSE = 1.2733
2025-03-02 11:40:45 - INFO - Classwise accuracy : (0: 85.54% (83)), (1: 18.48% (92)), (2: 0.00% (81))
2025-03-02 11:43:07 - INFO - Epoch : 50 , Batch [ 50 / 93 ] : Loss = 0.489417, Accuracy = 33.73%, MSE = 1.3215
2025-03-02 11:43:07 - INFO - Classwise accuracy : (0: 11.49% (87)), (1: 89.41% (85)), (2: 0.00% (84))
2025-03-02 11:45:33 - INFO - Epoch : 50 , Batch [ 60 / 93 ] : Loss = 0.490358, Accuracy = 33.88%, MSE = 1.2882
2025-03-02 11:45:33 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (67)), (2: 0.93% (107))
2025-03-02 11:48:17 - INFO - Epoch : 50 , Batch [ 70 / 93 ] : Loss = 0.519710, Accuracy = 33.76%, MSE = 1.2968
2025-03-02 11:48:17 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (69)), (2: 0.00% (110))
2025-03-02 11:50:48 - INFO - Epoch : 50 , Batch [ 80 / 93 ] : Loss = 0.563571, Accuracy = 33.66%, MSE = 1.3149
2025-03-02 11:50:48 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (99)), (2: 100.00% (79))
2025-03-02 11:53:11 - INFO - Epoch : 50 , Batch [ 90 / 93 ] : Loss = 10.323574, Accuracy = 33.42%, MSE = 1.3137
2025-03-02 11:53:11 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (89)), (2: 100.00% (84))
2025-03-02 11:53:36 - INFO - Epoch 50: Train Loss=0.8223, Train Acc=33.37%, Train MSE=1.3109
2025-03-02 11:53:52 - INFO - Epoch 50: Val Loss=6.5672, Val Acc=16.19%
2025-03-02 11:53:52 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 11:53:52 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 11:53:57 - INFO - Epoch : 51 , Batch [ 0 / 93 ] : Loss = 4.593507, Accuracy = 34.38%, MSE = 1.6055
2025-03-02 11:53:57 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 0.00% (87)), (2: 100.00% (88))
2025-03-02 11:56:15 - INFO - Epoch : 51 , Batch [ 10 / 93 ] : Loss = 3.436558, Accuracy = 34.09%, MSE = 1.3675
2025-03-02 11:56:15 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (80)), (2: 0.00% (90))
2025-03-02 11:58:36 - INFO - Epoch : 51 , Batch [ 20 / 93 ] : Loss = 0.538517, Accuracy = 33.46%, MSE = 1.3607
2025-03-02 11:58:36 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 98.82% (85)), (2: 0.00% (87))
2025-03-02 12:00:45 - INFO - Epoch : 51 , Batch [ 30 / 93 ] : Loss = 0.601663, Accuracy = 33.39%, MSE = 1.3582
2025-03-02 12:00:45 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (92)), (2: 0.00% (81))
2025-03-02 12:02:51 - INFO - Epoch : 51 , Batch [ 40 / 93 ] : Loss = 0.495855, Accuracy = 33.44%, MSE = 1.3853
2025-03-02 12:02:51 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 1.27% (79)), (2: 100.00% (82))
2025-03-02 12:05:27 - INFO - Epoch : 51 , Batch [ 50 / 93 ] : Loss = 0.494571, Accuracy = 33.51%, MSE = 1.4374
2025-03-02 12:05:27 - INFO - Classwise accuracy : (0: 100.00% (95)), (1: 0.00% (88)), (2: 0.00% (73))
2025-03-02 12:07:41 - INFO - Epoch : 51 , Batch [ 60 / 93 ] : Loss = 0.497645, Accuracy = 33.72%, MSE = 1.4030
2025-03-02 12:07:41 - INFO - Classwise accuracy : (0: 13.58% (81)), (1: 87.64% (89)), (2: 0.00% (86))
2025-03-02 12:09:51 - INFO - Epoch : 51 , Batch [ 70 / 93 ] : Loss = 0.495373, Accuracy = 33.51%, MSE = 1.4055
2025-03-02 12:09:51 - INFO - Classwise accuracy : (0: 93.06% (72)), (1: 2.53% (79)), (2: 0.00% (105))
2025-03-02 12:12:00 - INFO - Epoch : 51 , Batch [ 80 / 93 ] : Loss = 0.494082, Accuracy = 33.41%, MSE = 1.3666
2025-03-02 12:12:00 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 100.00% (83)), (2: 0.00% (93))
2025-03-02 12:14:05 - INFO - Epoch : 51 , Batch [ 90 / 93 ] : Loss = 0.494312, Accuracy = 33.34%, MSE = 1.3454
2025-03-02 12:14:05 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 0.00% (99)), (2: 0.00% (75))
2025-03-02 12:14:32 - INFO - Epoch 51: Train Loss=0.8007, Train Acc=33.27%, Train MSE=1.3421
2025-03-02 12:14:48 - INFO - Epoch 51: Val Loss=0.4241, Val Acc=68.00%
2025-03-02 12:14:48 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 12:14:48 - INFO - Updated class weights : tensor([1.0000, 1.0000, 1.0000], device='mps:0')
2025-03-02 12:14:52 - INFO - Epoch : 52 , Batch [ 0 / 93 ] : Loss = 0.499841, Accuracy = 34.38%, MSE = 0.6562
2025-03-02 12:14:52 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 100.00% (88)), (2: 0.00% (78))
2025-03-02 12:16:58 - INFO - Epoch : 52 , Batch [ 10 / 93 ] : Loss = 0.506114, Accuracy = 32.03%, MSE = 1.0600
2025-03-02 12:16:58 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (86)), (2: 0.00% (88))
2025-03-02 12:19:09 - INFO - Epoch : 52 , Batch [ 20 / 93 ] : Loss = 0.499930, Accuracy = 32.89%, MSE = 1.1801
2025-03-02 12:19:09 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 77.50% (80)), (2: 14.94% (87))
2025-03-02 12:21:24 - INFO - Epoch : 52 , Batch [ 30 / 93 ] : Loss = 0.493562, Accuracy = 33.04%, MSE = 1.2480
2025-03-02 12:21:24 - INFO - Classwise accuracy : (0: 24.10% (83)), (1: 0.00% (83)), (2: 67.78% (90))
2025-03-02 12:23:52 - INFO - Epoch : 52 , Batch [ 40 / 93 ] : Loss = 0.505888, Accuracy = 33.03%, MSE = 1.2036
2025-03-02 12:23:52 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (70)), (2: 0.00% (93))
2025-03-02 12:26:12 - INFO - Epoch : 52 , Batch [ 50 / 93 ] : Loss = 0.489418, Accuracy = 32.97%, MSE = 1.2585
2025-03-02 12:26:12 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (79)), (2: 0.00% (90))
2025-03-02 12:28:25 - INFO - Epoch : 52 , Batch [ 60 / 93 ] : Loss = 0.494795, Accuracy = 33.00%, MSE = 1.2567
2025-03-02 12:28:25 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 86.59% (82)), (2: 6.82% (88))
2025-03-02 12:30:59 - INFO - Epoch : 52 , Batch [ 70 / 93 ] : Loss = 0.491852, Accuracy = 33.29%, MSE = 1.2320
2025-03-02 12:30:59 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (75)), (2: 0.00% (90))
2025-03-02 12:33:50 - INFO - Epoch : 52 , Batch [ 80 / 93 ] : Loss = 0.490028, Accuracy = 33.27%, MSE = 1.2445
2025-03-02 12:33:50 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 100.00% (95)), (2: 0.00% (90))
2025-03-02 12:36:22 - INFO - Epoch : 52 , Batch [ 90 / 93 ] : Loss = 0.489581, Accuracy = 33.25%, MSE = 1.2333
2025-03-02 12:36:22 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 87.50% (80)), (2: 9.20% (87))
2025-03-02 12:36:49 - INFO - Epoch 52: Train Loss=0.4941, Train Acc=33.31%, Train MSE=1.2200
2025-03-02 12:37:05 - INFO - Epoch 52: Val Loss=0.4502, Val Acc=68.00%
2025-03-02 12:37:05 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 12:37:05 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 12:37:09 - INFO - Epoch : 53 , Batch [ 0 / 93 ] : Loss = 0.492852, Accuracy = 34.38%, MSE = 0.6562
2025-03-02 12:37:09 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (88)), (2: 0.00% (90))
2025-03-02 12:39:18 - INFO - Epoch : 53 , Batch [ 10 / 93 ] : Loss = 0.489129, Accuracy = 34.02%, MSE = 1.1989
2025-03-02 12:39:18 - INFO - Classwise accuracy : (0: 0.00% (70)), (1: 0.00% (104)), (2: 100.00% (82))
2025-03-02 12:41:54 - INFO - Epoch : 53 , Batch [ 20 / 93 ] : Loss = 0.500877, Accuracy = 33.87%, MSE = 1.2840
2025-03-02 12:41:54 - INFO - Classwise accuracy : (0: 100.00% (77)), (1: 0.00% (92)), (2: 0.00% (87))
2025-03-02 12:44:26 - INFO - Epoch : 53 , Batch [ 30 / 93 ] : Loss = 0.488882, Accuracy = 33.37%, MSE = 1.2576
2025-03-02 12:44:26 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (88)), (2: 0.00% (86))
2025-03-02 12:46:45 - INFO - Epoch : 53 , Batch [ 40 / 93 ] : Loss = 0.496442, Accuracy = 33.08%, MSE = 1.3395
2025-03-02 12:46:45 - INFO - Classwise accuracy : (0: 0.00% (98)), (1: 90.00% (80)), (2: 7.69% (78))
2025-03-02 12:49:11 - INFO - Epoch : 53 , Batch [ 50 / 93 ] : Loss = 0.566501, Accuracy = 33.34%, MSE = 1.3447
2025-03-02 12:49:11 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (83)), (2: 100.00% (84))
2025-03-02 12:51:27 - INFO - Epoch : 53 , Batch [ 60 / 93 ] : Loss = 2.538336, Accuracy = 33.20%, MSE = 1.3487
2025-03-02 12:51:27 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (83)), (2: 100.00% (89))
2025-03-02 12:53:45 - INFO - Epoch : 53 , Batch [ 70 / 93 ] : Loss = 4.822709, Accuracy = 33.11%, MSE = 1.3446
2025-03-02 12:53:45 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 100.00% (84)), (2: 0.00% (88))
2025-03-02 12:56:04 - INFO - Epoch : 53 , Batch [ 80 / 93 ] : Loss = 1.021516, Accuracy = 33.13%, MSE = 1.3365
2025-03-02 12:56:04 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (85)), (2: 0.00% (88))
2025-03-02 12:58:26 - INFO - Epoch : 53 , Batch [ 90 / 93 ] : Loss = 1.157269, Accuracy = 33.39%, MSE = 1.3376
2025-03-02 12:58:26 - INFO - Classwise accuracy : (0: 100.00% (91)), (1: 0.00% (75)), (2: 0.00% (90))
2025-03-02 12:58:52 - INFO - Epoch 53: Train Loss=1.1386, Train Acc=33.30%, Train MSE=1.3372
2025-03-02 12:59:07 - INFO - Epoch 53: Val Loss=0.5614, Val Acc=16.19%
2025-03-02 12:59:07 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 12:59:07 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 12:59:12 - INFO - Epoch : 54 , Batch [ 0 / 93 ] : Loss = 0.665152, Accuracy = 37.50%, MSE = 1.3516
2025-03-02 12:59:12 - INFO - Classwise accuracy : (0: 0.00% (62)), (1: 0.00% (98)), (2: 100.00% (96))
2025-03-02 13:00:59 - INFO - Epoch : 54 , Batch [ 10 / 93 ] : Loss = 0.506656, Accuracy = 33.45%, MSE = 1.3984
2025-03-02 13:00:59 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (79)), (2: 0.00% (92))
2025-03-02 13:02:56 - INFO - Epoch : 54 , Batch [ 20 / 93 ] : Loss = 0.506258, Accuracy = 33.74%, MSE = 1.3250
2025-03-02 13:02:56 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (83)), (2: 98.94% (94))
2025-03-02 13:05:02 - INFO - Epoch : 54 , Batch [ 30 / 93 ] : Loss = 0.496802, Accuracy = 33.49%, MSE = 1.3104
2025-03-02 13:05:02 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (84)), (2: 0.00% (89))
2025-03-02 13:07:12 - INFO - Epoch : 54 , Batch [ 40 / 93 ] : Loss = 0.485995, Accuracy = 33.57%, MSE = 1.3028
2025-03-02 13:07:12 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 30.77% (78)), (2: 75.49% (102))
2025-03-02 13:09:20 - INFO - Epoch : 54 , Batch [ 50 / 93 ] : Loss = 0.488173, Accuracy = 33.65%, MSE = 1.3198
2025-03-02 13:09:20 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (99)), (2: 0.00% (80))
2025-03-02 13:11:43 - INFO - Epoch : 54 , Batch [ 60 / 93 ] : Loss = 0.498689, Accuracy = 33.77%, MSE = 1.3195
2025-03-02 13:11:43 - INFO - Classwise accuracy : (0: 58.67% (75)), (1: 40.45% (89)), (2: 0.00% (92))
2025-03-02 13:14:02 - INFO - Epoch : 54 , Batch [ 70 / 93 ] : Loss = 0.493243, Accuracy = 33.88%, MSE = 1.3495
2025-03-02 13:14:02 - INFO - Classwise accuracy : (0: 98.86% (88)), (1: 3.61% (83)), (2: 0.00% (85))
2025-03-02 13:16:22 - INFO - Epoch : 54 , Batch [ 80 / 93 ] : Loss = 0.491342, Accuracy = 33.85%, MSE = 1.3335
2025-03-02 13:16:22 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (84)), (2: 0.00% (80))
2025-03-02 13:19:11 - INFO - Epoch : 54 , Batch [ 90 / 93 ] : Loss = 0.487357, Accuracy = 33.97%, MSE = 1.3251
2025-03-02 13:19:11 - INFO - Classwise accuracy : (0: 26.83% (82)), (1: 1.35% (74)), (2: 76.00% (100))
2025-03-02 13:19:39 - INFO - Epoch 54: Train Loss=0.5082, Train Acc=33.93%, Train MSE=1.3296
2025-03-02 13:19:55 - INFO - Epoch 54: Val Loss=0.4566, Val Acc=68.00%
2025-03-02 13:19:55 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 13:19:55 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 13:20:00 - INFO - Epoch : 55 , Batch [ 0 / 93 ] : Loss = 0.489890, Accuracy = 34.38%, MSE = 0.6562
2025-03-02 13:20:00 - INFO - Classwise accuracy : (0: 1.14% (88)), (1: 100.00% (87)), (2: 0.00% (81))
2025-03-02 13:22:05 - INFO - Epoch : 55 , Batch [ 10 / 93 ] : Loss = 0.491210, Accuracy = 33.77%, MSE = 0.9851
2025-03-02 13:22:05 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (91)), (2: 0.00% (89))
2025-03-02 13:24:12 - INFO - Epoch : 55 , Batch [ 20 / 93 ] : Loss = 0.491029, Accuracy = 33.37%, MSE = 1.1479
2025-03-02 13:24:12 - INFO - Classwise accuracy : (0: 1.00% (100)), (1: 98.61% (72)), (2: 0.00% (84))
2025-03-02 13:26:39 - INFO - Epoch : 55 , Batch [ 30 / 93 ] : Loss = 0.488236, Accuracy = 32.93%, MSE = 1.2892
2025-03-02 13:26:39 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (95)), (2: 0.00% (73))
2025-03-02 13:29:05 - INFO - Epoch : 55 , Batch [ 40 / 93 ] : Loss = 0.488564, Accuracy = 32.88%, MSE = 1.3769
2025-03-02 13:29:05 - INFO - Classwise accuracy : (0: 100.00% (75)), (1: 0.00% (108)), (2: 0.00% (73))
2025-03-02 13:31:42 - INFO - Epoch : 55 , Batch [ 50 / 93 ] : Loss = 0.489253, Accuracy = 33.05%, MSE = 1.3745
2025-03-02 13:31:42 - INFO - Classwise accuracy : (0: 25.76% (66)), (1: 0.00% (87)), (2: 81.55% (103))
2025-03-02 13:34:25 - INFO - Epoch : 55 , Batch [ 60 / 93 ] : Loss = 0.488206, Accuracy = 32.95%, MSE = 1.3598
2025-03-02 13:34:25 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (90)), (2: 0.00% (78))
2025-03-02 13:37:01 - INFO - Epoch : 55 , Batch [ 70 / 93 ] : Loss = 0.487845, Accuracy = 33.10%, MSE = 1.3391
2025-03-02 13:37:01 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 1.35% (74)), (2: 95.92% (98))
2025-03-02 13:39:21 - INFO - Epoch : 55 , Batch [ 80 / 93 ] : Loss = 0.503745, Accuracy = 33.09%, MSE = 1.3408
2025-03-02 13:39:21 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (85)), (2: 0.00% (81))
2025-03-02 13:41:35 - INFO - Epoch : 55 , Batch [ 90 / 93 ] : Loss = 0.492279, Accuracy = 33.04%, MSE = 1.3501
2025-03-02 13:41:35 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 54.65% (86)), (2: 42.53% (87))
2025-03-02 13:42:00 - INFO - Epoch 55: Train Loss=0.4925, Train Acc=32.98%, Train MSE=1.3481
2025-03-02 13:42:16 - INFO - Epoch 55: Val Loss=0.4293, Val Acc=68.00%
2025-03-02 13:42:16 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 13:42:16 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 13:42:20 - INFO - Epoch : 56 , Batch [ 0 / 93 ] : Loss = 0.507669, Accuracy = 29.30%, MSE = 0.7070
2025-03-02 13:42:20 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 100.00% (75)), (2: 0.00% (91))
2025-03-02 13:44:22 - INFO - Epoch : 56 , Batch [ 10 / 93 ] : Loss = 0.487868, Accuracy = 33.10%, MSE = 1.3157
2025-03-02 13:44:22 - INFO - Classwise accuracy : (0: 82.98% (94)), (1: 25.32% (79)), (2: 0.00% (83))
2025-03-02 13:46:36 - INFO - Epoch : 56 , Batch [ 20 / 93 ] : Loss = 0.489058, Accuracy = 32.66%, MSE = 1.2526
2025-03-02 13:46:36 - INFO - Classwise accuracy : (0: 35.16% (91)), (1: 0.00% (91)), (2: 74.32% (74))
2025-03-02 13:49:14 - INFO - Epoch : 56 , Batch [ 30 / 93 ] : Loss = 1.893671, Accuracy = 32.31%, MSE = 1.2277
2025-03-02 13:49:14 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (95)), (2: 0.00% (75))
2025-03-02 13:51:54 - INFO - Epoch : 56 , Batch [ 40 / 93 ] : Loss = 1.091133, Accuracy = 32.54%, MSE = 1.2752
2025-03-02 13:51:54 - INFO - Classwise accuracy : (0: 100.00% (72)), (1: 0.00% (94)), (2: 0.00% (90))
2025-03-02 13:54:57 - INFO - Epoch : 56 , Batch [ 50 / 93 ] : Loss = 0.578706, Accuracy = 32.50%, MSE = 1.2885
2025-03-02 13:54:57 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (60)), (2: 0.00% (105))
2025-03-02 14:01:22 - INFO - Epoch : 56 , Batch [ 60 / 93 ] : Loss = 0.560815, Accuracy = 32.77%, MSE = 1.2909
2025-03-02 14:01:22 - INFO - Classwise accuracy : (0: 77.42% (93)), (1: 23.29% (73)), (2: 0.00% (90))
2025-03-02 14:36:41 - INFO - Epoch : 56 , Batch [ 70 / 93 ] : Loss = 0.488362, Accuracy = 32.85%, MSE = 1.3139
2025-03-02 14:36:41 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (88)), (2: 0.00% (92))
2025-03-02 14:55:14 - INFO - Epoch : 56 , Batch [ 80 / 93 ] : Loss = 0.491793, Accuracy = 32.74%, MSE = 1.3171
2025-03-02 14:55:14 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 95.40% (87)), (2: 1.03% (97))
2025-03-02 14:57:06 - INFO - Epoch : 56 , Batch [ 90 / 93 ] : Loss = 0.511494, Accuracy = 32.86%, MSE = 1.3189
2025-03-02 14:57:06 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (78)), (2: 0.00% (85))
2025-03-02 15:12:36 - INFO - Epoch 56: Train Loss=0.8061, Train Acc=32.81%, Train MSE=1.3287
2025-03-02 15:12:52 - INFO - Epoch 56: Val Loss=0.4780, Val Acc=15.81%
2025-03-02 15:12:52 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 15:12:52 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 15:12:56 - INFO - Epoch : 57 , Batch [ 0 / 93 ] : Loss = 0.490540, Accuracy = 31.64%, MSE = 1.3047
2025-03-02 15:12:56 - INFO - Classwise accuracy : (0: 60.92% (87)), (1: 30.34% (89)), (2: 1.25% (80))
2025-03-02 15:29:55 - INFO - Epoch : 57 , Batch [ 10 / 93 ] : Loss = 0.490237, Accuracy = 34.41%, MSE = 1.1055
2025-03-02 15:29:55 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 100.00% (88)), (2: 0.00% (78))
2025-03-02 15:54:38 - INFO - Epoch : 57 , Batch [ 20 / 93 ] : Loss = 0.489131, Accuracy = 32.96%, MSE = 1.2446
2025-03-02 15:54:38 - INFO - Classwise accuracy : (0: 88.76% (89)), (1: 0.00% (82)), (2: 12.94% (85))
2025-03-02 15:57:20 - INFO - Epoch : 57 , Batch [ 30 / 93 ] : Loss = 0.489041, Accuracy = 32.89%, MSE = 1.3648
2025-03-02 15:57:20 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 16.09% (87)), (2: 69.66% (89))
2025-03-02 15:59:56 - INFO - Epoch : 57 , Batch [ 40 / 93 ] : Loss = 0.491374, Accuracy = 32.84%, MSE = 1.3750
2025-03-02 15:59:56 - INFO - Classwise accuracy : (0: 98.59% (71)), (1: 0.00% (95)), (2: 1.11% (90))
2025-03-02 16:02:15 - INFO - Epoch : 57 , Batch [ 50 / 93 ] : Loss = 0.485921, Accuracy = 33.11%, MSE = 1.3660
2025-03-02 16:02:15 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 100.00% (95)), (2: 0.00% (76))
2025-03-02 16:04:25 - INFO - Epoch : 57 , Batch [ 60 / 93 ] : Loss = 0.487964, Accuracy = 33.12%, MSE = 1.3379
2025-03-02 16:04:25 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (91)), (2: 0.00% (89))
2025-03-02 16:06:34 - INFO - Epoch : 57 , Batch [ 70 / 93 ] : Loss = 0.492383, Accuracy = 33.20%, MSE = 1.2997
2025-03-02 16:06:34 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (90)), (2: 100.00% (82))
2025-03-02 16:08:56 - INFO - Epoch : 57 , Batch [ 80 / 93 ] : Loss = 0.488800, Accuracy = 33.24%, MSE = 1.3049
2025-03-02 16:08:56 - INFO - Classwise accuracy : (0: 6.94% (72)), (1: 84.21% (95)), (2: 0.00% (89))
2025-03-02 16:11:27 - INFO - Epoch : 57 , Batch [ 90 / 93 ] : Loss = 0.492936, Accuracy = 33.29%, MSE = 1.3003
2025-03-02 16:11:27 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 0.00% (75)), (2: 100.00% (95))
2025-03-02 16:11:55 - INFO - Epoch 57: Train Loss=0.4915, Train Acc=33.31%, Train MSE=1.3087
2025-03-02 16:12:11 - INFO - Epoch 57: Val Loss=0.5275, Val Acc=15.81%
2025-03-02 16:12:11 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 16:12:11 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 16:12:16 - INFO - Epoch : 58 , Batch [ 0 / 93 ] : Loss = 0.488532, Accuracy = 35.16%, MSE = 1.6445
2025-03-02 16:12:16 - INFO - Classwise accuracy : (0: 95.70% (93)), (1: 0.00% (81)), (2: 1.22% (82))
2025-03-02 16:14:21 - INFO - Epoch : 58 , Batch [ 10 / 93 ] : Loss = 0.497838, Accuracy = 33.88%, MSE = 1.3612
2025-03-02 16:14:21 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (94)), (2: 0.00% (73))
2025-03-02 16:16:35 - INFO - Epoch : 58 , Batch [ 20 / 93 ] : Loss = 0.517030, Accuracy = 32.92%, MSE = 1.2701
2025-03-02 16:16:35 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (69)), (2: 0.00% (95))
2025-03-02 16:18:49 - INFO - Epoch : 58 , Batch [ 30 / 93 ] : Loss = 0.491008, Accuracy = 32.80%, MSE = 1.3003
2025-03-02 16:18:49 - INFO - Classwise accuracy : (0: 93.33% (75)), (1: 1.09% (92)), (2: 11.24% (89))
2025-03-02 16:21:28 - INFO - Epoch : 58 , Batch [ 40 / 93 ] : Loss = 0.494380, Accuracy = 32.96%, MSE = 1.3300
2025-03-02 16:21:28 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (96)), (2: 100.00% (84))
2025-03-02 16:23:40 - INFO - Epoch : 58 , Batch [ 50 / 93 ] : Loss = 0.488326, Accuracy = 32.97%, MSE = 1.3156
2025-03-02 16:23:40 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (79)), (2: 100.00% (92))
2025-03-02 16:26:01 - INFO - Epoch : 58 , Batch [ 60 / 93 ] : Loss = 0.496496, Accuracy = 33.16%, MSE = 1.3389
2025-03-02 16:26:01 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (89)), (2: 100.00% (91))
2025-03-02 16:28:26 - INFO - Epoch : 58 , Batch [ 70 / 93 ] : Loss = 0.525300, Accuracy = 33.13%, MSE = 1.3278
2025-03-02 16:28:26 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 0.00% (81)), (2: 100.00% (89))
2025-03-02 16:30:49 - INFO - Epoch : 58 , Batch [ 80 / 93 ] : Loss = 1.093416, Accuracy = 33.21%, MSE = 1.3126
2025-03-02 16:30:49 - INFO - Classwise accuracy : (0: 0.00% (67)), (1: 87.10% (93)), (2: 18.75% (96))
2025-03-02 16:33:23 - INFO - Epoch : 58 , Batch [ 90 / 93 ] : Loss = 1.257856, Accuracy = 33.30%, MSE = 1.3248
2025-03-02 16:33:23 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 0.00% (108)), (2: 100.00% (76))
2025-03-02 16:33:51 - INFO - Epoch 58: Train Loss=0.7329, Train Acc=33.31%, Train MSE=1.3229
2025-03-02 16:34:07 - INFO - Epoch 58: Val Loss=2.3796, Val Acc=16.19%
2025-03-02 16:34:07 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 16:34:07 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 16:34:11 - INFO - Epoch : 59 , Batch [ 0 / 93 ] : Loss = 1.564811, Accuracy = 33.98%, MSE = 1.6328
2025-03-02 16:34:11 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (86)), (2: 100.00% (87))
2025-03-02 16:36:20 - INFO - Epoch : 59 , Batch [ 10 / 93 ] : Loss = 0.748596, Accuracy = 32.32%, MSE = 1.1040
2025-03-02 16:36:20 - INFO - Classwise accuracy : (0: 0.00% (96)), (1: 100.00% (79)), (2: 0.00% (81))
2025-03-02 16:38:30 - INFO - Epoch : 59 , Batch [ 20 / 93 ] : Loss = 0.613088, Accuracy = 33.02%, MSE = 1.1531
2025-03-02 16:38:30 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (76)), (2: 0.00% (88))
2025-03-02 16:40:46 - INFO - Epoch : 59 , Batch [ 30 / 93 ] : Loss = 0.535415, Accuracy = 33.25%, MSE = 1.1918
2025-03-02 16:40:46 - INFO - Classwise accuracy : (0: 0.00% (100)), (1: 89.61% (77)), (2: 15.19% (79))
2025-03-02 16:42:58 - INFO - Epoch : 59 , Batch [ 40 / 93 ] : Loss = 0.493253, Accuracy = 32.87%, MSE = 1.2304
2025-03-02 16:42:58 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 100.00% (83)), (2: 0.00% (89))
2025-03-02 16:45:28 - INFO - Epoch : 59 , Batch [ 50 / 93 ] : Loss = 0.500864, Accuracy = 33.04%, MSE = 1.2360
2025-03-02 16:45:28 - INFO - Classwise accuracy : (0: 60.26% (78)), (1: 34.41% (93)), (2: 0.00% (85))
2025-03-02 16:48:13 - INFO - Epoch : 59 , Batch [ 60 / 93 ] : Loss = 0.490019, Accuracy = 33.19%, MSE = 1.2512
2025-03-02 16:48:13 - INFO - Classwise accuracy : (0: 93.55% (93)), (1: 6.58% (76)), (2: 0.00% (87))
2025-03-02 16:50:31 - INFO - Epoch : 59 , Batch [ 70 / 93 ] : Loss = 0.489693, Accuracy = 33.10%, MSE = 1.2625
2025-03-02 16:50:31 - INFO - Classwise accuracy : (0: 75.31% (81)), (1: 24.47% (94)), (2: 0.00% (81))
2025-03-02 16:53:06 - INFO - Epoch : 59 , Batch [ 80 / 93 ] : Loss = 0.488273, Accuracy = 33.06%, MSE = 1.2479
2025-03-02 16:53:06 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 89.58% (96)), (2: 5.06% (79))
2025-03-02 16:55:35 - INFO - Epoch : 59 , Batch [ 90 / 93 ] : Loss = 0.488116, Accuracy = 33.12%, MSE = 1.2267
2025-03-02 16:55:35 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 78.49% (93)), (2: 26.51% (83))
2025-03-02 16:56:03 - INFO - Epoch 59: Train Loss=0.5545, Train Acc=33.11%, Train MSE=1.2274
2025-03-02 16:56:19 - INFO - Epoch 59: Val Loss=0.4966, Val Acc=16.19%
2025-03-02 16:56:19 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 16:56:19 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 16:56:24 - INFO - Epoch : 60 , Batch [ 0 / 93 ] : Loss = 0.496214, Accuracy = 29.69%, MSE = 1.7930
2025-03-02 16:56:24 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 2.25% (89)), (2: 100.00% (74))
2025-03-02 16:58:34 - INFO - Epoch : 60 , Batch [ 10 / 93 ] : Loss = 0.488242, Accuracy = 34.69%, MSE = 1.1868
2025-03-02 16:58:34 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 0.00% (70)), (2: 100.00% (98))
2025-03-02 17:00:44 - INFO - Epoch : 60 , Batch [ 20 / 93 ] : Loss = 0.488411, Accuracy = 34.54%, MSE = 1.1496
2025-03-02 17:00:44 - INFO - Classwise accuracy : (0: 86.32% (95)), (1: 0.00% (87)), (2: 12.16% (74))
2025-03-02 17:02:50 - INFO - Epoch : 60 , Batch [ 30 / 93 ] : Loss = 0.488824, Accuracy = 34.45%, MSE = 1.2286
2025-03-02 17:02:50 - INFO - Classwise accuracy : (0: 81.48% (81)), (1: 0.00% (85)), (2: 21.11% (90))
2025-03-02 17:05:29 - INFO - Epoch : 60 , Batch [ 40 / 93 ] : Loss = 0.494868, Accuracy = 34.16%, MSE = 1.2427
2025-03-02 17:05:29 - INFO - Classwise accuracy : (0: 98.67% (75)), (1: 0.00% (90)), (2: 0.00% (91))
2025-03-02 17:08:13 - INFO - Epoch : 60 , Batch [ 50 / 93 ] : Loss = 0.500674, Accuracy = 33.94%, MSE = 1.2509
2025-03-02 17:08:13 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 100.00% (90)), (2: 0.00% (77))
2025-03-02 17:10:49 - INFO - Epoch : 60 , Batch [ 60 / 93 ] : Loss = 0.492232, Accuracy = 33.63%, MSE = 1.2704
2025-03-02 17:10:49 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (91)), (2: 0.00% (87))
2025-03-02 17:13:24 - INFO - Epoch : 60 , Batch [ 70 / 93 ] : Loss = 0.489277, Accuracy = 33.70%, MSE = 1.2674
2025-03-02 17:13:24 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (91)), (2: 0.00% (79))
2025-03-02 17:16:13 - INFO - Epoch : 60 , Batch [ 80 / 93 ] : Loss = 0.492756, Accuracy = 33.58%, MSE = 1.3085
2025-03-02 17:16:13 - INFO - Classwise accuracy : (0: 87.34% (79)), (1: 12.50% (88)), (2: 0.00% (89))
2025-03-02 17:18:46 - INFO - Epoch : 60 , Batch [ 90 / 93 ] : Loss = 0.496536, Accuracy = 33.76%, MSE = 1.3161
2025-03-02 17:18:46 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (83)), (2: 100.00% (80))
2025-03-02 17:19:16 - INFO - Epoch 60: Train Loss=0.4923, Train Acc=33.73%, Train MSE=1.3290
2025-03-02 17:19:31 - INFO - Epoch 60: Val Loss=0.5525, Val Acc=16.19%
2025-03-02 17:19:31 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 17:19:31 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 17:19:36 - INFO - Epoch : 61 , Batch [ 0 / 93 ] : Loss = 0.518725, Accuracy = 33.59%, MSE = 1.6367
2025-03-02 17:19:36 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (87)), (2: 100.00% (86))
2025-03-02 17:21:58 - INFO - Epoch : 61 , Batch [ 10 / 93 ] : Loss = 0.490140, Accuracy = 33.52%, MSE = 1.5692
2025-03-02 17:21:58 - INFO - Classwise accuracy : (0: 89.41% (85)), (1: 0.00% (92)), (2: 7.59% (79))
2025-03-02 17:24:58 - INFO - Epoch : 61 , Batch [ 20 / 93 ] : Loss = 1.016276, Accuracy = 33.63%, MSE = 1.3757
2025-03-02 17:24:58 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (107)), (2: 0.00% (73))
2025-03-02 17:27:34 - INFO - Epoch : 61 , Batch [ 30 / 93 ] : Loss = 1.167093, Accuracy = 33.88%, MSE = 1.3250
2025-03-02 17:27:34 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (93)), (2: 0.00% (86))
2025-03-02 17:30:14 - INFO - Epoch : 61 , Batch [ 40 / 93 ] : Loss = 0.733042, Accuracy = 34.15%, MSE = 1.3191
2025-03-02 17:30:14 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (81)), (2: 100.00% (99))
2025-03-02 17:33:04 - INFO - Epoch : 61 , Batch [ 50 / 93 ] : Loss = 0.542915, Accuracy = 34.02%, MSE = 1.3297
2025-03-02 17:33:04 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (87)), (2: 100.00% (90))
2025-03-02 17:35:53 - INFO - Epoch : 61 , Batch [ 60 / 93 ] : Loss = 0.500703, Accuracy = 34.18%, MSE = 1.2931
2025-03-02 17:35:53 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (94)), (2: 0.00% (76))
2025-03-02 17:38:40 - INFO - Epoch : 61 , Batch [ 70 / 93 ] : Loss = 0.504075, Accuracy = 34.07%, MSE = 1.3153
2025-03-02 17:38:40 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (85)), (2: 0.00% (87))
2025-03-02 17:41:16 - INFO - Epoch : 61 , Batch [ 80 / 93 ] : Loss = 0.519414, Accuracy = 34.07%, MSE = 1.3253
2025-03-02 17:41:16 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (81)), (2: 0.00% (85))
2025-03-02 17:44:18 - INFO - Epoch : 61 , Batch [ 90 / 93 ] : Loss = 0.490156, Accuracy = 33.92%, MSE = 1.3511
2025-03-02 17:44:18 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 1.11% (90)), (2: 0.00% (84))
2025-03-02 17:44:48 - INFO - Epoch 61: Train Loss=0.7778, Train Acc=33.86%, Train MSE=1.3584
2025-03-02 17:45:04 - INFO - Epoch 61: Val Loss=0.4676, Val Acc=16.19%
2025-03-02 17:45:04 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 17:45:04 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 17:45:09 - INFO - Epoch : 62 , Batch [ 0 / 93 ] : Loss = 0.493131, Accuracy = 32.03%, MSE = 1.5586
2025-03-02 17:45:09 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 4.35% (92)), (2: 93.98% (83))
2025-03-02 17:47:26 - INFO - Epoch : 62 , Batch [ 10 / 93 ] : Loss = 0.486027, Accuracy = 32.42%, MSE = 1.3672
2025-03-02 17:47:26 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 15.00% (80)), (2: 76.92% (104))
2025-03-02 17:49:47 - INFO - Epoch : 62 , Batch [ 20 / 93 ] : Loss = 0.489638, Accuracy = 32.16%, MSE = 1.4580
2025-03-02 17:49:47 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 23.33% (90)), (2: 79.76% (84))
2025-03-02 17:51:58 - INFO - Epoch : 62 , Batch [ 30 / 93 ] : Loss = 0.495651, Accuracy = 33.22%, MSE = 1.3615
2025-03-02 17:51:58 - INFO - Classwise accuracy : (0: 86.90% (84)), (1: 0.00% (87)), (2: 7.06% (85))
2025-03-02 17:54:27 - INFO - Epoch : 62 , Batch [ 40 / 93 ] : Loss = 0.489160, Accuracy = 33.56%, MSE = 1.2890
2025-03-02 17:54:27 - INFO - Classwise accuracy : (0: 28.89% (90)), (1: 0.00% (80)), (2: 72.09% (86))
2025-03-02 17:57:27 - INFO - Epoch : 62 , Batch [ 50 / 93 ] : Loss = 0.487749, Accuracy = 33.84%, MSE = 1.3291
2025-03-02 17:57:27 - INFO - Classwise accuracy : (0: 1.25% (80)), (1: 98.90% (91)), (2: 2.35% (85))
2025-03-02 18:00:29 - INFO - Epoch : 62 , Batch [ 60 / 93 ] : Loss = 0.489336, Accuracy = 33.57%, MSE = 1.3336
2025-03-02 18:00:29 - INFO - Classwise accuracy : (0: 79.78% (89)), (1: 19.23% (78)), (2: 0.00% (89))
2025-03-02 18:03:15 - INFO - Epoch : 62 , Batch [ 70 / 93 ] : Loss = 0.489656, Accuracy = 33.68%, MSE = 1.3254
2025-03-02 18:03:15 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (87)), (2: 0.00% (83))
2025-03-02 18:05:59 - INFO - Epoch : 62 , Batch [ 80 / 93 ] : Loss = 0.489035, Accuracy = 33.57%, MSE = 1.3192
2025-03-02 18:05:59 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 76.24% (101)), (2: 14.06% (64))
2025-03-02 18:08:50 - INFO - Epoch : 62 , Batch [ 90 / 93 ] : Loss = 0.489157, Accuracy = 33.54%, MSE = 1.2993
2025-03-02 18:08:50 - INFO - Classwise accuracy : (0: 36.90% (84)), (1: 66.27% (83)), (2: 0.00% (89))
2025-03-02 18:09:21 - INFO - Epoch 62: Train Loss=0.4907, Train Acc=33.56%, Train MSE=1.2860
2025-03-02 18:09:37 - INFO - Epoch 62: Val Loss=0.4873, Val Acc=16.19%
2025-03-02 18:09:37 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 18:09:37 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 18:09:43 - INFO - Epoch : 63 , Batch [ 0 / 93 ] : Loss = 0.498096, Accuracy = 32.03%, MSE = 1.7227
2025-03-02 18:09:43 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (85)), (2: 100.00% (82))
2025-03-02 18:11:55 - INFO - Epoch : 63 , Batch [ 10 / 93 ] : Loss = 0.504619, Accuracy = 33.63%, MSE = 1.4329
2025-03-02 18:11:55 - INFO - Classwise accuracy : (0: 82.14% (84)), (1: 0.00% (104)), (2: 19.12% (68))
2025-03-02 18:14:33 - INFO - Epoch : 63 , Batch [ 20 / 93 ] : Loss = 0.495047, Accuracy = 33.72%, MSE = 1.3754
2025-03-02 18:14:33 - INFO - Classwise accuracy : (0: 2.50% (80)), (1: 100.00% (87)), (2: 0.00% (89))
2025-03-02 18:17:26 - INFO - Epoch : 63 , Batch [ 30 / 93 ] : Loss = 0.493902, Accuracy = 33.90%, MSE = 1.2912
2025-03-02 18:17:26 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 97.59% (83)), (2: 0.00% (83))
2025-03-02 18:20:22 - INFO - Epoch : 63 , Batch [ 40 / 93 ] : Loss = 0.487766, Accuracy = 33.42%, MSE = 1.3778
2025-03-02 18:20:22 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 20.99% (81)), (2: 81.91% (94))
2025-03-02 18:23:05 - INFO - Epoch : 63 , Batch [ 50 / 93 ] : Loss = 0.490796, Accuracy = 33.67%, MSE = 1.3542
2025-03-02 18:23:05 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (95)), (2: 0.00% (82))
2025-03-02 18:25:55 - INFO - Epoch : 63 , Batch [ 60 / 93 ] : Loss = 0.488888, Accuracy = 33.67%, MSE = 1.3343
2025-03-02 18:25:55 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (93)), (2: 0.00% (73))
2025-03-02 18:28:30 - INFO - Epoch : 63 , Batch [ 70 / 93 ] : Loss = 0.524856, Accuracy = 33.29%, MSE = 1.3159
2025-03-02 18:28:30 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 91.36% (81)), (2: 8.05% (87))
2025-03-02 18:31:07 - INFO - Epoch : 63 , Batch [ 80 / 93 ] : Loss = 0.516743, Accuracy = 33.14%, MSE = 1.3410
2025-03-02 18:31:07 - INFO - Classwise accuracy : (0: 100.00% (74)), (1: 0.00% (87)), (2: 0.00% (95))
2025-03-02 18:33:58 - INFO - Epoch : 63 , Batch [ 90 / 93 ] : Loss = 0.498249, Accuracy = 33.08%, MSE = 1.3443
2025-03-02 18:33:58 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (98)), (2: 98.73% (79))
2025-03-02 18:34:25 - INFO - Epoch 63: Train Loss=0.4929, Train Acc=33.10%, Train MSE=1.3364
2025-03-02 18:34:41 - INFO - Epoch 63: Val Loss=0.5782, Val Acc=15.81%
2025-03-02 18:34:41 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 18:34:41 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 18:34:46 - INFO - Epoch : 64 , Batch [ 0 / 93 ] : Loss = 0.589594, Accuracy = 25.00%, MSE = 1.8398
2025-03-02 18:34:46 - INFO - Classwise accuracy : (0: 100.00% (64)), (1: 0.00% (99)), (2: 0.00% (93))
2025-03-02 18:36:52 - INFO - Epoch : 64 , Batch [ 10 / 93 ] : Loss = 3.917153, Accuracy = 33.06%, MSE = 1.4812
2025-03-02 18:36:52 - INFO - Classwise accuracy : (0: 100.00% (96)), (1: 0.00% (62)), (2: 0.00% (98))
2025-03-02 18:39:08 - INFO - Epoch : 64 , Batch [ 20 / 93 ] : Loss = 4.407473, Accuracy = 32.98%, MSE = 1.3320
2025-03-02 18:39:08 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (85)), (2: 0.00% (84))
2025-03-02 18:41:35 - INFO - Epoch : 64 , Batch [ 30 / 93 ] : Loss = 2.444283, Accuracy = 32.91%, MSE = 1.3177
2025-03-02 18:41:35 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (89)), (2: 100.00% (83))
2025-03-02 18:43:50 - INFO - Epoch : 64 , Batch [ 40 / 93 ] : Loss = 0.698071, Accuracy = 32.97%, MSE = 1.3102
2025-03-02 18:43:50 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (87)), (2: 100.00% (93))
2025-03-02 18:46:07 - INFO - Epoch : 64 , Batch [ 50 / 93 ] : Loss = 0.579416, Accuracy = 33.08%, MSE = 1.3353
2025-03-02 18:46:07 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (83)), (2: 0.00% (88))
2025-03-02 18:48:30 - INFO - Epoch : 64 , Batch [ 60 / 93 ] : Loss = 0.520633, Accuracy = 33.33%, MSE = 1.3470
2025-03-02 18:48:30 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 0.00% (79)), (2: 100.00% (103))
2025-03-02 18:50:59 - INFO - Epoch : 64 , Batch [ 70 / 93 ] : Loss = 0.493342, Accuracy = 33.54%, MSE = 1.3192
2025-03-02 18:50:59 - INFO - Classwise accuracy : (0: 40.43% (94)), (1: 0.00% (80)), (2: 60.98% (82))
2025-03-02 18:53:21 - INFO - Epoch : 64 , Batch [ 80 / 93 ] : Loss = 0.503435, Accuracy = 33.76%, MSE = 1.2720
2025-03-02 18:53:21 - INFO - Classwise accuracy : (0: 1.32% (76)), (1: 97.65% (85)), (2: 0.00% (95))
2025-03-02 18:55:59 - INFO - Epoch : 64 , Batch [ 90 / 93 ] : Loss = 0.489035, Accuracy = 33.69%, MSE = 1.2608
2025-03-02 18:55:59 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 91.46% (82)), (2: 10.59% (85))
2025-03-02 18:56:25 - INFO - Epoch 64: Train Loss=1.1045, Train Acc=33.65%, Train MSE=1.2513
2025-03-02 18:56:41 - INFO - Epoch 64: Val Loss=0.5475, Val Acc=16.19%
2025-03-02 18:56:41 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 18:56:41 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 18:56:45 - INFO - Epoch : 65 , Batch [ 0 / 93 ] : Loss = 0.493812, Accuracy = 35.55%, MSE = 1.6289
2025-03-02 18:56:45 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (81)), (2: 100.00% (91))
2025-03-02 18:59:07 - INFO - Epoch : 65 , Batch [ 10 / 93 ] : Loss = 0.492829, Accuracy = 33.91%, MSE = 1.5046
2025-03-02 18:59:07 - INFO - Classwise accuracy : (0: 80.00% (90)), (1: 0.00% (94)), (2: 25.00% (72))
2025-03-02 19:01:24 - INFO - Epoch : 65 , Batch [ 20 / 93 ] : Loss = 0.491905, Accuracy = 33.35%, MSE = 1.4215
2025-03-02 19:01:24 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (96)), (2: 100.00% (82))
2025-03-02 19:03:47 - INFO - Epoch : 65 , Batch [ 30 / 93 ] : Loss = 0.494390, Accuracy = 33.77%, MSE = 1.3231
2025-03-02 19:03:47 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 80.82% (73)), (2: 19.57% (92))
2025-03-02 19:06:30 - INFO - Epoch : 65 , Batch [ 40 / 93 ] : Loss = 0.488537, Accuracy = 33.54%, MSE = 1.3306
2025-03-02 19:06:30 - INFO - Classwise accuracy : (0: 15.91% (88)), (1: 3.75% (80)), (2: 70.45% (88))
2025-03-02 19:08:52 - INFO - Epoch : 65 , Batch [ 50 / 93 ] : Loss = 0.488466, Accuracy = 33.87%, MSE = 1.2206
2025-03-02 19:08:52 - INFO - Classwise accuracy : (0: 85.26% (95)), (1: 14.47% (76)), (2: 1.18% (85))
2025-03-02 19:10:59 - INFO - Epoch : 65 , Batch [ 60 / 93 ] : Loss = 0.487722, Accuracy = 33.65%, MSE = 1.2302
2025-03-02 19:10:59 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 100.00% (89)), (2: 0.00% (92))
2025-03-02 19:12:56 - INFO - Epoch : 65 , Batch [ 70 / 93 ] : Loss = 0.486787, Accuracy = 33.47%, MSE = 1.2477
2025-03-02 19:12:56 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 0.00% (96)), (2: 100.00% (89))
2025-03-02 19:14:56 - INFO - Epoch : 65 , Batch [ 80 / 93 ] : Loss = 0.489238, Accuracy = 33.36%, MSE = 1.2482
2025-03-02 19:14:56 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 100.00% (84)), (2: 0.00% (88))
2025-03-02 19:16:47 - INFO - Epoch : 65 , Batch [ 90 / 93 ] : Loss = 0.487968, Accuracy = 33.29%, MSE = 1.2412
2025-03-02 19:16:47 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 0.00% (82)), (2: 100.00% (103))
2025-03-02 19:17:10 - INFO - Epoch 65: Train Loss=0.4915, Train Acc=33.34%, Train MSE=1.2462
2025-03-02 19:17:26 - INFO - Epoch 65: Val Loss=0.5036, Val Acc=16.19%
2025-03-02 19:17:26 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 19:17:26 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 19:17:31 - INFO - Epoch : 66 , Batch [ 0 / 93 ] : Loss = 0.500242, Accuracy = 33.59%, MSE = 1.7070
2025-03-02 19:17:31 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 0.00% (81)), (2: 100.00% (86))
2025-03-02 19:19:54 - INFO - Epoch : 66 , Batch [ 10 / 93 ] : Loss = 0.490390, Accuracy = 33.20%, MSE = 1.4585
2025-03-02 19:19:54 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 7.87% (89)), (2: 89.33% (75))
2025-03-02 19:22:36 - INFO - Epoch : 66 , Batch [ 20 / 93 ] : Loss = 0.491943, Accuracy = 33.59%, MSE = 1.3186
2025-03-02 19:22:36 - INFO - Classwise accuracy : (0: 19.28% (83)), (1: 0.00% (92)), (2: 82.72% (81))
2025-03-02 19:25:36 - INFO - Epoch : 66 , Batch [ 30 / 93 ] : Loss = 0.491916, Accuracy = 33.76%, MSE = 1.2453
2025-03-02 19:25:36 - INFO - Classwise accuracy : (0: 0.00% (100)), (1: 100.00% (76)), (2: 0.00% (80))
2025-03-02 19:28:20 - INFO - Epoch : 66 , Batch [ 40 / 93 ] : Loss = 0.488457, Accuracy = 33.44%, MSE = 1.2710
2025-03-02 19:28:20 - INFO - Classwise accuracy : (0: 97.70% (87)), (1: 1.15% (87)), (2: 0.00% (82))
2025-03-02 19:31:05 - INFO - Epoch : 66 , Batch [ 50 / 93 ] : Loss = 0.488288, Accuracy = 33.27%, MSE = 1.2695
2025-03-02 19:31:05 - INFO - Classwise accuracy : (0: 10.45% (67)), (1: 6.60% (106)), (2: 72.29% (83))
2025-03-02 19:33:50 - INFO - Epoch : 66 , Batch [ 60 / 93 ] : Loss = 0.491014, Accuracy = 33.06%, MSE = 1.3003
2025-03-02 19:33:50 - INFO - Classwise accuracy : (0: 100.00% (78)), (1: 0.00% (90)), (2: 0.00% (88))
2025-03-02 19:36:14 - INFO - Epoch : 66 , Batch [ 70 / 93 ] : Loss = 0.490221, Accuracy = 33.25%, MSE = 1.2844
2025-03-02 19:36:14 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (88)), (2: 0.00% (81))
2025-03-02 19:38:31 - INFO - Epoch : 66 , Batch [ 80 / 93 ] : Loss = 0.492820, Accuracy = 33.25%, MSE = 1.2989
2025-03-02 19:38:31 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (80)), (2: 0.00% (88))
2025-03-02 19:40:45 - INFO - Epoch : 66 , Batch [ 90 / 93 ] : Loss = 0.480140, Accuracy = 33.33%, MSE = 1.2968
2025-03-02 19:40:45 - INFO - Classwise accuracy : (0: 0.00% (71)), (1: 100.00% (109)), (2: 0.00% (76))
2025-03-02 19:41:15 - INFO - Epoch 66: Train Loss=0.4950, Train Acc=33.33%, Train MSE=1.2942
2025-03-02 19:41:31 - INFO - Epoch 66: Val Loss=0.6315, Val Acc=16.19%
2025-03-02 19:41:31 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 19:41:31 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 19:41:36 - INFO - Epoch : 67 , Batch [ 0 / 93 ] : Loss = 0.503349, Accuracy = 35.94%, MSE = 1.6250
2025-03-02 19:41:36 - INFO - Classwise accuracy : (0: 9.30% (86)), (1: 0.00% (80)), (2: 93.33% (90))
2025-03-02 19:43:48 - INFO - Epoch : 67 , Batch [ 10 / 93 ] : Loss = 0.561760, Accuracy = 33.98%, MSE = 1.3153
2025-03-02 19:43:48 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (86)), (2: 0.00% (83))
2025-03-02 19:46:14 - INFO - Epoch : 67 , Batch [ 20 / 93 ] : Loss = 0.491766, Accuracy = 34.28%, MSE = 1.2420
2025-03-02 19:46:14 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (92)), (2: 0.00% (80))
2025-03-02 19:48:55 - INFO - Epoch : 67 , Batch [ 30 / 93 ] : Loss = 0.494433, Accuracy = 34.11%, MSE = 1.2263
2025-03-02 19:48:55 - INFO - Classwise accuracy : (0: 0.00% (100)), (1: 100.00% (99)), (2: 0.00% (57))
2025-03-02 19:51:46 - INFO - Epoch : 67 , Batch [ 40 / 93 ] : Loss = 0.500028, Accuracy = 34.05%, MSE = 1.2066
2025-03-02 19:51:46 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (75)), (2: 0.00% (88))
2025-03-02 19:54:29 - INFO - Epoch : 67 , Batch [ 50 / 93 ] : Loss = 0.489860, Accuracy = 34.11%, MSE = 1.2207
2025-03-02 19:54:29 - INFO - Classwise accuracy : (0: 8.14% (86)), (1: 0.00% (83)), (2: 94.25% (87))
2025-03-02 19:57:16 - INFO - Epoch : 67 , Batch [ 60 / 93 ] : Loss = 0.520868, Accuracy = 33.76%, MSE = 1.2737
2025-03-02 19:57:16 - INFO - Classwise accuracy : (0: 100.00% (75)), (1: 0.00% (81)), (2: 0.00% (100))
2025-03-02 20:00:06 - INFO - Epoch : 67 , Batch [ 70 / 93 ] : Loss = 0.490876, Accuracy = 33.84%, MSE = 1.2792
2025-03-02 20:00:06 - INFO - Classwise accuracy : (0: 0.00% (68)), (1: 100.00% (94)), (2: 0.00% (94))
2025-03-02 20:06:33 - INFO - Epoch : 67 , Batch [ 80 / 93 ] : Loss = 0.493879, Accuracy = 33.89%, MSE = 1.2942
2025-03-02 20:06:33 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (84)), (2: 0.00% (85))
2025-03-02 20:35:32 - INFO - Epoch : 67 , Batch [ 90 / 93 ] : Loss = 0.603588, Accuracy = 33.86%, MSE = 1.3032
2025-03-02 20:35:32 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (83)), (2: 0.00% (96))
2025-03-02 20:36:01 - INFO - Epoch 67: Train Loss=0.5161, Train Acc=33.83%, Train MSE=1.3118
2025-03-02 20:36:17 - INFO - Epoch 67: Val Loss=0.3747, Val Acc=68.00%
2025-03-02 20:36:17 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 20:36:17 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 20:36:22 - INFO - Epoch : 68 , Batch [ 0 / 93 ] : Loss = 0.594851, Accuracy = 38.67%, MSE = 0.6133
2025-03-02 20:36:22 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 100.00% (99)), (2: 0.00% (83))
2025-03-02 20:38:13 - INFO - Epoch : 68 , Batch [ 10 / 93 ] : Loss = 0.599112, Accuracy = 35.58%, MSE = 1.2205
2025-03-02 20:38:13 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 100.00% (95)), (2: 0.00% (76))
2025-03-02 20:40:17 - INFO - Epoch : 68 , Batch [ 20 / 93 ] : Loss = 0.505800, Accuracy = 34.10%, MSE = 1.3510
2025-03-02 20:40:17 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (73)), (2: 98.89% (90))
2025-03-02 20:42:25 - INFO - Epoch : 68 , Batch [ 30 / 93 ] : Loss = 0.516045, Accuracy = 33.61%, MSE = 1.3690
2025-03-02 20:42:25 - INFO - Classwise accuracy : (0: 68.35% (79)), (1: 0.00% (89)), (2: 26.14% (88))
2025-03-02 20:44:49 - INFO - Epoch : 68 , Batch [ 40 / 93 ] : Loss = 0.500258, Accuracy = 33.49%, MSE = 1.3565
2025-03-02 20:44:49 - INFO - Classwise accuracy : (0: 17.14% (70)), (1: 0.00% (92)), (2: 73.40% (94))
2025-03-02 20:47:38 - INFO - Epoch : 68 , Batch [ 50 / 93 ] : Loss = 0.517790, Accuracy = 33.39%, MSE = 1.3045
2025-03-02 20:47:38 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 87.65% (81)), (2: 7.14% (84))
2025-03-02 20:50:15 - INFO - Epoch : 68 , Batch [ 60 / 93 ] : Loss = 0.490356, Accuracy = 33.50%, MSE = 1.2969
2025-03-02 20:50:15 - INFO - Classwise accuracy : (0: 2.50% (80)), (1: 0.00% (91)), (2: 95.29% (85))
2025-03-02 20:52:33 - INFO - Epoch : 68 , Batch [ 70 / 93 ] : Loss = 0.489538, Accuracy = 33.44%, MSE = 1.2687
2025-03-02 20:52:33 - INFO - Classwise accuracy : (0: 98.63% (73)), (1: 0.00% (84)), (2: 5.05% (99))
2025-03-02 20:54:57 - INFO - Epoch : 68 , Batch [ 80 / 93 ] : Loss = 0.491254, Accuracy = 33.50%, MSE = 1.2874
2025-03-02 20:54:57 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (86)), (2: 98.89% (90))
2025-03-02 20:57:18 - INFO - Epoch : 68 , Batch [ 90 / 93 ] : Loss = 0.490564, Accuracy = 33.52%, MSE = 1.2812
2025-03-02 20:57:18 - INFO - Classwise accuracy : (0: 70.97% (93)), (1: 0.00% (81)), (2: 34.15% (82))
2025-03-02 20:57:46 - INFO - Epoch 68: Train Loss=0.5433, Train Acc=33.49%, Train MSE=1.2920
2025-03-02 20:58:02 - INFO - Epoch 68: Val Loss=0.4601, Val Acc=68.00%
2025-03-02 20:58:02 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 20:58:02 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 20:58:07 - INFO - Epoch : 69 , Batch [ 0 / 93 ] : Loss = 0.486331, Accuracy = 39.06%, MSE = 0.6094
2025-03-02 20:58:07 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (100)), (2: 0.00% (73))
2025-03-02 21:00:06 - INFO - Epoch : 69 , Batch [ 10 / 93 ] : Loss = 0.490375, Accuracy = 33.88%, MSE = 1.0053
2025-03-02 21:00:06 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (83)), (2: 0.00% (96))
2025-03-02 21:02:24 - INFO - Epoch : 69 , Batch [ 20 / 93 ] : Loss = 0.487435, Accuracy = 34.11%, MSE = 1.1097
2025-03-02 21:02:24 - INFO - Classwise accuracy : (0: 90.91% (99)), (1: 0.00% (80)), (2: 7.79% (77))
2025-03-02 21:04:34 - INFO - Epoch : 69 , Batch [ 30 / 93 ] : Loss = 0.488304, Accuracy = 33.40%, MSE = 1.2160
2025-03-02 21:04:34 - INFO - Classwise accuracy : (0: 12.05% (83)), (1: 6.98% (86)), (2: 89.66% (87))
2025-03-02 21:06:48 - INFO - Epoch : 69 , Batch [ 40 / 93 ] : Loss = 0.486176, Accuracy = 33.69%, MSE = 1.1436
2025-03-02 21:06:48 - INFO - Classwise accuracy : (0: 27.27% (88)), (1: 0.00% (75)), (2: 74.19% (93))
2025-03-02 21:09:05 - INFO - Epoch : 69 , Batch [ 50 / 93 ] : Loss = 0.491196, Accuracy = 33.62%, MSE = 1.2107
2025-03-02 21:09:05 - INFO - Classwise accuracy : (0: 87.84% (74)), (1: 0.00% (86)), (2: 8.33% (96))
2025-03-02 21:11:31 - INFO - Epoch : 69 , Batch [ 60 / 93 ] : Loss = 0.487803, Accuracy = 33.72%, MSE = 1.1850
2025-03-02 21:11:31 - INFO - Classwise accuracy : (0: 1.25% (80)), (1: 98.94% (94)), (2: 0.00% (82))
2025-03-02 21:14:17 - INFO - Epoch : 69 , Batch [ 70 / 93 ] : Loss = 0.487077, Accuracy = 33.69%, MSE = 1.2079
2025-03-02 21:14:17 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 94.90% (98)), (2: 0.00% (81))
2025-03-02 21:16:38 - INFO - Epoch : 69 , Batch [ 80 / 93 ] : Loss = 0.492269, Accuracy = 33.48%, MSE = 1.2145
2025-03-02 21:16:38 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 96.39% (83)), (2: 0.00% (90))
2025-03-02 21:19:09 - INFO - Epoch : 69 , Batch [ 90 / 93 ] : Loss = 0.494090, Accuracy = 33.49%, MSE = 1.2336
2025-03-02 21:19:09 - INFO - Classwise accuracy : (0: 0.00% (96)), (1: 100.00% (77)), (2: 1.20% (83))
2025-03-02 21:19:34 - INFO - Epoch 69: Train Loss=0.4913, Train Acc=33.48%, Train MSE=1.2357
2025-03-02 21:19:50 - INFO - Epoch 69: Val Loss=0.5108, Val Acc=16.19%
2025-03-02 21:19:50 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 21:19:50 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 21:19:55 - INFO - Epoch : 70 , Batch [ 0 / 93 ] : Loss = 0.488227, Accuracy = 36.72%, MSE = 1.5000
2025-03-02 21:19:55 - INFO - Classwise accuracy : (0: 7.79% (77)), (1: 2.33% (86)), (2: 92.47% (93))
2025-03-02 21:22:15 - INFO - Epoch : 70 , Batch [ 10 / 93 ] : Loss = 0.505817, Accuracy = 35.16%, MSE = 1.3228
2025-03-02 21:22:15 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (102)), (2: 1.49% (67))
2025-03-02 21:24:32 - INFO - Epoch : 70 , Batch [ 20 / 93 ] : Loss = 0.495362, Accuracy = 34.39%, MSE = 1.2582
2025-03-02 21:24:32 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 0.00% (76)), (2: 100.00% (93))
2025-03-02 21:26:42 - INFO - Epoch : 70 , Batch [ 30 / 93 ] : Loss = 0.745733, Accuracy = 34.06%, MSE = 1.2272
2025-03-02 21:26:42 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (82)), (2: 0.00% (93))
2025-03-02 21:28:53 - INFO - Epoch : 70 , Batch [ 40 / 93 ] : Loss = 0.584893, Accuracy = 34.16%, MSE = 1.2209
2025-03-02 21:28:53 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (95)), (2: 0.00% (74))
2025-03-02 21:31:05 - INFO - Epoch : 70 , Batch [ 50 / 93 ] : Loss = 0.588477, Accuracy = 33.73%, MSE = 1.2174
2025-03-02 21:31:05 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (96)), (2: 100.00% (76))
2025-03-02 21:33:17 - INFO - Epoch : 70 , Batch [ 60 / 93 ] : Loss = 0.525434, Accuracy = 33.89%, MSE = 1.2286
2025-03-02 21:33:17 - INFO - Classwise accuracy : (0: 79.52% (83)), (1: 25.33% (75)), (2: 0.00% (98))
2025-03-02 21:35:43 - INFO - Epoch : 70 , Batch [ 70 / 93 ] : Loss = 0.498136, Accuracy = 33.86%, MSE = 1.2376
2025-03-02 21:35:43 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (88)), (2: 100.00% (91))
2025-03-02 21:38:06 - INFO - Epoch : 70 , Batch [ 80 / 93 ] : Loss = 0.496369, Accuracy = 33.68%, MSE = 1.2396
2025-03-02 21:38:06 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (80)), (2: 0.00% (84))
2025-03-02 21:40:19 - INFO - Epoch : 70 , Batch [ 90 / 93 ] : Loss = 0.497434, Accuracy = 33.50%, MSE = 1.2470
2025-03-02 21:40:19 - INFO - Classwise accuracy : (0: 0.00% (100)), (1: 98.63% (73)), (2: 0.00% (83))
2025-03-02 21:40:45 - INFO - Epoch 70: Train Loss=0.5277, Train Acc=33.51%, Train MSE=1.2417
2025-03-02 21:41:00 - INFO - Epoch 70: Val Loss=0.5244, Val Acc=16.19%
2025-03-02 21:41:01 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 21:41:01 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 21:41:05 - INFO - Epoch : 71 , Batch [ 0 / 93 ] : Loss = 0.500717, Accuracy = 29.69%, MSE = 1.6992
2025-03-02 21:41:05 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (95)), (2: 100.00% (76))
2025-03-02 21:43:31 - INFO - Epoch : 71 , Batch [ 10 / 93 ] : Loss = 0.488030, Accuracy = 32.32%, MSE = 1.6112
2025-03-02 21:43:31 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 23.00% (100)), (2: 73.08% (78))
2025-03-02 21:46:07 - INFO - Epoch : 71 , Batch [ 20 / 93 ] : Loss = 0.489828, Accuracy = 32.85%, MSE = 1.3451
2025-03-02 21:46:07 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (91)), (2: 0.00% (87))
2025-03-02 21:48:53 - INFO - Epoch : 71 , Batch [ 30 / 93 ] : Loss = 0.488936, Accuracy = 32.64%, MSE = 1.3817
2025-03-02 21:48:53 - INFO - Classwise accuracy : (0: 3.75% (80)), (1: 0.00% (91)), (2: 91.76% (85))
2025-03-02 21:51:31 - INFO - Epoch : 71 , Batch [ 40 / 93 ] : Loss = 0.491660, Accuracy = 32.40%, MSE = 1.4228
2025-03-02 21:51:31 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (85)), (2: 100.00% (80))
2025-03-02 21:54:24 - INFO - Epoch : 71 , Batch [ 50 / 93 ] : Loss = 0.502272, Accuracy = 32.38%, MSE = 1.3784
2025-03-02 21:54:24 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 100.00% (76)), (2: 0.00% (99))
2025-03-02 21:57:13 - INFO - Epoch : 71 , Batch [ 60 / 93 ] : Loss = 0.488657, Accuracy = 32.31%, MSE = 1.3829
2025-03-02 21:57:13 - INFO - Classwise accuracy : (0: 98.67% (75)), (1: 0.00% (81)), (2: 0.00% (100))
2025-03-02 21:59:57 - INFO - Epoch : 71 , Batch [ 70 / 93 ] : Loss = 0.488363, Accuracy = 32.64%, MSE = 1.3929
2025-03-02 21:59:57 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 100.00% (90)), (2: 0.00% (90))
2025-03-02 22:02:34 - INFO - Epoch : 71 , Batch [ 80 / 93 ] : Loss = 0.493972, Accuracy = 32.91%, MSE = 1.3882
2025-03-02 22:02:34 - INFO - Classwise accuracy : (0: 0.00% (98)), (1: 0.00% (73)), (2: 100.00% (85))
2025-03-02 22:05:35 - INFO - Epoch : 71 , Batch [ 90 / 93 ] : Loss = 0.561547, Accuracy = 32.95%, MSE = 1.3756
2025-03-02 22:05:35 - INFO - Classwise accuracy : (0: 0.00% (69)), (1: 100.00% (93)), (2: 0.00% (94))
2025-03-02 22:06:07 - INFO - Epoch 71: Train Loss=0.4979, Train Acc=32.96%, Train MSE=1.3765
2025-03-02 22:06:23 - INFO - Epoch 71: Val Loss=0.4081, Val Acc=68.00%
2025-03-02 22:06:23 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 22:06:23 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 22:06:27 - INFO - Epoch : 72 , Batch [ 0 / 93 ] : Loss = 0.809765, Accuracy = 30.86%, MSE = 0.6914
2025-03-02 22:06:27 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (79)), (2: 0.00% (89))
2025-03-02 22:08:24 - INFO - Epoch : 72 , Batch [ 10 / 93 ] : Loss = 0.765593, Accuracy = 33.03%, MSE = 1.2940
2025-03-02 22:08:24 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (78)), (2: 0.00% (86))
2025-03-02 22:11:03 - INFO - Epoch : 72 , Batch [ 20 / 93 ] : Loss = 0.517968, Accuracy = 32.98%, MSE = 1.3761
2025-03-02 22:11:03 - INFO - Classwise accuracy : (0: 0.00% (76)), (1: 0.00% (91)), (2: 100.00% (89))
2025-03-02 22:13:44 - INFO - Epoch : 72 , Batch [ 30 / 93 ] : Loss = 0.521962, Accuracy = 32.99%, MSE = 1.3124
2025-03-02 22:13:44 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 0.00% (83)), (2: 100.00% (81))
2025-03-02 22:16:25 - INFO - Epoch : 72 , Batch [ 40 / 93 ] : Loss = 0.525083, Accuracy = 33.12%, MSE = 1.3202
2025-03-02 22:16:25 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 0.00% (94)), (2: 100.00% (85))
2025-03-02 22:18:52 - INFO - Epoch : 72 , Batch [ 50 / 93 ] : Loss = 0.518489, Accuracy = 33.03%, MSE = 1.3244
2025-03-02 22:18:52 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 100.00% (73)), (2: 0.00% (96))
2025-03-02 22:21:18 - INFO - Epoch : 72 , Batch [ 60 / 93 ] : Loss = 0.490995, Accuracy = 33.23%, MSE = 1.2759
2025-03-02 22:21:18 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 98.88% (89)), (2: 3.95% (76))
2025-03-02 22:23:50 - INFO - Epoch : 72 , Batch [ 70 / 93 ] : Loss = 0.499454, Accuracy = 33.29%, MSE = 1.2998
2025-03-02 22:23:50 - INFO - Classwise accuracy : (0: 100.00% (79)), (1: 0.00% (98)), (2: 0.00% (79))
2025-03-02 22:26:31 - INFO - Epoch : 72 , Batch [ 80 / 93 ] : Loss = 0.488232, Accuracy = 33.28%, MSE = 1.2881
2025-03-02 22:26:31 - INFO - Classwise accuracy : (0: 100.00% (92)), (1: 0.00% (90)), (2: 0.00% (74))
2025-03-02 22:28:49 - INFO - Epoch : 72 , Batch [ 90 / 93 ] : Loss = 0.494639, Accuracy = 33.11%, MSE = 1.2834
2025-03-02 22:28:49 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (88)), (2: 0.00% (84))
2025-03-02 22:29:13 - INFO - Epoch 72: Train Loss=0.5437, Train Acc=33.09%, Train MSE=1.2930
2025-03-02 22:29:29 - INFO - Epoch 72: Val Loss=0.4973, Val Acc=16.19%
2025-03-02 22:29:29 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-02 22:29:29 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 22:29:33 - INFO - Epoch : 73 , Batch [ 0 / 93 ] : Loss = 0.488702, Accuracy = 32.81%, MSE = 1.6680
2025-03-02 22:29:33 - INFO - Classwise accuracy : (0: 5.19% (77)), (1: 2.30% (87)), (2: 84.78% (92))
2025-03-02 22:31:55 - INFO - Epoch : 73 , Batch [ 10 / 93 ] : Loss = 0.491521, Accuracy = 33.59%, MSE = 1.2330
2025-03-02 22:31:55 - INFO - Classwise accuracy : (0: 1.14% (88)), (1: 0.00% (86)), (2: 100.00% (82))
2025-03-02 22:34:19 - INFO - Epoch : 73 , Batch [ 20 / 93 ] : Loss = 0.485331, Accuracy = 33.43%, MSE = 1.2612
2025-03-02 22:34:19 - INFO - Classwise accuracy : (0: 100.00% (102)), (1: 0.00% (83)), (2: 0.00% (71))
2025-03-02 22:36:44 - INFO - Epoch : 73 , Batch [ 30 / 93 ] : Loss = 0.491254, Accuracy = 33.44%, MSE = 1.3022
2025-03-02 22:36:44 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 1.01% (99)), (2: 100.00% (79))
2025-03-02 22:39:31 - INFO - Epoch : 73 , Batch [ 40 / 93 ] : Loss = 0.490462, Accuracy = 33.62%, MSE = 1.2471
2025-03-02 22:39:31 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 100.00% (88)), (2: 0.00% (79))
2025-03-02 22:42:05 - INFO - Epoch : 73 , Batch [ 50 / 93 ] : Loss = 0.490341, Accuracy = 33.75%, MSE = 1.2367
2025-03-02 22:42:05 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 47.67% (86)), (2: 57.83% (83))
2025-03-02 23:03:31 - INFO - Epoch : 73 , Batch [ 60 / 93 ] : Loss = 0.490526, Accuracy = 33.81%, MSE = 1.2776
2025-03-02 23:03:31 - INFO - Classwise accuracy : (0: 88.51% (87)), (1: 6.59% (91)), (2: 0.00% (78))
2025-03-02 23:08:58 - INFO - Epoch : 73 , Batch [ 70 / 93 ] : Loss = 0.480977, Accuracy = 33.98%, MSE = 1.2550
2025-03-02 23:08:58 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 0.00% (67)), (2: 100.00% (94))
2025-03-02 23:11:26 - INFO - Epoch : 73 , Batch [ 80 / 93 ] : Loss = 0.492065, Accuracy = 33.89%, MSE = 1.2606
2025-03-02 23:11:26 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 9.78% (92)), (2: 81.93% (83))
2025-03-02 23:13:38 - INFO - Epoch : 73 , Batch [ 90 / 93 ] : Loss = 1.108549, Accuracy = 34.01%, MSE = 1.2561
2025-03-02 23:13:38 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (86)), (2: 100.00% (95))
2025-03-02 23:14:05 - INFO - Epoch 73: Train Loss=0.5120, Train Acc=33.97%, Train MSE=1.2552
2025-03-02 23:14:21 - INFO - Epoch 73: Val Loss=0.4820, Val Acc=15.81%
2025-03-02 23:14:21 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-02 23:14:21 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 23:14:27 - INFO - Epoch : 74 , Batch [ 0 / 93 ] : Loss = 0.492057, Accuracy = 33.20%, MSE = 1.5586
2025-03-02 23:14:27 - INFO - Classwise accuracy : (0: 89.89% (89)), (1: 0.00% (93)), (2: 6.76% (74))
2025-03-02 23:16:34 - INFO - Epoch : 74 , Batch [ 10 / 93 ] : Loss = 0.516341, Accuracy = 33.70%, MSE = 1.2575
2025-03-02 23:16:34 - INFO - Classwise accuracy : (0: 92.71% (96)), (1: 8.99% (89)), (2: 0.00% (71))
2025-03-02 23:18:48 - INFO - Epoch : 74 , Batch [ 20 / 93 ] : Loss = 0.493735, Accuracy = 33.05%, MSE = 1.2950
2025-03-02 23:18:48 - INFO - Classwise accuracy : (0: 93.10% (87)), (1: 3.57% (84)), (2: 0.00% (85))
2025-03-02 23:21:01 - INFO - Epoch : 74 , Batch [ 30 / 93 ] : Loss = 0.495379, Accuracy = 33.34%, MSE = 1.2457
2025-03-02 23:21:01 - INFO - Classwise accuracy : (0: 14.29% (77)), (1: 80.21% (96)), (2: 0.00% (83))
2025-03-02 23:23:29 - INFO - Epoch : 74 , Batch [ 40 / 93 ] : Loss = 0.485653, Accuracy = 33.28%, MSE = 1.2646
2025-03-02 23:23:29 - INFO - Classwise accuracy : (0: 94.68% (94)), (1: 4.55% (88)), (2: 0.00% (74))
2025-03-02 23:25:59 - INFO - Epoch : 74 , Batch [ 50 / 93 ] : Loss = 0.486070, Accuracy = 33.33%, MSE = 1.2655
2025-03-02 23:25:59 - INFO - Classwise accuracy : (0: 0.00% (67)), (1: 79.22% (77)), (2: 26.79% (112))
2025-03-02 23:28:19 - INFO - Epoch : 74 , Batch [ 60 / 93 ] : Loss = 0.497833, Accuracy = 33.24%, MSE = 1.3106
2025-03-02 23:28:19 - INFO - Classwise accuracy : (0: 0.00% (73)), (1: 0.00% (100)), (2: 100.00% (83))
2025-03-02 23:30:49 - INFO - Epoch : 74 , Batch [ 70 / 93 ] : Loss = 0.485795, Accuracy = 33.23%, MSE = 1.3197
2025-03-02 23:30:49 - INFO - Classwise accuracy : (0: 100.00% (94)), (1: 0.00% (84)), (2: 0.00% (78))
2025-03-02 23:33:09 - INFO - Epoch : 74 , Batch [ 80 / 93 ] : Loss = 0.487124, Accuracy = 33.41%, MSE = 1.3349
2025-03-02 23:33:09 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (94)), (2: 0.00% (83))
2025-03-02 23:35:31 - INFO - Epoch : 74 , Batch [ 90 / 93 ] : Loss = 0.502299, Accuracy = 33.45%, MSE = 1.3404
2025-03-02 23:35:31 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 22.50% (80)), (2: 73.81% (84))
2025-03-02 23:35:56 - INFO - Epoch 74: Train Loss=0.5048, Train Acc=33.46%, Train MSE=1.3405
2025-03-02 23:36:12 - INFO - Epoch 74: Val Loss=0.4595, Val Acc=68.00%
2025-03-02 23:36:12 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-02 23:36:12 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-02 23:36:18 - INFO - Epoch : 75 , Batch [ 0 / 93 ] : Loss = 0.488614, Accuracy = 36.72%, MSE = 0.6797
2025-03-02 23:36:18 - INFO - Classwise accuracy : (0: 1.32% (76)), (1: 97.89% (95)), (2: 0.00% (85))
2025-03-02 23:38:22 - INFO - Epoch : 75 , Batch [ 10 / 93 ] : Loss = 0.494853, Accuracy = 33.95%, MSE = 0.9567
2025-03-02 23:38:22 - INFO - Classwise accuracy : (0: 8.86% (79)), (1: 92.77% (83)), (2: 0.00% (94))
2025-03-02 23:40:57 - INFO - Epoch : 75 , Batch [ 20 / 93 ] : Loss = 0.487697, Accuracy = 33.87%, MSE = 0.9860
2025-03-02 23:40:57 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 89.41% (85)), (2: 11.83% (93))
2025-03-02 23:43:43 - INFO - Epoch : 75 , Batch [ 30 / 93 ] : Loss = 0.495496, Accuracy = 33.77%, MSE = 1.0929
2025-03-02 23:43:43 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 77.78% (81)), (2: 21.84% (87))
2025-03-02 23:46:40 - INFO - Epoch : 75 , Batch [ 40 / 93 ] : Loss = 0.488422, Accuracy = 33.63%, MSE = 1.1570
2025-03-02 23:46:40 - INFO - Classwise accuracy : (0: 100.00% (94)), (1: 0.00% (78)), (2: 0.00% (84))
2025-03-02 23:49:30 - INFO - Epoch : 75 , Batch [ 50 / 93 ] : Loss = 0.496313, Accuracy = 33.70%, MSE = 1.2216
2025-03-02 23:49:30 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (78)), (2: 0.00% (90))
2025-03-02 23:52:17 - INFO - Epoch : 75 , Batch [ 60 / 93 ] : Loss = 0.487118, Accuracy = 33.59%, MSE = 1.2450
2025-03-02 23:52:17 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 94.57% (92)), (2: 3.61% (83))
2025-03-02 23:55:08 - INFO - Epoch : 75 , Batch [ 70 / 93 ] : Loss = 0.514646, Accuracy = 33.58%, MSE = 1.2729
2025-03-02 23:55:08 - INFO - Classwise accuracy : (0: 0.00% (90)), (1: 0.00% (83)), (2: 100.00% (83))
2025-03-02 23:57:43 - INFO - Epoch : 75 , Batch [ 80 / 93 ] : Loss = 0.494219, Accuracy = 33.52%, MSE = 1.2868
2025-03-02 23:57:43 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 0.00% (91)), (2: 100.00% (84))
2025-03-03 00:00:18 - INFO - Epoch : 75 , Batch [ 90 / 93 ] : Loss = 0.487839, Accuracy = 33.56%, MSE = 1.2751
2025-03-03 00:00:18 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 39.51% (81)), (2: 69.79% (96))
2025-03-03 00:00:47 - INFO - Epoch 75: Train Loss=0.4934, Train Acc=33.53%, Train MSE=1.2802
2025-03-03 00:01:03 - INFO - Epoch 75: Val Loss=0.4473, Val Acc=68.00%
2025-03-03 00:01:03 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-03 00:01:03 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 00:01:09 - INFO - Epoch : 76 , Batch [ 0 / 93 ] : Loss = 0.508185, Accuracy = 33.59%, MSE = 0.6641
2025-03-03 00:01:09 - INFO - Classwise accuracy : (0: 1.20% (83)), (1: 100.00% (85)), (2: 0.00% (88))
2025-03-03 00:03:08 - INFO - Epoch : 76 , Batch [ 10 / 93 ] : Loss = 0.523095, Accuracy = 32.21%, MSE = 1.2362
2025-03-03 00:03:08 - INFO - Classwise accuracy : (0: 100.00% (79)), (1: 0.00% (85)), (2: 0.00% (92))
2025-03-03 00:05:08 - INFO - Epoch : 76 , Batch [ 20 / 93 ] : Loss = 0.500933, Accuracy = 32.05%, MSE = 1.2967
2025-03-03 00:05:08 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (93)), (2: 100.00% (88))
2025-03-03 00:07:25 - INFO - Epoch : 76 , Batch [ 30 / 93 ] : Loss = 0.519378, Accuracy = 32.74%, MSE = 1.2805
2025-03-03 00:07:25 - INFO - Classwise accuracy : (0: 57.65% (85)), (1: 26.25% (80)), (2: 0.00% (91))
2025-03-03 00:09:45 - INFO - Epoch : 76 , Batch [ 40 / 93 ] : Loss = 0.610408, Accuracy = 32.51%, MSE = 1.2969
2025-03-03 00:09:45 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 0.00% (99)), (2: 100.00% (83))
2025-03-03 00:12:00 - INFO - Epoch : 76 , Batch [ 50 / 93 ] : Loss = 0.541144, Accuracy = 32.53%, MSE = 1.2891
2025-03-03 00:12:00 - INFO - Classwise accuracy : (0: 100.00% (85)), (1: 0.00% (82)), (2: 0.00% (89))
2025-03-03 00:14:12 - INFO - Epoch : 76 , Batch [ 60 / 93 ] : Loss = 0.486871, Accuracy = 32.63%, MSE = 1.2674
2025-03-03 00:14:12 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 0.00% (97)), (2: 100.00% (85))
2025-03-03 00:16:19 - INFO - Epoch : 76 , Batch [ 70 / 93 ] : Loss = 0.501210, Accuracy = 32.52%, MSE = 1.2842
2025-03-03 00:16:19 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 0.00% (90)), (2: 100.00% (92))
2025-03-03 00:18:24 - INFO - Epoch : 76 , Batch [ 80 / 93 ] : Loss = 0.489225, Accuracy = 32.77%, MSE = 1.2822
2025-03-03 00:18:24 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (88)), (2: 0.00% (90))
2025-03-03 00:20:36 - INFO - Epoch : 76 , Batch [ 90 / 93 ] : Loss = 0.503837, Accuracy = 32.98%, MSE = 1.2813
2025-03-03 00:20:36 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 100.00% (84)), (2: 0.00% (81))
2025-03-03 00:20:59 - INFO - Epoch 76: Train Loss=0.5121, Train Acc=33.01%, Train MSE=1.2717
2025-03-03 00:21:15 - INFO - Epoch 76: Val Loss=0.4994, Val Acc=15.81%
2025-03-03 00:21:15 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 00:21:15 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 00:21:19 - INFO - Epoch : 77 , Batch [ 0 / 93 ] : Loss = 0.521174, Accuracy = 26.17%, MSE = 1.9102
2025-03-03 00:21:19 - INFO - Classwise accuracy : (0: 100.00% (67)), (1: 0.00% (89)), (2: 0.00% (100))
2025-03-03 00:23:40 - INFO - Epoch : 77 , Batch [ 10 / 93 ] : Loss = 0.532189, Accuracy = 32.07%, MSE = 1.4421
2025-03-03 00:23:40 - INFO - Classwise accuracy : (0: 0.00% (97)), (1: 0.00% (89)), (2: 100.00% (70))
2025-03-03 00:26:03 - INFO - Epoch : 77 , Batch [ 20 / 93 ] : Loss = 0.485119, Accuracy = 32.85%, MSE = 1.3986
2025-03-03 00:26:03 - INFO - Classwise accuracy : (0: 95.88% (97)), (1: 15.12% (86)), (2: 0.00% (73))
2025-03-03 00:28:40 - INFO - Epoch : 77 , Batch [ 30 / 93 ] : Loss = 0.488814, Accuracy = 32.96%, MSE = 1.4211
2025-03-03 00:28:40 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 13.19% (91)), (2: 86.08% (79))
2025-03-03 00:31:24 - INFO - Epoch : 77 , Batch [ 40 / 93 ] : Loss = 0.506346, Accuracy = 33.37%, MSE = 1.4314
2025-03-03 00:31:24 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 0.00% (78)), (2: 100.00% (91))
2025-03-03 00:34:01 - INFO - Epoch : 77 , Batch [ 50 / 93 ] : Loss = 0.491035, Accuracy = 33.54%, MSE = 1.3967
2025-03-03 00:34:01 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (85)), (2: 0.00% (87))
2025-03-03 00:36:45 - INFO - Epoch : 77 , Batch [ 60 / 93 ] : Loss = 0.489987, Accuracy = 33.74%, MSE = 1.4137
2025-03-03 00:36:45 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 0.00% (76)), (2: 100.00% (89))
2025-03-03 00:39:30 - INFO - Epoch : 77 , Batch [ 70 / 93 ] : Loss = 0.487652, Accuracy = 33.75%, MSE = 1.3859
2025-03-03 00:39:30 - INFO - Classwise accuracy : (0: 1.16% (86)), (1: 95.92% (98)), (2: 0.00% (72))
2025-03-03 00:42:07 - INFO - Epoch : 77 , Batch [ 80 / 93 ] : Loss = 0.488482, Accuracy = 33.76%, MSE = 1.3600
2025-03-03 00:42:07 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 97.73% (88)), (2: 6.10% (82))
2025-03-03 00:44:38 - INFO - Epoch : 77 , Batch [ 90 / 93 ] : Loss = 0.488423, Accuracy = 33.82%, MSE = 1.3556
2025-03-03 00:44:38 - INFO - Classwise accuracy : (0: 19.51% (82)), (1: 74.71% (87)), (2: 1.15% (87))
2025-03-03 00:45:04 - INFO - Epoch 77: Train Loss=0.4924, Train Acc=33.84%, Train MSE=1.3438
2025-03-03 00:45:20 - INFO - Epoch 77: Val Loss=0.4537, Val Acc=15.81%
2025-03-03 00:45:20 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 00:45:20 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 00:45:24 - INFO - Epoch : 78 , Batch [ 0 / 93 ] : Loss = 0.507080, Accuracy = 32.03%, MSE = 1.3008
2025-03-03 00:45:24 - INFO - Classwise accuracy : (0: 66.67% (87)), (1: 28.24% (85)), (2: 0.00% (84))
2025-03-03 00:47:36 - INFO - Epoch : 78 , Batch [ 10 / 93 ] : Loss = 0.492189, Accuracy = 33.84%, MSE = 1.3626
2025-03-03 00:47:36 - INFO - Classwise accuracy : (0: 0.00% (89)), (1: 100.00% (92)), (2: 0.00% (75))
2025-03-03 00:49:48 - INFO - Epoch : 78 , Batch [ 20 / 93 ] : Loss = 0.493229, Accuracy = 33.80%, MSE = 1.3116
2025-03-03 00:49:48 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (85)), (2: 0.00% (88))
2025-03-03 00:52:24 - INFO - Epoch : 78 , Batch [ 30 / 93 ] : Loss = 0.513545, Accuracy = 33.78%, MSE = 1.2765
2025-03-03 00:52:24 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 0.00% (84)), (2: 97.65% (85))
2025-03-03 00:55:01 - INFO - Epoch : 78 , Batch [ 40 / 93 ] : Loss = 0.492939, Accuracy = 33.97%, MSE = 1.2525
2025-03-03 00:55:01 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 7.50% (80)), (2: 94.51% (91))
2025-03-03 00:57:33 - INFO - Epoch : 78 , Batch [ 50 / 93 ] : Loss = 0.485392, Accuracy = 33.66%, MSE = 1.2553
2025-03-03 00:57:33 - INFO - Classwise accuracy : (0: 0.00% (73)), (1: 0.00% (79)), (2: 100.00% (104))
2025-03-03 01:00:08 - INFO - Epoch : 78 , Batch [ 60 / 93 ] : Loss = 0.496789, Accuracy = 33.53%, MSE = 1.2276
2025-03-03 01:00:08 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 100.00% (86)), (2: 0.00% (90))
2025-03-03 01:02:29 - INFO - Epoch : 78 , Batch [ 70 / 93 ] : Loss = 0.497829, Accuracy = 33.40%, MSE = 1.2480
2025-03-03 01:02:29 - INFO - Classwise accuracy : (0: 100.00% (79)), (1: 0.00% (83)), (2: 0.00% (94))
2025-03-03 01:05:06 - INFO - Epoch : 78 , Batch [ 80 / 93 ] : Loss = 0.502158, Accuracy = 33.44%, MSE = 1.2563
2025-03-03 01:05:06 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (98)), (2: 0.00% (76))
2025-03-03 01:07:21 - INFO - Epoch : 78 , Batch [ 90 / 93 ] : Loss = 0.501912, Accuracy = 33.24%, MSE = 1.2728
2025-03-03 01:07:21 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 0.00% (94)), (2: 100.00% (76))
2025-03-03 01:07:47 - INFO - Epoch 78: Train Loss=0.5040, Train Acc=33.20%, Train MSE=1.2613
2025-03-03 01:08:03 - INFO - Epoch 78: Val Loss=0.5603, Val Acc=15.81%
2025-03-03 01:08:03 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 01:08:03 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 01:08:08 - INFO - Epoch : 79 , Batch [ 0 / 93 ] : Loss = 0.520237, Accuracy = 32.42%, MSE = 1.6719
2025-03-03 01:08:08 - INFO - Classwise accuracy : (0: 100.00% (83)), (1: 0.00% (88)), (2: 0.00% (85))
2025-03-03 01:10:25 - INFO - Epoch : 79 , Batch [ 10 / 93 ] : Loss = 0.488347, Accuracy = 32.63%, MSE = 1.4215
2025-03-03 01:10:25 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 100.00% (93)), (2: 0.00% (84))
2025-03-03 01:13:58 - INFO - Epoch : 79 , Batch [ 20 / 93 ] : Loss = 0.490544, Accuracy = 33.44%, MSE = 1.3385
2025-03-03 01:13:58 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (78)), (2: 0.00% (97))
2025-03-03 01:17:06 - INFO - Epoch : 79 , Batch [ 30 / 93 ] : Loss = 0.496386, Accuracy = 33.72%, MSE = 1.3440
2025-03-03 01:17:06 - INFO - Classwise accuracy : (0: 87.01% (77)), (1: 0.00% (93)), (2: 16.28% (86))
2025-03-03 01:20:00 - INFO - Epoch : 79 , Batch [ 40 / 93 ] : Loss = 0.505197, Accuracy = 33.49%, MSE = 1.3145
2025-03-03 01:20:00 - INFO - Classwise accuracy : (0: 0.00% (95)), (1: 88.10% (84)), (2: 14.29% (77))
2025-03-03 01:22:48 - INFO - Epoch : 79 , Batch [ 50 / 93 ] : Loss = 0.527707, Accuracy = 33.44%, MSE = 1.3375
2025-03-03 01:22:48 - INFO - Classwise accuracy : (0: 91.46% (82)), (1: 7.23% (83)), (2: 0.00% (91))
2025-03-03 01:25:58 - INFO - Epoch : 79 , Batch [ 60 / 93 ] : Loss = 0.490979, Accuracy = 33.54%, MSE = 1.3199
2025-03-03 01:25:58 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 23.60% (89)), (2: 71.74% (92))
2025-03-03 01:29:15 - INFO - Epoch : 79 , Batch [ 70 / 93 ] : Loss = 0.509033, Accuracy = 33.40%, MSE = 1.3420
2025-03-03 01:29:15 - INFO - Classwise accuracy : (0: 100.00% (71)), (1: 0.00% (88)), (2: 0.00% (97))
2025-03-03 01:32:38 - INFO - Epoch : 79 , Batch [ 80 / 93 ] : Loss = 0.489808, Accuracy = 33.52%, MSE = 1.3660
2025-03-03 01:32:38 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 0.00% (85)), (2: 98.92% (93))
2025-03-03 01:35:55 - INFO - Epoch : 79 , Batch [ 90 / 93 ] : Loss = 0.486306, Accuracy = 33.53%, MSE = 1.3537
2025-03-03 01:35:55 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 100.00% (92)), (2: 0.00% (89))
2025-03-03 01:36:31 - INFO - Epoch 79: Train Loss=0.4960, Train Acc=33.56%, Train MSE=1.3584
2025-03-03 01:36:47 - INFO - Epoch 79: Val Loss=0.4893, Val Acc=16.19%
2025-03-03 01:36:47 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.16      1.00      0.28       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.03      0.16      0.05      4744

2025-03-03 01:36:47 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 01:36:52 - INFO - Epoch : 80 , Batch [ 0 / 93 ] : Loss = 0.487506, Accuracy = 36.72%, MSE = 1.5703
2025-03-03 01:36:52 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 0.00% (80)), (2: 97.92% (96))
2025-03-03 01:39:18 - INFO - Epoch : 80 , Batch [ 10 / 93 ] : Loss = 0.491761, Accuracy = 35.87%, MSE = 1.3924
2025-03-03 01:39:19 - INFO - Classwise accuracy : (0: 100.00% (94)), (1: 0.00% (82)), (2: 0.00% (80))
2025-03-03 01:42:03 - INFO - Epoch : 80 , Batch [ 20 / 93 ] : Loss = 0.494953, Accuracy = 34.99%, MSE = 1.2388
2025-03-03 01:42:03 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 87.06% (85)), (2: 21.43% (84))
2025-03-03 01:44:25 - INFO - Epoch : 80 , Batch [ 30 / 93 ] : Loss = 0.509067, Accuracy = 34.82%, MSE = 1.2332
2025-03-03 01:44:25 - INFO - Classwise accuracy : (0: 100.00% (79)), (1: 0.00% (92)), (2: 0.00% (85))
2025-03-03 01:46:25 - INFO - Epoch : 80 , Batch [ 40 / 93 ] : Loss = 0.493161, Accuracy = 34.17%, MSE = 1.2674
2025-03-03 01:46:25 - INFO - Classwise accuracy : (0: 100.00% (84)), (1: 0.00% (90)), (2: 0.00% (82))
2025-03-03 01:48:29 - INFO - Epoch : 80 , Batch [ 50 / 93 ] : Loss = 0.486923, Accuracy = 34.60%, MSE = 1.2919
2025-03-03 01:48:29 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 0.00% (66)), (2: 100.00% (97))
2025-03-03 01:50:46 - INFO - Epoch : 80 , Batch [ 60 / 93 ] : Loss = 0.499702, Accuracy = 34.33%, MSE = 1.3020
2025-03-03 01:50:46 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 0.00% (99)), (2: 100.00% (85))
2025-03-03 01:53:01 - INFO - Epoch : 80 , Batch [ 70 / 93 ] : Loss = 0.488378, Accuracy = 34.30%, MSE = 1.2808
2025-03-03 01:53:01 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (87)), (2: 0.00% (82))
2025-03-03 01:55:10 - INFO - Epoch : 80 , Batch [ 80 / 93 ] : Loss = 0.500079, Accuracy = 34.24%, MSE = 1.2915
2025-03-03 01:55:10 - INFO - Classwise accuracy : (0: 0.00% (77)), (1: 100.00% (82)), (2: 0.00% (97))
2025-03-03 01:57:30 - INFO - Epoch : 80 , Batch [ 90 / 93 ] : Loss = 0.481403, Accuracy = 34.26%, MSE = 1.2852
2025-03-03 01:57:30 - INFO - Classwise accuracy : (0: 0.00% (75)), (1: 0.00% (78)), (2: 100.00% (103))
2025-03-03 01:57:56 - INFO - Epoch 80: Train Loss=0.4951, Train Acc=34.24%, Train MSE=1.2820
2025-03-03 01:58:12 - INFO - Epoch 80: Val Loss=0.5276, Val Acc=15.81%
2025-03-03 01:58:12 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 01:58:12 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 01:58:16 - INFO - Epoch : 81 , Batch [ 0 / 93 ] : Loss = 0.491510, Accuracy = 33.98%, MSE = 1.6914
2025-03-03 01:58:16 - INFO - Classwise accuracy : (0: 100.00% (87)), (1: 0.00% (81)), (2: 0.00% (88))
2025-03-03 02:00:14 - INFO - Epoch : 81 , Batch [ 10 / 93 ] : Loss = 0.498837, Accuracy = 32.07%, MSE = 1.4155
2025-03-03 02:00:14 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (77)), (2: 1.15% (87))
2025-03-03 02:02:02 - INFO - Epoch : 81 , Batch [ 20 / 93 ] : Loss = 0.483568, Accuracy = 32.89%, MSE = 1.4284
2025-03-03 02:02:02 - INFO - Classwise accuracy : (0: 0.00% (67)), (1: 0.00% (99)), (2: 100.00% (90))
2025-03-03 02:04:19 - INFO - Epoch : 81 , Batch [ 30 / 93 ] : Loss = 0.486795, Accuracy = 32.84%, MSE = 1.4148
2025-03-03 02:04:19 - INFO - Classwise accuracy : (0: 100.00% (98)), (1: 0.00% (86)), (2: 0.00% (72))
2025-03-03 02:07:10 - INFO - Epoch : 81 , Batch [ 40 / 93 ] : Loss = 0.506375, Accuracy = 32.91%, MSE = 1.3683
2025-03-03 02:07:10 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 98.89% (90)), (2: 0.00% (82))
2025-03-03 02:09:45 - INFO - Epoch : 81 , Batch [ 50 / 93 ] : Loss = 0.490056, Accuracy = 32.96%, MSE = 1.3150
2025-03-03 02:09:45 - INFO - Classwise accuracy : (0: 0.00% (72)), (1: 0.00% (96)), (2: 100.00% (88))
2025-03-03 02:47:51 - INFO - Epoch : 81 , Batch [ 60 / 93 ] : Loss = 0.490044, Accuracy = 33.10%, MSE = 1.3147
2025-03-03 02:47:51 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (85)), (2: 0.00% (83))
2025-03-03 03:06:49 - INFO - Epoch : 81 , Batch [ 70 / 93 ] : Loss = 0.489477, Accuracy = 32.87%, MSE = 1.3263
2025-03-03 03:06:49 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 0.00% (86)), (2: 100.00% (88))
2025-03-03 03:19:13 - INFO - Epoch : 81 , Batch [ 80 / 93 ] : Loss = 0.488431, Accuracy = 32.90%, MSE = 1.3363
2025-03-03 03:19:13 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (93)), (2: 0.00% (77))
2025-03-03 03:21:52 - INFO - Epoch : 81 , Batch [ 90 / 93 ] : Loss = 0.487328, Accuracy = 32.92%, MSE = 1.3541
2025-03-03 03:21:52 - INFO - Classwise accuracy : (0: 0.00% (74)), (1: 93.33% (90)), (2: 5.43% (92))
2025-03-03 03:22:22 - INFO - Epoch 81: Train Loss=0.4930, Train Acc=32.89%, Train MSE=1.3409
2025-03-03 03:22:38 - INFO - Epoch 81: Val Loss=0.4948, Val Acc=15.81%
2025-03-03 03:22:38 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 03:22:38 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 03:22:43 - INFO - Epoch : 82 , Batch [ 0 / 93 ] : Loss = 0.488691, Accuracy = 34.38%, MSE = 1.6289
2025-03-03 03:22:43 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (85)), (2: 0.00% (83))
2025-03-03 03:24:52 - INFO - Epoch : 82 , Batch [ 10 / 93 ] : Loss = 0.489023, Accuracy = 33.84%, MSE = 1.4819
2025-03-03 03:24:52 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 58.62% (87)), (2: 39.33% (89))
2025-03-03 03:27:06 - INFO - Epoch : 82 , Batch [ 20 / 93 ] : Loss = 0.495678, Accuracy = 33.98%, MSE = 1.3410
2025-03-03 03:27:06 - INFO - Classwise accuracy : (0: 100.00% (88)), (1: 0.00% (81)), (2: 0.00% (87))
2025-03-03 03:29:37 - INFO - Epoch : 82 , Batch [ 30 / 93 ] : Loss = 0.505062, Accuracy = 33.92%, MSE = 1.3000
2025-03-03 03:29:37 - INFO - Classwise accuracy : (0: 3.75% (80)), (1: 0.00% (91)), (2: 98.82% (85))
2025-03-03 03:32:17 - INFO - Epoch : 82 , Batch [ 40 / 93 ] : Loss = 0.495560, Accuracy = 33.50%, MSE = 1.3147
2025-03-03 03:32:17 - INFO - Classwise accuracy : (0: 100.00% (91)), (1: 0.00% (88)), (2: 0.00% (77))
2025-03-03 03:35:06 - INFO - Epoch : 82 , Batch [ 50 / 93 ] : Loss = 0.491892, Accuracy = 33.54%, MSE = 1.3135
2025-03-03 03:35:06 - INFO - Classwise accuracy : (0: 0.00% (79)), (1: 0.00% (87)), (2: 100.00% (90))
2025-03-03 03:37:44 - INFO - Epoch : 82 , Batch [ 60 / 93 ] : Loss = 0.487610, Accuracy = 33.45%, MSE = 1.2991
2025-03-03 03:37:44 - INFO - Classwise accuracy : (0: 100.00% (89)), (1: 0.00% (87)), (2: 0.00% (80))
2025-03-03 03:40:31 - INFO - Epoch : 82 , Batch [ 70 / 93 ] : Loss = 0.489645, Accuracy = 33.33%, MSE = 1.2975
2025-03-03 03:40:31 - INFO - Classwise accuracy : (0: 19.75% (81)), (1: 3.23% (93)), (2: 89.02% (82))
2025-03-03 03:43:02 - INFO - Epoch : 82 , Batch [ 80 / 93 ] : Loss = 0.487219, Accuracy = 33.39%, MSE = 1.3018
2025-03-03 03:43:02 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 12.79% (86)), (2: 93.26% (89))
2025-03-03 03:45:53 - INFO - Epoch : 82 , Batch [ 90 / 93 ] : Loss = 0.493630, Accuracy = 33.50%, MSE = 1.2824
2025-03-03 03:45:53 - INFO - Classwise accuracy : (0: 0.00% (82)), (1: 100.00% (82)), (2: 0.00% (92))
2025-03-03 03:46:23 - INFO - Epoch 82: Train Loss=0.4948, Train Acc=33.45%, Train MSE=1.2887
2025-03-03 03:46:39 - INFO - Epoch 82: Val Loss=0.4902, Val Acc=15.81%
2025-03-03 03:46:39 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 03:46:39 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 03:46:44 - INFO - Epoch : 83 , Batch [ 0 / 93 ] : Loss = 0.488812, Accuracy = 33.20%, MSE = 1.3828
2025-03-03 03:46:44 - INFO - Classwise accuracy : (0: 59.21% (76)), (1: 29.55% (88)), (2: 15.22% (92))
2025-03-03 03:49:08 - INFO - Epoch : 83 , Batch [ 10 / 93 ] : Loss = 0.492820, Accuracy = 34.98%, MSE = 1.2340
2025-03-03 03:49:08 - INFO - Classwise accuracy : (0: 0.00% (81)), (1: 3.26% (92)), (2: 100.00% (83))
2025-03-03 03:52:25 - INFO - Epoch : 83 , Batch [ 20 / 93 ] : Loss = 0.491041, Accuracy = 34.15%, MSE = 1.2628
2025-03-03 03:52:25 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 70.13% (77)), (2: 31.40% (86))
2025-03-03 03:55:01 - INFO - Epoch : 83 , Batch [ 30 / 93 ] : Loss = 0.499010, Accuracy = 33.42%, MSE = 1.3383
2025-03-03 03:55:01 - INFO - Classwise accuracy : (0: 0.00% (108)), (1: 17.57% (74)), (2: 85.14% (74))
2025-03-03 03:57:15 - INFO - Epoch : 83 , Batch [ 40 / 93 ] : Loss = 0.495002, Accuracy = 33.49%, MSE = 1.2902
2025-03-03 03:57:15 - INFO - Classwise accuracy : (0: 0.00% (94)), (1: 90.00% (80)), (2: 2.44% (82))
2025-03-03 03:59:41 - INFO - Epoch : 83 , Batch [ 50 / 93 ] : Loss = 0.495126, Accuracy = 33.40%, MSE = 1.3098
2025-03-03 03:59:41 - INFO - Classwise accuracy : (0: 0.00% (80)), (1: 32.97% (91)), (2: 57.65% (85))
2025-03-03 04:02:14 - INFO - Epoch : 83 , Batch [ 60 / 93 ] : Loss = 0.488171, Accuracy = 33.48%, MSE = 1.2967
2025-03-03 04:02:14 - INFO - Classwise accuracy : (0: 85.54% (83)), (1: 5.88% (85)), (2: 12.50% (88))
2025-03-03 04:04:48 - INFO - Epoch : 83 , Batch [ 70 / 93 ] : Loss = 0.495433, Accuracy = 33.54%, MSE = 1.2989
2025-03-03 04:04:48 - INFO - Classwise accuracy : (0: 100.00% (91)), (1: 0.00% (86)), (2: 0.00% (79))
2025-03-03 04:07:03 - INFO - Epoch : 83 , Batch [ 80 / 93 ] : Loss = 0.500576, Accuracy = 33.30%, MSE = 1.2853
2025-03-03 04:07:03 - INFO - Classwise accuracy : (0: 0.00% (87)), (1: 13.25% (83)), (2: 83.72% (86))
2025-03-03 04:09:24 - INFO - Epoch : 83 , Batch [ 90 / 93 ] : Loss = 0.492784, Accuracy = 33.33%, MSE = 1.3151
2025-03-03 04:09:24 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (88)), (2: 0.00% (85))
2025-03-03 04:09:51 - INFO - Epoch 83: Train Loss=0.4957, Train Acc=33.40%, Train MSE=1.3210
2025-03-03 04:10:07 - INFO - Epoch 83: Val Loss=0.6475, Val Acc=15.81%
2025-03-03 04:10:07 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.16      1.00      0.27       750
        Hold       0.00      0.00      0.00      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.16      4744
   macro avg       0.05      0.33      0.09      4744
weighted avg       0.02      0.16      0.04      4744

2025-03-03 04:10:07 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 04:10:11 - INFO - Epoch : 84 , Batch [ 0 / 93 ] : Loss = 0.519254, Accuracy = 31.64%, MSE = 1.7031
2025-03-03 04:10:11 - INFO - Classwise accuracy : (0: 100.00% (81)), (1: 0.00% (88)), (2: 0.00% (87))
2025-03-03 04:12:12 - INFO - Epoch : 84 , Batch [ 10 / 93 ] : Loss = 0.499865, Accuracy = 33.31%, MSE = 1.2731
2025-03-03 04:12:12 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (76)), (2: 0.00% (88))
2025-03-03 04:14:28 - INFO - Epoch : 84 , Batch [ 20 / 93 ] : Loss = 0.502154, Accuracy = 32.53%, MSE = 1.2935
2025-03-03 04:14:28 - INFO - Classwise accuracy : (0: 88.57% (70)), (1: 0.00% (85)), (2: 4.95% (101))
2025-03-03 04:17:07 - INFO - Epoch : 84 , Batch [ 30 / 93 ] : Loss = 0.495159, Accuracy = 32.62%, MSE = 1.3187
2025-03-03 04:17:07 - INFO - Classwise accuracy : (0: 0.00% (93)), (1: 100.00% (82)), (2: 0.00% (81))
2025-03-03 04:23:34 - INFO - Epoch : 84 , Batch [ 40 / 93 ] : Loss = 0.489563, Accuracy = 32.52%, MSE = 1.3368
2025-03-03 04:23:34 - INFO - Classwise accuracy : (0: 96.20% (79)), (1: 4.82% (83)), (2: 0.00% (94))
2025-03-03 04:42:33 - INFO - Epoch : 84 , Batch [ 50 / 93 ] : Loss = 0.488882, Accuracy = 32.92%, MSE = 1.3025
2025-03-03 04:42:33 - INFO - Classwise accuracy : (0: 0.00% (85)), (1: 0.00% (79)), (2: 100.00% (92))
2025-03-03 05:00:47 - INFO - Epoch : 84 , Batch [ 60 / 93 ] : Loss = 0.489775, Accuracy = 32.99%, MSE = 1.2920
2025-03-03 05:00:47 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 76.92% (78)), (2: 22.09% (86))
2025-03-03 05:19:03 - INFO - Epoch : 84 , Batch [ 70 / 93 ] : Loss = 0.493477, Accuracy = 32.82%, MSE = 1.3355
2025-03-03 05:19:03 - INFO - Classwise accuracy : (0: 0.00% (86)), (1: 100.00% (75)), (2: 0.00% (95))
2025-03-03 05:38:02 - INFO - Epoch : 84 , Batch [ 80 / 93 ] : Loss = 0.491617, Accuracy = 32.92%, MSE = 1.2989
2025-03-03 05:38:02 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 100.00% (78)), (2: 0.00% (95))
2025-03-03 05:40:20 - INFO - Epoch : 84 , Batch [ 90 / 93 ] : Loss = 0.492109, Accuracy = 32.98%, MSE = 1.2830
2025-03-03 05:40:20 - INFO - Classwise accuracy : (0: 0.00% (88)), (1: 100.00% (82)), (2: 1.16% (86))
2025-03-03 05:40:48 - INFO - Epoch 84: Train Loss=0.4957, Train Acc=32.99%, Train MSE=1.2804
2025-03-03 05:41:04 - INFO - Epoch 84: Val Loss=0.4725, Val Acc=68.00%
2025-03-03 05:41:04 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-03 05:41:04 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 05:41:08 - INFO - Epoch : 85 , Batch [ 0 / 93 ] : Loss = 0.491949, Accuracy = 33.98%, MSE = 0.7891
2025-03-03 05:41:08 - INFO - Classwise accuracy : (0: 0.00% (91)), (1: 96.51% (86)), (2: 5.06% (79))
2025-03-03 05:58:19 - INFO - Epoch : 85 , Batch [ 10 / 93 ] : Loss = 0.489676, Accuracy = 32.71%, MSE = 1.0213
2025-03-03 05:58:19 - INFO - Classwise accuracy : (0: 0.00% (78)), (1: 100.00% (81)), (2: 0.00% (97))
2025-03-03 06:33:22 - INFO - Epoch : 85 , Batch [ 20 / 93 ] : Loss = 0.489803, Accuracy = 33.13%, MSE = 1.0236
2025-03-03 06:33:22 - INFO - Classwise accuracy : (0: 0.00% (99)), (1: 90.70% (86)), (2: 11.27% (71))
2025-03-03 06:51:10 - INFO - Epoch : 85 , Batch [ 30 / 93 ] : Loss = 0.490653, Accuracy = 32.72%, MSE = 1.1052
2025-03-03 06:51:10 - INFO - Classwise accuracy : (0: 0.00% (83)), (1: 0.00% (86)), (2: 100.00% (87))
2025-03-03 07:16:59 - INFO - Epoch : 85 , Batch [ 40 / 93 ] : Loss = 0.490245, Accuracy = 33.38%, MSE = 1.1258
2025-03-03 07:16:59 - INFO - Classwise accuracy : (0: 18.39% (87)), (1: 0.00% (81)), (2: 76.14% (88))
2025-03-03 07:36:07 - INFO - Epoch : 85 , Batch [ 50 / 93 ] : Loss = 0.487259, Accuracy = 33.39%, MSE = 1.1510
2025-03-03 07:36:07 - INFO - Classwise accuracy : (0: 24.36% (78)), (1: 81.63% (98)), (2: 0.00% (80))
2025-03-03 07:54:44 - INFO - Epoch : 85 , Batch [ 60 / 93 ] : Loss = 0.490387, Accuracy = 33.20%, MSE = 1.1727
2025-03-03 07:54:44 - INFO - Classwise accuracy : (0: 97.47% (79)), (1: 0.00% (94)), (2: 1.20% (83))
2025-03-03 08:00:32 - INFO - Epoch : 85 , Batch [ 70 / 93 ] : Loss = 0.489283, Accuracy = 33.07%, MSE = 1.1876
2025-03-03 08:00:32 - INFO - Classwise accuracy : (0: 1.20% (83)), (1: 91.25% (80)), (2: 10.75% (93))
2025-03-03 08:35:30 - INFO - Epoch : 85 , Batch [ 80 / 93 ] : Loss = 0.497157, Accuracy = 33.26%, MSE = 1.1933
2025-03-03 08:35:30 - INFO - Classwise accuracy : (0: 0.00% (84)), (1: 0.00% (90)), (2: 100.00% (82))
2025-03-03 08:53:49 - INFO - Epoch : 85 , Batch [ 90 / 93 ] : Loss = 0.488243, Accuracy = 33.46%, MSE = 1.2400
2025-03-03 08:53:49 - INFO - Classwise accuracy : (0: 100.00% (90)), (1: 0.00% (83)), (2: 0.00% (83))
2025-03-03 08:54:19 - INFO - Epoch 85: Train Loss=0.4905, Train Acc=33.45%, Train MSE=1.2490
2025-03-03 08:54:35 - INFO - Epoch 85: Val Loss=0.4672, Val Acc=68.00%
2025-03-03 08:54:35 - INFO - Classification Report :               precision    recall  f1-score   support

        Sell       0.00      0.00      0.00       750
        Hold       0.68      1.00      0.81      3226
         Buy       0.00      0.00      0.00       768

    accuracy                           0.68      4744
   macro avg       0.23      0.33      0.27      4744
weighted avg       0.46      0.68      0.55      4744

2025-03-03 08:54:35 - INFO - Updated class weights : tensor([1., 1., 1.], device='mps:0')
2025-03-03 08:54:39 - INFO - Epoch : 86 , Batch [ 0 / 93 ] : Loss = 0.492504, Accuracy = 28.91%, MSE = 0.7227
2025-03-03 08:54:39 - INFO - Classwise accuracy : (0: 0.00% (92)), (1: 100.00% (72)), (2: 2.17% (92))
2025-03-03 09:19:41 - INFO - Epoch : 86 , Batch [ 10 / 93 ] : Loss = 0.495725, Accuracy = 32.07%, MSE = 1.5050
2025-03-03 09:19:41 - INFO - Classwise accuracy : (0: 100.00% (82)), (1: 0.00% (85)), (2: 0.00% (89))
2025-03-03 09:50:41 - INFO - Epoch : 86 , Batch [ 20 / 93 ] : Loss = 0.485143, Accuracy = 32.35%, MSE = 1.3127
2025-03-03 09:50:41 - INFO - Classwise accuracy : (0: 1.18% (85)), (1: 100.00% (96)), (2: 0.00% (75))
2025-03-03 09:56:18 - INFO - Epoch : 86 , Batch [ 30 / 93 ] : Loss = 0.489259, Accuracy = 32.31%, MSE = 1.2897
2025-03-03 09:56:18 - INFO - Classwise accuracy : (0: 81.82% (77)), (1: 3.26% (92)), (2: 10.34% (87))
